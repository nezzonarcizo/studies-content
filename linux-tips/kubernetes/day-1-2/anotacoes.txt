Para a aula do Minikube é necessário ter uma desktop Windows/Linux/MacOS com um VirtualBox instalado de preferencia. Mas poderá ser um cloud provider como a AWS, Google Cloud ou Azure.

Já para a aula sobre a instalação do Kubernetes, é necessário ter 03 máquinas/vm/instancias com o setup mínimo abaixo:
Debian, Ubuntu, Centos, Red Hat, Fedora, SuSe.
2 Core CPU
2GB de memória RAM

-*- Day 1

-*- Comandos resumidos após a instalação
# minikube start
# kubectl get nodes
# minikube ip
# minikube ssh 
# minikube start
# minikube stop
# minikube dashboard (Comando muito legal, administração gráfica do Cluster)
# minikube logs


-*- Comandos detalhados
# minikube status

minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

# kubectl get nodes

NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   16m   v1.21.2

# kubectl get deployment --all-namespaces

NAMESPACE     NAME      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   coredns   1/1     1            1           16m

# kubectl get pods --all-namespaces

NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kube-system            coredns-558bd4d5db-6985p                     1/1     Running   0          111m
kube-system            etcd-minikube                                1/1     Running   0          111m
kube-system            kube-apiserver-minikube                      1/1     Running   0          111m
kube-system            kube-controller-manager-minikube             1/1     Running   0          111m
kube-system            kube-proxy-sjxkf                             1/1     Running   0          111m
kube-system            kube-scheduler-minikube                      1/1     Running   0          111m
kube-system            storage-provisioner                          1/1     Running   0          111m
kubernetes-dashboard   dashboard-metrics-scraper-7976b667d4-crzwx   1/1     Running   0          70m
kubernetes-dashboard   kubernetes-dashboard-6fcdf4f6d-46298         1/1     Running   0          70m

# minikube --help

minikube provisions and manages local Kubernetes clusters optimized for development workflows.

Basic Commands:
  start          Starts a local Kubernetes cluster
  status         Gets the status of a local Kubernetes cluster
  stop           Stops a running local Kubernetes cluster
  delete         Deletes a local Kubernetes cluster
  dashboard      Access the Kubernetes dashboard running within the minikube cluster
  pause          pause Kubernetes
  unpause        unpause Kubernetes

.......

# minikube ip

192.168.99.100

# minikube ssh                          
#                         ( )           ( )           
#   ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __  
# /' _ ` _ `\| |/' _ `\| || , <  ( ) ( )| '_`\  /'__`\
# | ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
# (_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

# (dentro da máquina criada pelo minikube) docker container ls
`
CONTAINER ID   IMAGE                    COMMAND                  CREATED          STATUS          PORTS     NAMES
b490f590e88a   6e38f40d628d             "/storage-provisioner"   27 minutes ago   Up 27 minutes             k8s_storage-provisioner_storage-provisioner_kube-system_e916960c-48c4-4942-b039-cc77bc9db303_0
8d4d99d4f4d1   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_storage-provisioner_kube-system_e916960c-48c4-4942-b039-cc77bc9db303_0
58522af84c15   296a6d5035e2             "/coredns -conf /etc…"   27 minutes ago   Up 27 minutes             k8s_coredns_coredns-558bd4d5db-6985p_kube-system_d44581cd-88eb-465f-aee7-9a6c15e1d813_0
8bfe7f5333e6   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_coredns-558bd4d5db-6985p_kube-system_d44581cd-88eb-465f-aee7-9a6c15e1d813_0
a13c267e5895   a6ebd1c1ad98             "/usr/local/bin/kube…"   27 minutes ago   Up 27 minutes             k8s_kube-proxy_kube-proxy-sjxkf_kube-system_fb95ad02-4c2e-4dcd-af9d-3bf62242ca84_0
53c3d4931d83   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_kube-proxy-sjxkf_kube-system_fb95ad02-4c2e-4dcd-af9d-3bf62242ca84_0
f92a2e8ab31d   0369cf4303ff             "etcd --advertise-cl…"   27 minutes ago   Up 27 minutes             k8s_etcd_etcd-minikube_kube-system_78fc23fad92f2f70ef0e37efc3574016_0
ea4ec90f6238   f917b8c8f55b             "kube-scheduler --au…"   27 minutes ago   Up 27 minutes             k8s_kube-scheduler_kube-scheduler-minikube_kube-system_a2acd1bccd50fd7790183537181f658e_0
f1827bb50b58   ae24db9aa2cc             "kube-controller-man…"   27 minutes ago   Up 27 minutes             k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_533a59bbe1f287c135099b24dd8b3b1c_0
0948f401dbd6   106ff58d4308             "kube-apiserver --ad…"   27 minutes ago   Up 27 minutes             k8s_kube-apiserver_kube-apiserver-minikube_kube-system_e01ce89185b44a8c35a51e9afd1a8021_0
12c6aa3a267c   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_kube-scheduler-minikube_kube-system_a2acd1bccd50fd7790183537181f658e_0
7fa191f1d1c6   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_kube-controller-manager-minikube_kube-system_533a59bbe1f287c135099b24dd8b3b1c_0
3d6f6d9a6ebf   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_kube-apiserver-minikube_kube-system_e01ce89185b44a8c35a51e9afd1a8021_0
23fca23328ab   k8s.gcr.io/pause:3.4.1   "/pause"                 27 minutes ago   Up 27 minutes             k8s_POD_etcd-minikube_kube-system_78fc23fad92f2f70ef0e37efc3574016_0
`
Portas que devemos nos preocupar: 

MASTER kube-apiserver => 6443 TCP 
etcd server API => 2379-2380 TCP 
Kubelet API => 10250 TCP 
kube-scheduler => 10251 TCP 
kube-controller-manager => 10252 TCP 
Kubelet API Read-only => 10255 TCP 
WORKER Kubelet API => 10250 TCP 
Kubelet API Read-only => 10255 TCP 
NodePort Services => 30000-32767 TCP


-*- Recursos utilizados

Máquinas que poderei utilizar para Instalar o cluster na AWS: t4g.medium
Valor por hora (on demand): 0.0336 x 3 =  0,1008

-*- Comandos para os terminais:

Navegando entre os terminais: Alt + -> / Alt + <-

Verificando dados de processamento: cat /proc/cpuinfo

Verificando quantidade de memória ram: free -m

Recolhendo os terminais: Ctrl + Shift + '`'


-*- Instalando o cluster Kubernetes Versão 1.19.3:

Elevando privilégios: sudo su -

Trocando o hostname: hostname "elliot-x" (depois "bash" para verificar) (exit e logout para validar)

Para adicionar o hostname ao /etc...

# echo "nome do host" > /etc/hostname

Entrar de novo nas máquinas e elevar privilégios:

'É importante adicionar alguns módulos de kernel antes de começar'

# vim /etc/modules-load.d/k8s.conf 

Adicionar os seguintes modulos:

br_netfilter
ip_vs
ip_vs_rr
ip_vs_sh
ip_vs_wrr
nf_conntrack_ipv4

Salvar e atualizar deixar o ambiente todo atualizado:

# apt-get update && apt-get upgrade

Rebootar as máquinas após terminar de atualizar:

# reboot

Após conectar de volta nas máquinas elevar permissões novamente:

# sudo su - "Muda para o root e vai para o diretório raiz"

# sudo su "Só muda para o usuário root"

Será necessário atribuir nome a elas novamente:

# hostname "elliot-X" (exit e logout novamente)

Agora vamos reconectar nas máquinas e instalar o Docker (ou outra container engine como o Podman):

# curl -fsSL https://get.docker.com | bash

'Obs: Através deste comando acima estamos instalando o Docker CE, pois estamos utilizando ubuntu nos nodes.'
      'Caso fossemos instalar o Kubernetes em nodes RedHat, teríamos que instalar o Docker EE que é o Enterprise'

Depois de instalarmos o Docker temos que executar um procedimento:

# vim /etc/docker/daemon.json

  Adicionar o seguinte conteúdo:

  {
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
      "max-size": "100m"
    },
    "storage-driver": "overlay2"
  }

  Para verificar se o conteúdo foi colocado corretamente no arquivo

    # cat /etc/docker/daemon.json

  Devemos fazer a criação do diretório do 'docker.service.d'
  
  # mkdir -p /etc/systemd/system/docker.service.d

    Este é o diretório que o Docker vai utilizar para armazenar arquivos relacionados ao systemd, afinal, mudamos o cgroups


    Depois disto iremos reinicializar os serviços utilizados pelo Docker:

      # systemctl daemon-reload
      # systemctl restart docker

      # docker info | grep -i cgroup



Agora precisamos de mais três componentes para fechar o nosso cluster: 

  Kubelet: É o agente que vai rodar nos nodes.

  Kubeadm: Que é quem vai criar o cluster. Ele é o que 'universaliza' o Kubernetes, pode ser utilizado para instalar o Kubernetes em qualquer lugar.
            Já vem por padrão com os 'rbacs' instalados.

  Kubectl: Ele é o nosso controller, toda vez que executarmos um comando no cluster é ele que vamos utilizar.



  Precisamos das chaves para baixa-los, primeiro faremos um 'curl' e depois um 'apt=key add'

  # curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

    A saída deverá ser apenas um ok


  Agora vamos adicionar o endereço do Kubernetes no arquivo das nossas máquinas:

  # echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list

  Depois precisamos de um cat para verificar se está tudo certo:

  # cat /etc/apt/sources.list.d/kubernetes.list

    A saída deverá ser:

      'deb http://apt.kubernetes.io/ kubernetes-xenial main'

  Como adicionamos mais um endereço de repositório, agora devemos realizar um:
  
  # apt-get update

  Agora devemos instalar os nossos 3 módulos:

  # apt-get install -y kubeadm kubelet kubectl

  Agora finalmente iremos instalar o nosso cluster

    'Obs: À partir daqui devemos apenas realizar estes comandos no Node Master'

  # kubeadm config images pull (Necessário para que ele baixe as imagens, ele faz também normalmente no init, mas assim poupamos tempo)


  Depois sim a inicialização do cluster:

  # kubeadm init

      'Obs: Caso algo tenha dado errado antes, tente: # kubeadm reset ''
        'E depois # kubeadm init novamente'

      '[reset] Reading configuration from the cluster...'
      '[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
      '[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.'
      '[reset] Are you sure you want to proceed? [y/N]: y'


  O Kubernetes vai recomendar que seja criado um diretório no '$HOME' 

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    ls -la "Para verificar que no diretório atual ele criou o .kube"

    'Obs: caso esteja recriando o cluster em uma máquina que já tinha kubernetes, o diretório acima deve ser deletado e recriado para reinstalação de'
      'alguns componentes'

  Agora precisamos resolver a questão de comunicação de rede entre, por exemplo: Pods do Node 1 se comunicando com Pods do Node 2.

  Antes de qualquer coisa, vamos subir alguns modulos do kernel. 
    (No Ubuntu já sobem por padrão estes módulos, mas nunca se sabe qual distribuição vamos estar utilizando, por desencargo...)

      # modprobe br_netfilter ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 ip_vs

        Obs: Caso tenha criado o arquivo do começo do tutorial, e, rebootado a máquina, não é necessário realizar estes comandos.

  Agora, para que isso funcione temos que fazer o download e instalação de um Pod Network, existem vários, mas como recomendação do curso vou utilizar o 
  'Weave Network'

  # kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


  Agora podemos começar a utilizar os comandos do Kubectl:

      Verificar quantos Nodes temos: # kubectl get nodes

      Verificar os Pods em todos os namespaces: # kubectl get pods -n kube-system ou kubectl get pods --all-namespaces

        Obs: Podemos verificar o Pod do Weave net que acabamos de instalar e que existem dois containers dentro de um mesmo Pod.

        NAMESPACE     NAME                                READY   STATUS    RESTARTS        AGE
        kube-system   coredns-78fcd69978-b8dh4            1/1     Running   0               15m
        kube-system   coredns-78fcd69978-hj6zp            1/1     Running   0               15m
        kube-system   etcd-elliot-01                      1/1     Running   0               15m
        kube-system   kube-apiserver-elliot-01            1/1     Running   0               15m
        kube-system   kube-controller-manager-elliot-01   1/1     Running   0               15m
        kube-system   kube-proxy-phf75                    1/1     Running   0               15m
        kube-system   kube-scheduler-elliot-01            1/1     Running   0               15m
        kube-system   weave-net-nbbs6                     2/2     Running   1 (4m10s ago)   4m15s


  Agora vamos adicionar os outros Nodes:

    OBs: Antes de realizar o comando para adicionar mais Nodes ao Cluster, se este estiver em algum Cloud Provider, temos que nos atentar se o acesso
          entre as máquinas que formam nosso Cluster está totalmente liberado.

     O comando será similar a este: kubeadm join 172.31.7.204:6443 --token m0g0o5.uin7e9mcvwv3tfnu \
     --discovery-token-ca-cert-hash sha256:53a6afecb9229e4b2c769da1c38569935643c85897b3dfecee49f1225130efff

     'Obs: Caso tenha realizado um "clear" e tenha perdido este comando, podemos ver ele novamente com o comando...'

     'kubeadm token create --print-join-command'


    Para verificar se os Nodes foram adicionados ao nosso Cluster, devemos realizar o comando:

      # kubectl get nodes

      Todos eles devem ficar ready em determinado momento..

    'OBs: Caso um dos Nodes demore para ficar pronto ("Ready"), reinicie o Kubelet: # systemctl restart kubelet'
        'Para verificar os possíveis problemas com o Node, que provavelmente será reportado pelo Kubelet: # journalctl -u kubelet'



-*- Desligar as máquinas na AWS na hora que terminar de utilizar


Caso de um erro dizendo que o kubelet não está funcionando execute os dois comandos abaixo:

'Obs: Em todos os Nodes'

# sudo swapoff -a
# sudo sed -i '/ swap / s/^/#/' /etc/fstab


-*- Primeiros passos

# kubectl describe nodes elliot-01

Neste comando são exibidas todas as informações sobre o node
Instrutor se atentou a sinalizar o "Taints", nele definimos restrições para nossos nós, logo abaixo temos uma restrição para que o node master não tenha
pods...

'node-role.kubernetes.io/master:NoSchedule'

Fazer com que o kubectl complete os comandos com Tab para facilitar a vida: # kubectl completion bash 
'Obs: Importante que já esteja instalado o bash-completion'
'apt-get install -y bash-completion'

Importante redirecionarmos o completion bash para um arquivo no diretório bash_completion.d
# kubectl completion bash > /etc/bash_completion.d/kubectl

Para forçar de ínicio o bash completion sem precisar sair da máquina e voltar
# source <(kubectl completion bash)

Ver os Pods do kube-system
# kubectl get pods -n kube-system

Descrevendo um Pod
# kubectl describe pods -n kube-system "nome do pod"
'Obs: Com este comando podemos ver todos os containers que estão rodando dentro deste Pod, assim como todas as informações destes containers'

Ver bem mais detalhes sobres os Pods em determinado namespace
# kubectl get pods --all-namespace -o wide

Verificar todos os namespaces
# kubectl get namespaces

Criar: # kubectl create namespace "nome_do_namespace"

Diferença entre versões antigas, tipo da 15 pra trás, para as de agora
O comando 'kubectl run nginx --image=nginx' por exemplo, agora cria apenas um Pod, antigamente criava um deployment.

Depois para descrever este Pod criado # kubectl describe pod nginx

Na seguinte linha podemos ver que ele foi criado no Node 'elliot-02'
'Normal  Scheduled  8s    default-scheduler  Successfully assigned default/nginx to elliot-02'

Podemos ver os detalhes com 'kubectl get pods nginx', se quisermos mais detalhes 'kubectl get pods -o wide'

Para vermos o manifesto com o qual foi criado este nosso pod 'kubectl get pods nginx -o yaml'

Para direcionar o conteúdo diretamente para um arquivo .yaml 'kubectl get pods nginx -o yaml > meu_primeiro_pod.yaml'

Temos um problema com a forma acima de pegar o manifesto de como o Pod foi criado, pois tem muita informação inútil que não será utilizada.
Para que possamos pegar apenas as informações que de fato são necessárias para a criação do POD, podemos utilizar a flag '--dry-run=client'.
Desta forma, ele "finge" que roda o Pod, e trás pra nós apenas ao manifesto básico necessário para criação do nosso Pod.

O comando ficaria assim # kubectl run nginx --image=nginx --dry-run=client -o yaml
Caso fosse um deployment # kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml

'Obs: Na linha de cima além de acrescentarmos o tipo do recurso criado no cluster também adicionamos o nome do arquivo yaml que desejamos'


Para criar qualquer recurso no Kubernetes através de manifestos # kubectl create -f "manifesto.yaml" 
                                                              ou # kubectl apply -f "manifesto.yaml"


Podemos criar um 'serviço' de forma automática para o nosso Pod, para que assim possamos acessá-lo externamente...

O comando seria # kubectl expose pod nginx
      Poderia ser um deployment também # kubectl expose deployment -n "nome_do_deployment"

Porém antes, dentro das configurações do container no manifesto, devemos colocar as seguinte configuração
ports:
- containerPort: 80

Normalmente o que vai ser criado com este comando é um ClusterIP, este tipo de serviço é para que o Pod seja acessível dentro do cluster

Para que possamos mudar o tipo de serviço em runtime, podemos utilizar o comando # kubectl edit service nginx

Para que possamos ver quais serviços temos realacionados a um pod # kubectl describe services nginx

Para ter uma explicação no console de cada recurso do Kubernetes podemos fazer # kubectl explain 'serviço'
'ex: kubectl explain pod'

Para pegar tudo o que temos criado no nosso cluster # kubectl get all

Temos também algumas abreviações como # kubectl get po (kubectl get pods)
                                    ou # kubectl get svc (kubectl get services)
                                    ou # kubectl get deploy (kubectl get deployments)
                                    ou # kubectl get ns (kubectl get namespaces)


Caso queiramos escalar algum recurso (Versão 1.14) # kubectl scale --replicas=10 deployment nginx

Para ver o histórico de comandos: # history


-*- Aula ao Vivo 16-05-2020

- Parte 1: Anúncios...
- Parte 2: Instalação Minikube, Kind e K3S (Raspberry)
- Parte 3: Continuação da instalação do K3S (Raspberry), instalação K8S na AWS
- Parte 4: Continuação da instalação do K8S na AWS, criando manifestos através da saída dos comandos

      Entrando dentro de um container em execução # kubectl exec -ti nginx sh
      Pasta dos arquivos "html" do nginx # /usr/share/nginx/html
      Para colocarmos alguma coisa dentro do arquivo index.html para testar # echo "Mensagem" > index.html
      Ver processamento e memória # htop
      Verificar load average # uptime


-*- Day 1 - Extended - K8S Multi-Master

      k8s-Multi-Master - Criando instâncias e configurando o HAproxy
      
      Dando nome para as máquinas
      
      # hostname k8s-haproxy-01
      # echo "k8s-haproxy-01" > /etc/hostname
      
      Instalando o HAproxy
      
      # apt-get update
      # apt-get install -y haproxy
      
      Configuração HAproxy
      
      # cd /etc/haproxy/
      # ls
      # vi haproxy.cfg
      
      Obs: Não foi mexida com a configuração básica, para mais informações sobre ela buscar na net
      
      frontend kubernetes
              mode tcp
              bind "IP DA MÁQUINA HAPROXY":6443
              option tcplog
              default_backend k8s-masters
      
      backend k8s-masters
              mode tcp
              balance roundrobin
              option tcp-check
              server k8s-master-01 "IP MASTER 1 PRIVADO":6443 check fall 3 rise 2
              server k8s-master-02 "IP MASTER 2 PRIVADO":6443 check fall 3 rise 2
              server k8s-master-03 "IP MASTER 3 PRIVADO":6443 check fall 3 rise 2
      
      Restartando o serviço
      
      # systemctl restart haproxy
      
      Verificando o serviço
      
      # systemctl status haproxy
      
      Verificando arquivos de log
      
      # tail -f /var/log/haproxy.log
      
      Verificando a disponibilidade da porta
      
      # netstat -atunp
      
      Conexão com a porta 
      
      # nc -v 172.31.19.2 6443

      Instalando o kubelet, kubeadm e o Kubectl 

      https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

      Configurando o /etc/hosts dos nodes masters para identificarem quem é o HAproxy

      127.0.0.1 localhost
      "IP PRIVADO HAPROXY" k8s-haproxy
      # The following lines are desirable for IPv6 capable hosts
      ::1 ip6-localhost ip6-loopback
      fe00::0 ip6-localnet
      ff00::0 ip6-mcastprefix
      ff02::1 ip6-allnodes
      ff02::2 ip6-allrouters
      ff02::3 ip6-allhosts

      Inicializando o ControlPlane

      sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs

      sudo kubeadm init --control-plane-endpoint "k8s-haproxy:6443" --upload-certs

      (https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)

      Meu comando ficou da seguinte maneira, lembrando que já estava como root

      # kubeadm init --control-plane-endpoint "k8s-haproxy:6443" --upload-certs

        'Obs: Caso apareça um aviso de que o Kubelet não está rodando, verifique se as portas necessárias estão abertas' ou o cgroup não está configurado
          isto tem na documentação'

        Caso algum erro menor aconteça ou um alerta que não deixa continuar: 
          kubeadm init --control-plane-endpoint "k8s-haproxy:6443" --upload-certs --ignore-preflight-errors=All

          Ou kubeadm reset ---- Se também tiver mudado o ip dos nodes master no HAPROXY resetar o serviço no servidor haproxy # systemctl restart haproxy


          You can now join any number of the control-plane node running the following command on each as root:

          kubeadm join k8s-haproxy:6443 --token 0isxrf.ds9oj7ts84c96cig \
                --discovery-token-ca-cert-hash sha256:7be6e205c1103fb996cf905109643bdcb554544d7961a601af70de8a6da1c7c6 \
                --control-plane --certificate-key 80f85abbf613ed9ea9db01133f58ead4545b96a8c578b80f710ee53b34ce7da4

          Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
          As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
          "kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

          Then you can join any number of worker nodes by running the following on each as root:

          kubeadm join k8s-haproxy:6443 --token 0isxrf.ds9oj7ts84c96cig \
                  --discovery-token-ca-cert-hash sha256:7be6e205c1103fb996cf905109643bdcb554544d7961a601af70de8a6da1c7c6


  Links Importantes:

    Tipos de topologias de K8s multi-master: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/

    Instalação kubeadm, kubelet e kubectl: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
    
    Instalação Kubernetes multi-master: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
    
    HAproxy: https://www.haproxy.org/


-*- Day 2

-------Revisão

Temos Nodes Master e Workers

'service': Uma maneira abstrata de expor um aplicativo em execução em um conjunto de pods como um serviço de rede.

São recursos dentro do cluster para realizar diversos tipos de conexões e exposições

O controller do 'Pod' é o 'ReplicaSet', e o controler do 'ReplicaSet' é o 'Deployment'.

'ReplicaSet' é responsável por levantar um pod caso ele caia, e deixar na mesma quantidade desejada de antes.

O responsável por atualizar o 'ReplicaSet' é o 'Deployment'. O 'Deployment' também é responsável por versionar a 'ReplicaSet'.

Podemos ter mais de um container rodando por Pod. Mas não podemos ter containers com IP's diferentes dentro do mesmo 'Pod', apenas portas diferentes.

Os 'Namespaces' são "domínios" dentro do cluster Kubernetes.

Podemos limitar os namespaces por quotas.

Temos os 'Storages' que são os nossos volumes. Pra utilizar eles temos o 'PV' e o 'PVC' (Persistent Volume e o Persistent Volume Claim).

-------

'kube-apiserver': Server é o cérebro do Kubernetes, ele é o cara que "conversa" com os Nodes, toda comunicação que acontece através dele é muito segura.
Somente ele tem acesso ao 'etcd' que é o cara que vai guardar todos os dados.

'kube-scheduler': Ele decide em qual Node determinado Pod vai ser hospedado.

'kube-controller-manager': É o controller principal, ele que sempre vai interagir com o 'kube-apiserver' para saber qual é o estado dos seus componentes.

'Kubelet': É o 'node agent' primário. Ele pode registrar o node junto ao apiserver.

'kube-proxy': É o cara responsável por gerenciar a rede dos containers, quando determinado pod vai subir em determinado node, ele que faz o node liberar
as portas.

'supervisord': Que é responsável por ver se o Kubelet e o container runtime estão rodando. Ele tenta reestabelece-los por algumas vezes.

'pods': Pod's são a menor unidade computacional implantável que você pode criar e gerenciar no Kubernetes.

'cni': Todos os Pods conversam entre si utilizando o IP definido para ele, o pod realmente tem o IP, não é como no docker que utilizamos NAT sempre.
No Kubernetes naõ utilizamos NAT, só o Kubernetes não faz sua rede funcionar por completo, fazer com que os toda esta comunicação dita acima funciona
usamos um recurso chamado 'cni', existem diversos plugins que usam 'cni'.

    Link do GitHub: https://github.com/containernetworking/cni

      "CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to
      configure network interfaces in Linux containers, along with a number of supported plugins. CNI concerns itself only with network connectivity of
      containers and removing allocated resources when the container is deleted. Because of this focus, CNI has a wide range of support and the
      specification is simple to implement."

      Entre estes plugins temos o 'wave cni' que é o que mais usamos no treinamento. Mas também temos o 'canal cni' e o 'calico cni'.

      Wave é disparado mais usado, é bem estável e tem muitos recursos. Se olharmos no numeros de contribuintes no projeto o wave só perde para o
      flannel que é o mais antigo.

      O mais importante de verificarmos é que, se lembrarmos das camadas do modelo OSI teremos:

        Camada 1 - Física
        Camada 2 - Enlace ou Ligação
          - A subcamada MAC
          - Subcamada LLC
        Camada 3 - Rede
          - Endereço IP
        Camada 4 - Transporte
        Camada 5 - Sessão
        Camada 6 - Apresentação
        Camada 7 - Aplicação

      O 'WaveNet' atua na camada 2.....O 'Calico' é camada 3, o 'Flannel' camada 2. O 'WaveNet' não precisa de um doc store externo, o 'Calico' precisa
      escrever o 'etcd'. Somente no 'WaveNet' e no 'Calico' tem o esquema de encryption. O esquema de policy de entrada e saída 'Ingress' e o 'Egress'
      só tem também no 'WaveNet' e no 'Calico'.

      Sobre a comunicação entre Pod's também depende do CNI como foi dito antes, só conseguimos comunicar um POD com outro neste momento se estivermos
      utilizando 'WaveNet' e 'Calico', o 'Flannel' não consegue fazer isso pois não tem a característica 'mesh'.

-*- Services

Nesta sessão utilizaremos o laboratório da aula 1...

Relembrando os passos para instalação do cluster Kubernetes:

Lembrando que o 'elliot-01' será o node master e o 'elliot-02' e 'elliot-03' serão os workers...

1 - Trocar os hostnames
hostname "xxxxxx"
echo "xxxxxx" > /etc/hostame

2 - Instalação do docker (container runtime)
curl -fsSL https://get.docker.com | bash

3 - Configuração do CGROUP driver
4 - Adição dos repositórios e da chave do Google
5 - Instalação do kubeadm kubelet e kubeclt
6 - Fazer o pull das imagens que vão compor o cluster
# kubeadm config images pull
7 - Inicializar o cluster (Fazer isso somente no node master)
8 - Instalar o CNI (Estamos usando wavenet - kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')""

Para esta aula o cluster criado, ou melhor, o node master criado gerou a seguinte chave para configurarmos os workers...

kubeadm join 172.31.19.197:6443 --token vmz5nr.cb3fgvyjow68ujyg \
--discovery-token-ca-cert-hash sha256:49b16d213bdcad14bc1a1d73be67d1b43e09e478382b182424685b8c9a2772f8


Alguns comandos do kubectl:

'kubectl get nodes'

'kubectl get deployments'

"kubectl get services ou kubectl get svc"

"kubectl get pods ou kubectl get po"

Na aula o Jeferson utilizou o yaml do deployment da outra aula, mas pra subir ele de novo manualmente podemos utilizar....

'kubectl create deployment nginx-deployment --image=nginx --replicas=10 --port=80' (Isto irá criar um deployment)

'kubectl run nginx --image nginx --port=80' (Isto irá criar um POD)

Para criar um deployment = 'kubectl run nginx --image nginx replicas=10'

Para ver ele rodando: "Kubectl get pods ou kubectl get po"

NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          51s

Para expor com um serviço: "kubectl expose pod nginx"

root@elliot-01:/home/ubuntu# kubectl expose pod nginx
service/nginx exposed

Para ver o serviço que expos o pod: "kubectl get svc ou kubectl get service"

root@elliot-01:/home/ubuntu# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   23m
nginx        ClusterIP   10.106.31.127   <none>        80/TCP    8s

Podemos ver que o tipo de serviço utilizado para expor este pod foi um "ClusterIP"
Sempre que criarmos o serviço desta forma ele criará um "ClusterIP", e este tipo de serviço só expõe o pod dentro do cluster, não externamente a este.
Mesmo que criassemos um serviço do tipo "LoadBalancer", ele criaria um "ExternalIP" mas também criaria para este serviço um "ClusterIP".

Lembrando que quando criamos o nosso cluster com estes serviços dele não precisamos utilizar o IP, podemos utilizar o DNS.

Se utilizarmos o comando: "kubectl get deployments --all-namespaces"...Veremos que já temos no namespace 'kube-system' um deployment com o nome 'core-dns'
rodando. Então ele já "conhece" pelo nome os componentes criados.

root@elliot-01:/home/ubuntu# kubectl get deployments --all-namespaces
NAMESPACE     NAME      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   coredns   2/2     2            2           30m

Podemos realizar também o comando: 'kubectl get endpoints'

Quando criamos o service ele já cria o endpoint...O endpoint nada mais é que o pod, o ip exposto é o ip do pod que criamos...

root@elliot-01:/home/ubuntu# kubectl get endpoints
NAME         ENDPOINTS            AGE
kubernetes   172.31.19.197:6443   32m
nginx        10.40.0.1:80         8m29s

Podemos observar que ele já criou com a porta 80, pois criamos ele com o '--port=80'

Quando dermos um 'curl' neste ip ele devolverá a página do 'nginx', porque é do ClusterIP, ele sabe qual é a porta, não precisamos específica-la.
Em qualquer pod que fizermos isso

root@elliot-01:/home/ubuntu# curl 10.40.0.1
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

Se quisermos olhar os detalhes: "kubectl logs -f nginx"

/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2021/10/23 19:46:28 [notice] 1#1: using the "epoll" event method
2021/10/23 19:46:28 [notice] 1#1: nginx/1.21.3
2021/10/23 19:46:28 [notice] 1#1: built by gcc 8.3.0 (Debian 8.3.0-6) 
2021/10/23 19:46:28 [notice] 1#1: OS: Linux 5.4.0-1045-aws
2021/10/23 19:46:28 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2021/10/23 19:46:28 [notice] 1#1: start worker processes
2021/10/23 19:46:28 [notice] 1#1: start worker process 31
2021/10/23 19:46:28 [notice] 1#1: start worker process 32
10.38.0.0 - - [23/Oct/2021:20:07:34 +0000] "GET / HTTP/1.1" 200 615 "-" "curl/7.68.0" "-"


-*- Services - parte 02

Para imprimirmos o service num arquivo yaml: 'kubectl get svc -o yaml'

Para redirecionar esta impressão para um arquivo: "kubectl get service nginx -o yaml > meu_primeiro_service.yaml"

Agora que imprimimos num arquivo as configurações do nosso service e editamos ele para que fique o mais limpo possível, vamos deletar o que está rodando
que foi construído através do comando....

'kubectl delete service nginx'

Vamos criar o serviço utilizando o arquivo: 'kubectl create -f meu_primeiro_service.yaml'

Se rodarmos de novo o comando para ver os services: 'kubectl get services' ... Veremos que ele foi criado da mesma forma, só que agora com IP's diferentes

-*- Service : parte 03

Sobre a parte do nosso meu_primeiro_service.yaml, 'sessionAffinity', o único tipo de afinidade que podemos criar neste momento sem ingress é o 'ClientIP'

'ClientIP': Ele vai fazer afinidade pelo endereço do cliente, como está chegando a requisição, ele vai pegar o endereço de origin e vai conectar sempre
            ao mesmo service.

            Obs: Lembrando que sempre é melhor ter o Ingress para que podemos ter outros tipos de afinidade

            Obs2: Obviamente não faz muito sentido configurarmos como afinidade o 'ClientIP' já que estamos trabalhando apenas dentro do Cluster...
                  Faria mais sentido se ao invés de um 'ClusterIP' nosso service estivesse configurado como 'NodePort'


Agora vamos expor aquele mesmo pod só que agora com 'NodePort': 'kubectl expose pod nginx --type=NodePort'

'kubectl get svc'

NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        96m
nginx        NodePort    10.96.26.175   <none>        80:30073/TCP   8s

Agora se pegarmos o IP público da nossa instância e utilizarmos a porta '30073' veremos a página do nginx...

Se utilizarmos o curl com 'ClusterIP:30073' ou 'IPprivado:30073' da instância teremos a mesma saída...

Obs: Utilizei o grupo de segurança em que estavam as instancia e nele liberei a porta TCP 30073 para 0.0.0.0/0 para ver a página do nginx no navegador.


Podemos descrever o serviço com: 'kubectl describe svc nginx'

Vamos criar agora o manifesto do service com o tipo NodePort: 'kubectl get svc nginx -o yaml > meu_primeiro_service_nodeport.yaml'

Obs: Como estava fazendo esta atividade em outro dia o número da porta e o IP do Pod haviam mudado.

-*- Services - Parte 4

Expondo um o pod via 'LoadBalancer': 'kubectl expose pod nginx --type=LoadBalancer'

Toda vez que criarmos um serviço do tipo 'Cluster IP', ele vai criar apenas o 'Cluster IP'
Toda vez que criarmos um serviço do tipo 'NodePort', ele vai criar um 'ClusterIP' e um 'NodePort'
Toda vez que criarmos um serviço do tipo 'LoadBalancer' ele vai criar um 'ClusterIP', um 'NodePort' e um 'LoadBalancer'


Para mandar para um arquivo o nosso primeiro serviço do tipo 'LoadBalancer': kubectl get svc nginx -o yaml > meu_primeiro_service_loadbalancer.yaml

Quando criamos um 'service' ele cria automaticamente um 'endpoint', e, relembrando, para ver os endpoints: kubectl get endpoints
Para descreve-lo: kubectl describe endpoint nginx:

Name:         nginx
Namespace:    default
Labels:       run=nginx
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2021-10-31T17:41:21Z
Subsets:
  Addresses:          10.36.0.1
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  80    TCP

Events:  <none>

Podemos juntar o deployment e o svc num só arquivo, basta separarmos por uma linha com a expressão '---' e colocar o conteúdo do service ou qualquer recurso que
queira adicionar logo abaixo.

No caso se aplicarmos o conteudo do 'svc-nginx-deployment-1.yaml' que é um service, dentro do arquivo 'meu_primeiro_deployment.yaml', separando os conteúdos
por '---', ao aplicarmos, teremos os dois recursos subindo ao mesmo tempo...

root@elliot-01:~# kubectl create -f meu_primeiro_deployment.yaml
deployment.apps/nginx-deployment created
service/nginx-deployment created


-*- Limitando recursos - parte 01

resources:
  limits: # O máximo de recurso que o kubernetes vai liberar pra o container
    memory: 512Mi
    cpu: 500m
  requests: # O quanto ele vai garantir
    memory: 256MSi
    cpu: 250m

    Estas formas acima de definir os valores de CPU e Memória são os padrões Kubernetes.
    Sempre que o Kubernetes for "schedular" um pod ou container em determinado nó, ele vai levar em consideração o 'requests'.

Caso estivessemos criando o deployment e já existisse outro com o mesmo nome, podemos utilizar o 'replace'

'kubectl replace -f deployment_limitado.yaml'


-*- Limitando recursos - parte 02

Podemos alterar qualquer atributo dos nossos services, deployments etc, utilizando o comando:

'kubectl edit service "nome_do_servico"' ou
'kubectl edit deployment "nome_do_deployment"'

Fizemos alguns testes com este comando na aula...
O primeiro foi utilizando 'kubectl edit service nginx-deployment' e alterando de ClusterIP para NodePort, o segundo foi alterando a porta do Node para '30080'.

Depois alteramos o número de réplicas para 5 no Deployment...

'kubectl edit deployment nginx-deployment'

Depois disso, para verificar que a alteração surtiu efeito, realizamos o comando: 'kubectl get replicaset', e vimos que deu tudo certo...

NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-78f8f7f6fc   5         5         5       21m

Obs: Erros de sintaxe ou alterações erradas em coisas que são vitais para o recurso, por exemplo, nome, labels etc, não serão permitidos e exibirão um erro após
a tentativa...

"A copy of your changes has been stored to "/tmp/kubectl-edit-2099933572.yaml"
error: At least one of apiVersion, kind and name was changed"


-*- Limitando recursos - parte 03

Podemos ver nossos POD's do ReplicaSet: "kubectl get pods -o wide"

NAME                                READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-78f8f7f6fc-2ll9k   1/1     Running   0          31m   10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-48jv8   1/1     Running   0          31m   10.44.0.3   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-dv684   1/1     Running   0          31m   10.40.0.5   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-pnn4s   1/1     Running   0          31m   10.40.0.1   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-prjsf   1/1     Running   0          31m   10.44.0.4   elliot-03   <none>           <none>

Para entrar em um dos nossos PODs: 'kubectl exec -it nginx-deployment-78f8f7f6fc-2ll9k -- bash'

'root@nginx-deployment-78f8f7f6fc-2ll9k:/#'

Instalamos uma ferramenta para gerar alto uso de CPU e Memória, o 'stress'

'apt-get update && apt-get install -y stress'

Vamos simular uma virtual machine "--vm 1" estressando o ambiente, passando a quantidade em megas "--vm-bytes 128M"

"stress --vm 1 --vm-bytes 128M"
Saída: stress: info: [313] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd # Nada de errado aconteceu

Aumentamos para 256M
"stress --vm 1 --vm-bytes 256M"
Saída: stress: info: [315] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd # Nada de errado aconteceu

Aumentamos para 300M
"stress --vm 1 --vm-bytes 300M"
Saída: stress: info: [317] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd # Nada de errado aconteceu

Aumentamos para 500M
"stress --vm 1 --vm-bytes 300M"
Saída: stress: info: [319] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd # Nada de errado aconteceu

Aumentamos para 550M
"stress --vm 1 --vm-bytes 550M"
Saída: stress: info: [321] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd
stress: FAIL: [321] (415) <-- worker 322 got signal 9
stress: WARN: [321] (417) now reaping child worker processes
stress: FAIL: [321] (451) failed run completed in 1s # Falha, pois o limite de recurso para cada container é de 500M

Nosso limite no caso testado que passou, ou seja, o limite em que deu pra deixar com o container funcionando foi 504M, pois já existem 7 megas sendo utilizados
de execução...

Saímos do container...

Obs: Se deseja aumentar os recursos para realizar outros testes, basta realizar o comando: 'kubectl edit deployment "nome_do_deployment"', e alterar o recurso
desejado.

-*- LimitRange - parte 01

Agora nós vamos aprender a trabalhar com 'namespaces'...

'kubectl create namespace terere'

'kubectl get namespaces'

'kubectl describe namespace terere'

Name:         terere
Labels:       kubernetes.io/metadata.name=terere
Annotations:  <none>
Status:       Active

No resource quota.

No LimitRange resource.

"kubectl get namespaces terere -o yaml > namespace-terere.yaml"

Pegamos seu conteúdo, limpamos o arquivo e trocamos o nome do namespace para chimarrão...Agora aplicamos o arquivo...

'kubectl create -f namespace-terere.yaml'

Teremos agora tanto o namespace terere quanto o namespace chimarrao:

'kubectl get namespace'

NAME              STATUS   AGE
chimarrao         Active   6s
default           Active   2d1h
kube-node-lease   Active   2d1h
kube-public       Active   2d1h
kube-system       Active   2d1h
terere            Active   6m36s

Agora aprenderemos a usar o LimitRange, que é uma regra que limita recursos mas não por deployment, mas sim, pra um namespace todo...

Neste primeiro exemplo vamos entender os rótulos...

apiVersion: v1
kind: LimitRange # Tipo do recurso do cluster
metadata:
  name: limitrange-1 # Nome do recurso em si, personalizado
spec:
  limits:
  - default: # Este é o equivalente ao limits do deployment, ou seja, o máximo que o cluster vai ceder de cpu e memória
      cpu: 1
      memory: 100Mi
    defaultRequest: # defaultRequest é o equivalente ao request do deployment, ou seja, o tanto de cpu e memória que o cluster vai garantir
      cpu: 0.5
      memory: 80Mi
    type: Container # Para quem no namespace esta limitação se dará


limits, request, default e defaultRequest em se tratando de recursos no kubernetes, equivalem aos conceitos de hard e soft limits.

Resumo: No namespace que eu aplicar este LimitRange, toda vez que eu criar um recurso, para os Containers, haverá um limit de CPU e Memória.


-*- LimitRange - parte 02

Agora com o arquivo criado precisamos aplicá-lo em algum namespace, para aplicá-lo em um namespace já que não passamos qual seria dentro do manifesto, precisamos
realizar o seguinte comando: 'kubectl create -f limitrange-1.yaml -n terere'

Para vermos os LimitRanges definidos para determinado namespace devemos utilizar o comando:

"kubectl get limitranges -n "nome_do_limitrange""

ou "kubectl get limitranges namespace "nome_do_limitrange""

Uma coisa bem legal é ver o describe do LimitRange que criamos, basta executar o comando:

"kubectl describe limitranges -n terere"

Name:       limitrange-1
Namespace:  terere
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    500m             1              -
Container   memory    -    -    128Mi            256Mi          -

Vamos criar um POD limitado...

vim pod-limitado.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod-limitado
spec:
  containers:
    - name: container-limitado
      image: nginx

      Obs: Se quisermos podemos colocar todas as configurações que tinhamos no 'deployment' na parte 'spec' do 'container'.

Obs: Não estamos colocando o namespace dentro do arquivo para que possamos reutiliza-lo.
  Caso queira colocar, basta dentro do 'metadata' acrescentar 'namespace: nome_do_namespace'


-*- LimitRange - parte 03

Vamos criar agora com o nosso arquivo do 'pod-limitado.yaml' o nosso pod no namespace que limitará seus recursos...

'kubectl create -f pod-limitado.yaml -n terere'

Para visualizarmos este pod que foi criado:

'kubectl get pod -n terere'

Obs: podemos abreviar colocando somente 'po' para 'pod'

NAME           READY   STATUS    RESTARTS   AGE
pod-limitado   1/1     Running   0          12s

'kubectl describe po -n terere'

---
Ready:          True
Restart Count:  0
Limits:
  cpu:     1
  memory:  256Mi
Requests:
  cpu:        500m
  memory:     128Mi
Environment:  <none>
---

Podemos ver que o nosso Pod agora está com seus recursos limitados mesmo não tendo declarado em seu próprio arquivo.


-*- Taints

Se realizarmos o comando: 'kubectl describe node elliot-02'
...teremos um descritivo completo de seus recursos, imagem, consumo, memory, storage etc

porém em 'Taints' temos 'none', ou seja, nao temos nada

Porém se formos no 'elliot-01', em 'Taints', teremos um 'node-role':

'node-role.kubernetes.io/master:NoSchedule'

Ou seja, esta regra serve para que tenhamos só os 'containers', 'pods', 'deployments', etc, que estão em execução serão mantidos, porém nenhum outro será 
'schedulado' para este 'Node'.

Os 'Taints' servem para que organizemos o nosso cluster, deixar que subam muitos recursos nos "nodes masters" faz com que o cluster todo fique lento, mas também,
serve para que façamos isso também nos nossos "nodes workers", caso desejemos.

Vamos verificar onde nossos pods estão rodando:

"kubectl get pods -o wide"

NAME                                READY   STATUS    RESTARTS   AGE     IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-78f8f7f6fc-2ll9k   1/1     Running   0          4h41m   10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-48jv8   1/1     Running   0          4h41m   10.44.0.3   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-4ml5p   1/1     Running   0          51s     10.40.0.4   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-dcmdx   1/1     Running   0          51s     10.40.0.2   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-dgqbp   1/1     Running   0          51s     10.40.0.6   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-dv684   1/1     Running   0          4h41m   10.40.0.5   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-pnn4s   1/1     Running   0          4h41m   10.40.0.1   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-prjsf   1/1     Running   0          4h41m   10.44.0.4   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-ptddm   1/1     Running   0          51s     10.44.0.5   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-q6d89   1/1     Running   0          51s     10.44.0.2   elliot-03   <none>           <none>

Nosso deployment tem 10 réplicas, podemos ver que 6 delas estão rodando no node elliot-02 e 4 no elliot-03.

Agora vamos adicionar aquela mesma regra do nosso node master (elliot-01) neste nosso node worker (elliot-02), ou seja, se caso escalarmos o nosso deployment
o node worker elliot-02 não poderá mais schedular mais pod nenhum.

Vamos editar primeiro o nosso deployment diminuindo o número de replicas para 6.

'kubectl edit deployment "nome_do_deployment"' # Assim no número de replicas podemos colocar '6'

Ou também podemos executar o comando: 'kubectl scale --replicas=6 deployment "nome_do_deployment"'

'kubectl get po -o wide'

NAME                                READY   STATUS    RESTARTS   AGE     IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-78f8f7f6fc-2ll9k   1/1     Running   0          4h53m   10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-48jv8   1/1     Running   0          4h53m   10.44.0.3   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-dv684   1/1     Running   0          4h53m   10.40.0.5   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-prjsf   1/1     Running   0          4h53m   10.44.0.4   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-ptddm   1/1     Running   0          12m     10.44.0.5   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-x4hrr   1/1     Running   0          3m5s    10.40.0.1   elliot-02   <none>           <none>

Agora temos 3 pods do nosso deployment rodando em cada um dos nodes workers, elliot-02 e elliot-03

Vamos aplicar o 'Taint' ao nosso node 'elliot-02'

'kubectl taint node elliot-02 key1=value1:NoSchedule'

Saída: node/elliot-02 tainted

Se dermos um describe agora no nosso node elliot-02, veremos na parte do 'Taints':

---
CreationTimestamp:  Sun, 31 Oct 2021 17:00:06 +0000
Taints:             key1=value1:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  elliot-02
  AcquireTime:     <unset>
---

Agora vamos subir o número de replicas no nosso deployment...

'kubectl scale --replicas=10 deployment nginx-deployment'

Como temos a questão do 'resources' setado no deployment, ele não conseguira subir todos os containers, vamos tirar aquele trecho de código para que ele termine
de subir todas as replicas...

'kubectl edit deployment nginx-deployment'

Explicação: Na parte dos recursos, no deployment, estamos dizendo que é pro cluster garantir (requests) 250m (0,25 milicores) de cpu para cada container, e a
nossas máquinas (Nodes) possuem 2vCPUs, ou seja, só caberiam 8 container no máximo, porém ainda temos que lembrar que temos outras recursos rodando no node, como
S.O e outras coisas.

'kubectl get po -o wide'

NAME                                READY   STATUS    RESTARTS   AGE     IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-78f8f7f6fc-2ll9k   1/1     Running   0          5h8m    10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-48jv8   1/1     Running   0          5h8m    10.44.0.3   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-dv684   1/1     Running   0          5h8m    10.40.0.5   elliot-02   <none>           <none>
nginx-deployment-78f8f7f6fc-f8pw8   0/1     Pending   0          7m14s   <none>      <none>      <none>           <none>
nginx-deployment-78f8f7f6fc-p2djv   1/1     Running   0          7m14s   10.44.0.6   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-p8ln6   1/1     Running   0          7m14s   10.44.0.2   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-prjsf   1/1     Running   0          5h8m    10.44.0.4   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-ptddm   1/1     Running   0          27m     10.44.0.5   elliot-03   <none>           <none>
nginx-deployment-78f8f7f6fc-rzdzg   0/1     Pending   0          7m14s   <none>      <none>      <none>           <none>
nginx-deployment-78f8f7f6fc-x4hrr   1/1     Running   0          18m     10.40.0.1   elliot-02   <none>           <none>

Vamos retirar o bloco onde definimos os soft e hard limits para que os requisitos sejam 'elásticos' e não tenhamos uma regra de recurso mínima.

Após a alteração poderemos ver com: 'kubectl get po -o wide' os container começando a subir e com 'kubectl get deployment "nome_do_deployment"' que estão os 10
container de pé, e mais o detalhe principal sobre o qual estavamos estudando, a nossa 'node-role' no 'Taints' fez com que todos os demais 'PODs' que subiram
fossem 'schedulados' no node 'elliot-03'.

Obs: Se olharmos de novo os Pods com 'kubectl get po -o wide' veremos o seguinte resultado...

NAME                                READY   STATUS    RESTARTS   AGE     IP           NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-7c9cfd4dc7-2c9lf   1/1     Running   0          6m19s   10.44.0.8    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-7j2pc   1/1     Running   0          6m16s   10.44.0.2    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-dbg2l   1/1     Running   0          6m13s   10.44.0.4    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-gj9sx   1/1     Running   0          6m19s   10.44.0.10   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-jt4qz   1/1     Running   0          6m12s   10.44.0.3    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-ltvt4   1/1     Running   0          6m13s   10.44.0.5    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-pgt58   1/1     Running   0          6m19s   10.44.0.7    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-pxxdh   1/1     Running   0          6m16s   10.44.0.6    elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-tjmbw   1/1     Running   0          6m19s   10.44.0.11   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-xvgmn   1/1     Running   0          6m19s   10.44.0.9    elliot-03   <none>           <none>

...Por que agora só temos Pods no elliot-03? É o seguinte, como removemos o bloco 'resources' de dentro do deployment e salvamos, ele precisou reiniciar os
recursos que já existiam, e como já tinhamos configurado o 'Taints' do elliot-02 para que não fossem mais alocados recursos criados futuramente, ele colocou todos
os novos Pods no elliot-03, que basicamente é o único node que deixamos livre para receber novos recursos.

Vamos fazer de uma forma diferente agora, vamos remover o taint e vamos ver de forma mais clara recursos novos sendo enviados apenas para um node (elliot-03)...

Deletando o deployment: 'kubectl delete deployment "nome_do_deployment"'

Depois precisamos remover o 'Taint' do node elliot-02, para isso: 'kubectl taint node elliot-02 key1:NoSchedule-'

Saída: node/elliot-02 untainted

Subindo um novo deployment com arquivo, sem o bloco resources e com apenas 6 replicas: 'kubectl create -f "arquivo_deployment_editado.yaml"'

'kubectl get po -o wide'

NAME                                READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-7c9cfd4dc7-5d4fm   1/1     Running   0          9s    10.40.0.2   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-6mbw4   1/1     Running   0          9s    10.40.0.4   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-gcf8l   1/1     Running   0          9s    10.40.0.5   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-jkt9b   1/1     Running   0          9s    10.44.0.2   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-jpgdj   1/1     Running   0          9s    10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-pd4dw   1/1     Running   0          9s    10.40.0.1   elliot-02   <none>           <none>

Agora vamos adicionar o 'Taint' ao node elliot-02 novamente e vamos

Obs: Para voltar rapidamente ao comando que realizou a algum tempo, utilize o 'reverse-i-search' (CTRL + R) e coloque a palavra chave.
Para as demais possibilidades do comando utilize 'TAB'.

'kubectl taint node elliot-02 key1=value1:NoSchedule'

Vamos fazer o scale do deployment para 10 agora...

'kubectl scale --replicas=10 deployment nginx-deployment'

Vamos ver a alteração com 'kubectl get po -o wide'

NAME                                READY   STATUS    RESTARTS   AGE    IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-7c9cfd4dc7-5d4fm   1/1     Running   0          6m8s   10.40.0.2   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-64w5x   1/1     Running   0          43s    10.44.0.6   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-6mbw4   1/1     Running   0          6m8s   10.40.0.4   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-d477c   1/1     Running   0          43s    10.44.0.4   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-gcf8l   1/1     Running   0          6m8s   10.40.0.5   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-jkt9b   1/1     Running   0          6m8s   10.44.0.2   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-jpgdj   1/1     Running   0          6m8s   10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-pd4dw   1/1     Running   0          6m8s   10.40.0.1   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-pgb7r   1/1     Running   0          43s    10.44.0.3   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-vlk8z   1/1     Running   0          43s    10.44.0.5   elliot-03   <none>           <none>

Já tinhamos 5 pods no elliot-02, podemos ver que todos os demais criados foram para o elliot-03 que tinha apenas 1.

Vamos fazer de outra maneira...Vamos começar deletando o nosso deployment...Depois deletando o Taint...
Vamos criar de novo o nosso deployment: 'kubectl create -f arquivo_do_deployment.yaml'
Vamos subir um novo 'Taint', mas agora não será 'NoSchedule', será 'NoExecute'...

'kubectl taint node elliot-02 key1=value1:NoExecute'

Realizamos mais uma vez 'kubectl ger po -o wide'

e teremos:

NAME                                READY   STATUS    RESTARTS   AGE    IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-7c9cfd4dc7-4tsgs   1/1     Running   0          7s     10.44.0.5   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-4z7hx   1/1     Running   0          7s     10.44.0.6   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-8v84p   1/1     Running   0          7s     10.44.0.4   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-ft6jb   1/1     Running   0          7s     10.44.0.7   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-hpwmm   1/1     Running   0          5m4s   10.44.0.2   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-zjbsk   1/1     Running   0          7s     10.44.0.3   elliot-03   <none>           <none>

...Todos os Pods agora estão executando no node 'elliot-02'...

Neste exemplo vimos um ótimo recurso para realizar manutenção...
Obviamente os recursos do Kubernetes continuam rodando no node 'elliot-02', porém agora os nossos serviços já não estão mais...
Basicamente o que continua rodando é o 'kube-proxy' e o 'weave-net'...

Se tirarmos o 'Taint' do 'NoExecute' agora...

'kubectl taint node elliot-02 key1:NoExecute-'

'kubectl get po -o wide'

nginx-deployment-7c9cfd4dc7-4tsgs   1/1     Running   0          4m8s   10.44.0.5   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-4z7hx   1/1     Running   0          4m8s   10.44.0.6   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-8v84p   1/1     Running   0          4m8s   10.44.0.4   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-ft6jb   1/1     Running   0          4m8s   10.44.0.7   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-hpwmm   1/1     Running   0          9m5s   10.44.0.2   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-zjbsk   1/1     Running   0          4m8s   10.44.0.3   elliot-03   <none>           <none>

Podemos ver que ele não realiza o balanceamento de forma automatica...Para que façamos o rebalanceamento, podemos fazer o 'scale down' e depois o 'scale up'
novamente...

'kubectl scale --replicas=1 deployment "nome_do_deployment"'
'kubectl scale --replicas=6 deployment "nome_do_deployment"'

'kubectl get po -o wide'

NAME                                READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
nginx-deployment-7c9cfd4dc7-fm6lh   1/1     Running   0          5s    10.40.0.4   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-hpwmm   1/1     Running   0          13m   10.44.0.2   elliot-03   <none>           <none>
nginx-deployment-7c9cfd4dc7-pb99b   1/1     Running   0          5s    10.40.0.1   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-qh6m7   1/1     Running   0          5s    10.40.0.3   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-wzh8x   1/1     Running   0          5s    10.40.0.2   elliot-02   <none>           <none>
nginx-deployment-7c9cfd4dc7-xvfp8   1/1     Running   0          5s    10.40.0.5   elliot-02   <none>           <none>

Não fez um balanceamento mas ta usando os dois novamente rsss...

E para o Node Master?

No caso, para remover o taint, teríamos o comando: "kubectl taint nodes --all node-role.kubernetes.io/master-" (Esta chave só Node Master tem)

Mas se executassemos: "kubectl taint node --all key1=value1:NoExecute" ...ele executaria em todos os nodes este role, o que nos geraria um problema...
Mais nenhum nó no cluster teria Pods em execução.


-*- Day 3

-*- Deployment, Label e NodeSelector

Regra "search-n-replace": Utilizando o VIM - :%s/primeiro-deployment/segundo-deployment/g

Obs: Uma coisa bem importante de se pensar num projeto são os Labels, pois eles auxiliam muito na hora de criarmos filtros.

Como os dois primeiros exemplos utilizamos a Label "dc" nos nossos deployments, ela nada mais significa do que 'data center'

Para o primeiro deployment utilizamos 'dc: UK' para dizer que este primeiro deployment está rodando no data center do United Kingdom (Reino Unido)
Para o segundo deployment utilizamos 'dc: NL' para dizer que o segundo deployment está rodando no data center de Netherlands (Holanda)

Para vermos os pods que estão rodando em Netherlands: # kubectl get pods -l dc=NL

Para vermos uma lista de pos rodando com uma coluna com a label dc: # kubectl get pods -L dc

Para vermos um ReplicaSet que esteja rodando em Netherlands: # kubectl get replicaset -l dc=NL

Para vermos onde podemos adicionar Label: # kubectl label (??? Não funcionou na aula ???)

Caso nosso nó possua um tipo específico de disco, podemos colocar a label 'disk' e especificar o tipo de disco que tem nele
Ex: # kubectl label nodes elliot-02 disk=SSD
ou # kubectl label nodes elliot-03 disk=HDD

Para nós vermos se as mudanças tiveram efeito: # kubectl describe node elliot-02 ou elliot-03

Para listarmos todas as labels de determinado node: # kubectl label node elliot-02 --list

Labels são informações persistidas, o kubectl conversa com a API do Kubernetes e estas informações são salvas no etcd

Para substituirmos uma informação que já colocamos em um determinado label de um Node...
# kubectl label node elliot-03 disk=SSD --overwrite

Para informarmos que nosso Nodes estão rodando em regiões diferentes colocamos...
# kubectl label nodes elliot-03 dc=NL
# kubectl label nodes elliot-02 dc=UK

Criamos um terceiro deployment para usar o parâmetro 'nodeSelector', onde definimos qual característica terá que ter o
Node que irá hospedar o nosso deployment

No caso definimos o atributo 'disk=SSD' para que o deployment subisse no node 'elliot-02', depois editamos o parâmetro no
'nodeSelector' e colocamos 'disk=HDD', assim o deployment foi hospedado no node 'elliot-03'.

Para verificarmos que a alteração teve efeito rodamos o comando: # kubectl get pods -o wide

Saída:

NAME                                   READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
primeiro-deployment-5f6b5fc5b-bdl6s    1/1     Running   0          22h   10.40.0.1   elliot-02   <none>           <none>
segundo-deployment-86757897fd-rvqqt    1/1     Running   0          22h   10.40.0.2   elliot-02   <none>           <none>
terceiro-deployment-67cd449d46-lck4x   1/1     Running   0          22m   10.44.0.2   elliot-03   <none>           <none>

 Com o comando # kubectl describe deployment terceiro-deployment
 veremos que é o atual ReplicaSet do mesmo, mostrando que foi criado um ReplicaSet no novo Node com o novo tipo de disco
 enquando o replicaset que estava no outro node foi terminado...

 Saída:

NewReplicaSet:   terceiro-deployment-67cd449d46 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set terceiro-deployment-6bdc449bb4 to 1
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set terceiro-deployment-67cd449d46 to 1
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled down replica set terceiro-deployment-6bdc449bb4 to 0


-*- Mais um pouco sobre labels

Vemos que quando mudamos a característica do Node onde queremos que o nosso Deployment/ReplicaSet/Pod rode, ele termina o que já
existe e cria um novo no Node que tem as características informadas no manifesto.

Para que removamos uma determinada tag de todos os nosso nodes

# kubectl label nodes 'label'- --all
Ex: # kubectl label nodes dc- --all

Saída:

node/elliot-01 not labeled
node/elliot-02 labeled
node/elliot-03 labeled

Acima podemos ver que ele tirou a label 'dc' (data center) dos nodes elliot-02 e elliot-03 mas no elliot-01 não tinha esta
label.

Vamos fazer isso agora com a label 'disk'

# kubectl label nodes disk- --all

Saída:

label "disk" not found.
node/elliot-01 not labeled
node/elliot-02 labeled
node/elliot-03 labeled

Mesma coisa de antes, 2 e 3 removida, no 1 esta label não existia...Para conferirmos que de fato as labels foram removidas
# kubectl label node elliot-02 --list

Saída:

beta.kubernetes.io/os=linux
kubernetes.io/arch=amd64
kubernetes.io/hostname=elliot-02
kubernetes.io/os=linux
beta.kubernetes.io/arch=amd64

Podemos ver que as labels 'dc' e 'disk' foram removidas...

Então criamos o primeiro ReplicaSet isolado do deployment, isso não é aconselhado, mas estamos fazendo isso para fins didáticos.


-*- ReplicaSet

Criamos o novo arquivo YAML que está na pasta do dia 3 (primeiro-replicaset.yaml)
# kubectl create -f primeiro-replicaset.yaml
Obs: Foi necessário abaixo de 'replicas' adicionar o selector com os seguintes atributos 

selector:
  matchLabels:
    system: Giropops

Para estar sincronizado com as configurações do 'template' dos containers do ReplicaSet...

template:
    metadata:
      labels:
        system: Giropops

Na aula o Jeferson não precisou fazer isso, acredito que seja por causa da 'apiVersion'...

Depois de criado podemos conferir se estão rodando

# kubectl get replicaset

NAME                             DESIRED   CURRENT   READY   AGE
primeiro-deployment-5f6b5fc5b    1         1         1       23h
replica-set-primeiro             3         3         3       5m6s
segundo-deployment-86757897fd    1         1         1       23h
terceiro-deployment-67cd449d46   1         1         1       58m
terceiro-deployment-6bdc449bb4   0         0         0       63m

Podemos ver o 'replica-set-primeiro'....

Também poderiamos utilizar de forma abreviada o comando # kubectl get rs

Para vermos os pods deste replicaset rodando # kubectl get pods

NAME                                   READY   STATUS    RESTARTS   AGE
primeiro-deployment-5f6b5fc5b-bdl6s    1/1     Running   0          23h
replica-set-primeiro-c7c5l             1/1     Running   0          6m15s
replica-set-primeiro-crjn9             1/1     Running   0          6m15s
replica-set-primeiro-zjkm6             1/1     Running   0          6m15s
segundo-deployment-86757897fd-rvqqt    1/1     Running   0          23h
terceiro-deployment-67cd449d46-lck4x   1/1     Running   0          60m

Se removermos um dos pods, virá outro no lugar porque o ReplicaSet se encarregará de que suba outro pod identico no lugar
pois ele irá garantir que as especificações seja respeitadas, neste caso, quantidade de replicas=3...

spec:
  replicas: 3

Podemos editar este atributo...

# kubectl edit rs replica-set-primeiro

root@elliot-01:~/day-3# kubectl get pods
NAME                                   READY   STATUS    RESTARTS   AGE
primeiro-deployment-5f6b5fc5b-bdl6s    1/1     Running   0          23h
replica-set-primeiro-c7c5l             1/1     Running   0          9m29s
replica-set-primeiro-crjn9             1/1     Running   0          9m29s
replica-set-primeiro-x9b8j             1/1     Running   0          6s
replica-set-primeiro-zjkm6             1/1     Running   0          9m29s
segundo-deployment-86757897fd-rvqqt    1/1     Running   0          23h
terceiro-deployment-67cd449d46-lck4x   1/1     Running   0          63m

Se dermos um describe no replicaset veremos a atitude que ele tomou...

# kubectl describe rs replica-set-primeiro

"Normal  SuccessfulCreate  63s   replicaset-controller  Created pod: replica-set-primeiro-x9b8j"

Basicamente a função do ReplicaSet é cuidar dos Pods, garantir a quantidade dos pods, das replicas...
O ReplicaSet é o controller dos Pods
E o Deployment é o controller do ReplicaSet

Lembrando que definimos a label 'system:', então podemos listar os pods, adicionando uma nova coluna para verificarmos
qual o valor da label 'system' em cada Pod.

# kubectl get pods -L system

NAME                                   READY   STATUS    RESTARTS   AGE     SYSTEM
primeiro-deployment-5f6b5fc5b-bdl6s    1/1     Running   0          23h     
replica-set-primeiro-c7c5l             1/1     Running   0          14m     Giropops
replica-set-primeiro-crjn9             1/1     Running   0          14m     Giropops
replica-set-primeiro-x9b8j             1/1     Running   0          5m19s   Giropops
replica-set-primeiro-zjkm6             1/1     Running   0          14m     Giropops
segundo-deployment-86757897fd-rvqqt    1/1     Running   0          23h     
terceiro-deployment-67cd449d46-lck4x   1/1     Running   0          68m

Fazendo o edit para mudar a versão da imagem do nginx que utilizamos no nosso ReplicaSet de 1.7.9 para 1.15.0 ....

# kubectl edit replicaset replica-set-primeiro

Se realizarmos o comando 'kubectl get pods', poderemos ver que os pods não foram terminados, a versão não foi atualizada.
Deletamos um dos Pods para fazer um teste...

# kubectl delete pod replica-set-primeiro-c7c5l

E demos um describe no novo pod

# kubectl describe pod replica-set-primeiro-zhq55

Veremos que agora, neste POD a versão do nginx foi atualizada, mas nos outros Pods continua a versão antiga...Por isso a importancia
dos Deployments. Devemos sempre respeitar os controllers.

Se tivessemos um Deployment, e tivessemos feito esta alteração através dele, ele criaria um novo ReplicaSet com as informações
atualizadas, isso manteria o ambiente uniforme.

Até a questão de ser necessário derrubar o Pod para que ele pudesse pegar a nova versão, não foi a forma correta de fazer.

....

Deletando o ReplicaSet através do arquivo: # kubectl delete -f primeiro-replicaset.yaml

Deletamos todos os outros recursos criados no cluster nesta aula também através do arquivo...


-*- DaemonSet

Podemos comparar o DaemonSet com o ReplicaSet, a diferenca é que não definimos a quantidade que iremos ter.

Normalmente ele é utilizado quando precisamos que um determinado Pod precisa estar rodando em todos os Nodes do nosso cluster.

Aí poderíamos nos perguntar...."Por que no caso de ter um cluster com 3 nodes não criar um ReplicaSet com 3 replicas?"

O problema é que: Quando subimos um Deployment onde o Replicaset tem 3 replicas, não conseguimos definir que cada um dos pods irá
subir em um nó diferente. Ele sobe os pods onde estiver mais livre no melhor dos casos, depende de como estiver balanceado o cluster.

Por isso devemos utilizar o DaemonSet toda vez que precisarmos da garantia que temos um pod do serviço desejado rodando em cada um dos nodes.

Podemos notar que vimos nesta e na aula anterior dois conceitos que são utilizados no Docker...

O 'Replicated' que no Kubernetes é o 'ReplicaSet' e o 'Global', que no Kubernetes seria o 'DaemonSet'.

Vamos criar o 'primeiro-daemonset.yaml'...

Podemos verificar ele rodando com # kubectl get daemonset ou kubectl get ds
Ou, se nos lembrarmos que ele sobe pods assim como o ReplicaSet, podemos verificar com # kubectl get pods

NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
daemon-set-primeiro-fmjlf   1/1     Running   0          87s   10.40.0.1   elliot-02   <none>           <none>
daemon-set-primeiro-q6q4h   1/1     Running   0          87s   10.44.0.2   elliot-03   <none>           <none>

É legal observar que ele não subiu pod no node master (elliot-01) isto porque o nosso node master tem um 'Taint' configurado
Podemos verificar o taint com o # kubectl describe node elliot-01

"Taints: node-role.kubernetes.io/master:NoSchedule"

Ou seja, pods não podem ser escalados neste Node...

Precisamos remover os taints de um ou mais nodes com:

# kubectl taint node --all node-role.kubernetes.io/master-

ou passando o nome do node...

# kubectl taint node elliot-01 node-role.kubernetes.io/master-

Se utilizarmos agora o comando # kubectl get pods -o wide
veremos que ele escalou um novo pod do daemonset no node master (Lembrando que isto não é recomendado)

NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
daemon-set-primeiro-fmjlf   1/1     Running   0          11m   10.40.0.1   elliot-02   <none>           <none>
daemon-set-primeiro-m9d8h   1/1     Running   0          20s   10.32.0.4   elliot-01   <none>           <none>
daemon-set-primeiro-q6q4h   1/1     Running   0          11m   10.44.0.2   elliot-03   <none>           <none>

Podemos até fazer isso, quando queremos que tenha um pod de monitoramento, por exemplo, rodando nos nodes master...
Porém, para que nenhum outro tipo de pod seja escalado lá, devemos adicionar novamente o 'Taint', para evitar que pods
desnecessários e indesejados atrapalhem o desempenho dos 'control-plane'...

# kubectl taint node elliot-01 key1=value1:NoSchedule

Agora vamos mudar a versão do nginx no nosso daemonset, mas agora utilizaremos o 'set' e não o 'edit'

# kubectl set image ds daemon-set-primeiro nginx=nginx:1.15.0

Na aula foi passado que ele não derrubaria os Pods, mas foi o que aconteceu, os Pods foram derrubados e novos subiram com a imagem nova,
e no Node elliot-01 onde adicionamos novamente o 'taint' não subiu mais pods...

Vamos tentar com o edit...

# kubectl edit ds daemon-set-primeiro

Também com o edit, assim que editei e salvei, os pods do daemonset foram derrubados e novos com a versão nova do nginx subiram


-*- Rollouts e Rollbacks

Para verificarmos as revisões que temos disponíveis...

# kubectl rollout history daemonset daemon-set-primeiro

REVISION  CHANGE-CAUSE
3         <none>
4         <none>

Para verificarmos os detalhes de cada revisão...

# kubectl rollout history ds daemon-set-primeiro --revision=3

Pod Template:
  Labels:       system=Strigus
  Containers:
   nginx:
    Image:      nginx:1.7.9
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

# kubectl rollout history ds daemon-set-primeiro --revision=4

Pod Template:
  Labels:       system=Strigus
  Containers:
   nginx:
    Image:      nginx:1.15.0
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

Podemos verificar que a diferença entre uma revisão e outra é a imagem do nginx que alteramos nos nossos testes...

Verificamos a versão atual dos nossos pods com:

# kubectl describe pods daemon-set-primeiro-dbgq2 | grep -i nginx

nginx:
Image:          nginx:1.15.0
Image ID:       docker-pullable://nginx@sha256:62a095e5da5f977b9f830adaf64d604c614024bf239d21068e4ca826d0d629a4
Normal  Pulled     19m   kubelet            Container image "nginx:1.15.0" already present on machine
Normal  Created    19m   kubelet            Created container nginx
Normal  Started    19m   kubelet            Started container nginx

Podemos ver que a versão é '1.15.0' ...e se quisermos voltar a versão '1.7.0'?

Para nos lembrarmos da versão anterior...# kubectl rollout history daemonset daemon-set-primeiro --revision=3

Pod Template:
  Labels:       system=Strigus
  Containers:
   nginx:
    Image:      nginx:1.7.9
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

Vamos voltar para esta versão então...

# kubectl rollout undo daemonset daemon-set-primeiro --to-revision=3

Vamos verificar nos pods se voltaram a versão anterior...

# kubectl describe pods daemon-set-primeiro-j5h97 | grep -i nginx

Ele de fato voltou pra versão anterior...

nginx:
Image:          nginx:1.7.9
Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
Normal  Pulled     95s   kubelet            Container image "nginx:1.7.9" already present on machine
Normal  Created    95s   kubelet            Created container nginx
Normal  Started    95s   kubelet            Started container nginx

Obs: Na aula, talvez pela diferença de versões, a versão não foi atualizada por falta do parâmetro 'updateStrategy' 'type: RollingUpdate'

Porém, criei o manifesto a partir do daemonset que estava rodando e verifiquei que este parâmetro foi adicionado automaticamente...

# kubectl get ds daemon-set-primeiro -o yaml

---
  updateStrategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
---


-*- Exemplo Canary Deploy

Repositório do Canary Deploy: https://github.com/badtuxx/k8s-canary-deploy-example.git

Utilizamos os arquivos 'app-v1.yaml' e 'service-app.yaml' da pasta 'deploy-app-v1-playbook/roles/common/files'
e o arquivo 'app-v2-canary.yaml' da pasta 'canary-deploy-app-v2-playbook/roles/common/files'

Se dermos um 'cat' no service-app.yaml, veremos que o serviço serve para os dois Deployments...
O nosso selector tem como 'flag':

selector:
app: giropops

E os nosso dois deployments tem esta labels, por isso conseguimos rodar dois deployments utilizando o mesmo service.


Depois que aplicamos os dois arquivos de deployments, podemos verificar que o 'app-v1' tem 10 replicas enquanto o 'app-v2-canary'
tem 1 replica, ou seja, 10%....Isto para simular o deploy canario...

NAME          READY   UP-TO-DATE   AVAILABLE   AGE
giropops-v1   10/10   10           10          7h24m
giropops-v2   1/1     1            1           2m54s

Utilizando o 'kubectl get svc' veremos em que porta a aplicação está exposta...

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE
giropops     NodePort    10.108.186.59   <none>        80:32222/TCP,32111:32111/TCP   7h26m
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP                        15d

Se começarmos dar o 'curl http://172.31.7.162:32222/' em seguida

Veremos o acesso sendo balanceado entre as portas, e em determinado momento cairemos na versão 'v2'

root@elliot-01:~# curl http://172.31.7.162:32222/
Giropops App - Version 1.0.0
root@elliot-01:~# curl http://172.31.7.162:32222/
Giropops App - Version 1.0.0
root@elliot-01:~# curl http://172.31.7.162:32222/
Giropops App - Version 1.0.0
root@elliot-01:~# curl http://172.31.7.162:32222/
"Giropops App - Version 2.0.0"


Se colocarmos num Grafana, veremos que realmente que 10% das requisições apenas estão chegando para v2.

Depois de um tempo veremos que esta versão é estável e podemos colocá-la em full em produção...

Para fazer isso utilizaremos o arquivo que está em 'deploy-app-v2-playbook/roles/common/files'

'app-v2.yaml'


Obs: Neste arquivo tem especificações adicionais do Kubernetes que veremos com mais calma mais a frente, que é o
'LivenessProbe' e o 'ReadinessProbe'. Eles servem para fazer o 'check' para sabermos quando a aplicação estará pronta para ser consumida


Obs2: Já temos o 'giropops-v2' rodando, por isso o 'kubectl create -f' não funcionará, precisamos utilizar o 'kubectl apply -f'.

Depois de aplicá-lo teremos 10 replicas de cada versão...

NAME                           READY   STATUS    RESTARTS        AGE     IP           NODE        NOMINATED NODE   READINESS GATES
giropops-v1-65df785568-5j6rl   1/1     Running   1 (4h40m ago)   8h      10.40.0.2    elliot-02   <none>           <none>
giropops-v1-65df785568-djvk5   1/1     Running   1 (4h40m ago)   8h      10.40.0.4    elliot-02   <none>           <none>
giropops-v1-65df785568-hcntq   1/1     Running   1 (4h40m ago)   8h      10.40.0.6    elliot-02   <none>           <none>
giropops-v1-65df785568-j2rmr   1/1     Running   1 (56m ago)     8h      10.40.0.7    elliot-02   <none>           <none>
giropops-v1-65df785568-k9mkv   1/1     Running   1 (4h40m ago)   8h      10.44.0.1    elliot-03   <none>           <none>
giropops-v1-65df785568-pqqbf   1/1     Running   1 (4h40m ago)   8h      10.40.0.5    elliot-02   <none>           <none>
giropops-v1-65df785568-pt297   1/1     Running   1 (4h40m ago)   8h      10.44.0.4    elliot-03   <none>           <none>
giropops-v1-65df785568-tcwcx   1/1     Running   1 (4h40m ago)   8h      10.44.0.3    elliot-03   <none>           <none>
giropops-v1-65df785568-w2qc6   1/1     Running   1 (4h40m ago)   8h      10.40.0.3    elliot-02   <none>           <none>
giropops-v1-65df785568-zs66m   1/1     Running   1 (4h40m ago)   8h      10.40.0.1    elliot-02   <none>           <none>
giropops-v2-78d77495cc-5gkbk   1/1     Running   0               5m1s    10.44.0.5    elliot-03   <none>           <none>
giropops-v2-78d77495cc-c2fkq   1/1     Running   0               5m1s    10.40.0.9    elliot-02   <none>           <none>
giropops-v2-78d77495cc-kfsvd   1/1     Running   0               5m1s    10.44.0.11   elliot-03   <none>           <none>
giropops-v2-78d77495cc-mbf92   1/1     Running   0               4m56s   10.44.0.6    elliot-03   <none>           <none>
giropops-v2-78d77495cc-pcs5t   1/1     Running   0               4m57s   10.44.0.9    elliot-03   <none>           <none>
giropops-v2-78d77495cc-qfk4c   1/1     Running   0               5m1s    10.44.0.7    elliot-03   <none>           <none>
giropops-v2-78d77495cc-r6tqf   1/1     Running   0               5m1s    10.40.0.15   elliot-02   <none>           <none>
giropops-v2-78d77495cc-rlxq2   1/1     Running   0               4m57s   10.40.0.14   elliot-02   <none>           <none>
giropops-v2-78d77495cc-skqmx   1/1     Running   0               4m58s   10.44.0.12   elliot-03   <none>           <none>
giropops-v2-78d77495cc-zfgkj   1/1     Running   0               4m55s   10.40.0.11   elliot-02   <none>           <none>


Agora precisamos começar a fazer o 'scale down' dos pods v1.

'kubectl scale deployment --replicas=3 giropops-v1'

depois...

'kubectl scale deployment --replicas=1 giropops-v1'

por fim...

'kubectl delete deployment giropops-v1'

agora se fizermos 'curl http://172.31.7.162:32222/' repetitas vezes, ele só nos trará a versão 2...

Se dermos um 'kubectl rollout' veremos os tipos que podemos fazer 'rollback'

Valid resource types include:

*  deployments
*  daemonsets
*  statefulsets

'kubectl rollout history deployments' para vermos o histórico de revisões...

REVISION  CHANGE-CAUSE
1         <none>
2         <none>

'kubectl rollout history deployment giropops-v2'

'kubectl rollout history deployment giropops-v2 revision=1'

deployment.apps/giropops-v2 with revision #1
Pod Template:
  Labels:       app=giropops
        pod-template-hash=57674bdf77
        version=2.0.0
  Annotations:  prometheus.io/port: 32111
        prometheus.io/scrape: true
  Containers:
   giropops:
    Image:      linuxtips/nginx-prometheus-exporter:2.0.0
    Ports:      80/TCP, 32111/TCP
    Host Ports: 0/TCP, 0/TCP
    Environment:
      VERSION:  2.0.0
    Mounts:     <none>
  Volumes:      <none>

  'kubectl rollout history deployment giropops-v2 revision=2'

  deployment.apps/giropops-v2 with revision #2
  Pod Template:
    Labels:       app=giropops
          pod-template-hash=78d77495cc
          version=2.0.0
    Annotations:  prometheus.io/port: 32111
          prometheus.io/scrape: true
    Containers:
     giropops:
      Image:      linuxtips/nginx-prometheus-exporter:2.0.0
      Ports:      80/TCP, 32111/TCP
      Host Ports: 0/TCP, 0/TCP
      Liveness:   http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
      Readiness:  http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
      Environment:
        VERSION:  2.0.0
      Mounts:     <none>
    Volumes:      <none>

Basicamente mudamos de uma revisão para outra a quantidade de réplicas e adicionamos o liveness e o readiness probe...

Lembrando que se tivessemos deixado pelo menos uma replica da versão 1 através do scale down teriamos a chance ainda de fazer rollback.


-*- Um pouco mais de Rollouts e Rollbacks

'kubectl describe deploy giropops-v2'

Name:                   giropops-v2
Namespace:              default
CreationTimestamp:      Mon, 15 Nov 2021 23:16:38 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=giropops
Replicas:               10 desired | 10 updated | 10 total | 10 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:       app=giropops
                version=2.0.0
  Annotations:  prometheus.io/port: 32111
                prometheus.io/scrape: true
  Containers:
   giropops:
    Image:       linuxtips/nginx-prometheus-exporter:2.0.0
    Ports:       80/TCP, 32111/TCP
    Host Ports:  0/TCP, 0/TCP
    Liveness:    http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:   http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      VERSION:  2.0.0
    Mounts:     <none>
  Volumes:      <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   giropops-v2-78d77495cc (10/10 replicas created)
Events:
  Type    Reason             Age                From                   Message
  ----    ------             ----               ----                   -------
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled up replica set giropops-v2-57674bdf77 to 10
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled up replica set giropops-v2-78d77495cc to 3
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled down replica set giropops-v2-57674bdf77 to 8
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled up replica set giropops-v2-78d77495cc to 5
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled down replica set giropops-v2-57674bdf77 to 7
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled up replica set giropops-v2-78d77495cc to 6
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled down replica set giropops-v2-57674bdf77 to 6
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled up replica set giropops-v2-78d77495cc to 7
  Normal  ScalingReplicaSet  43m                deployment-controller  Scaled down replica set giropops-v2-57674bdf77 to 5
  Normal  ScalingReplicaSet  43m (x8 over 43m)  deployment-controller  (combined from similar events): Scaled down replica set giropops-v2-57674bdf77 to 0


Vamos editar o app-v2.yaml adicionando a estratégia para update...

strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 2
    maxUnavailable: 3

No caso estamos dizendo que a estratégia definida ali é pra quando 'rolar a atualização'...
Os atributos:

  maxSurge: O quanto de pods que podemos ultrapassar da quantidade de réplicas, ou seja, se a quantidade de réplicas é 10 na hora
              do update o Kubernetes elevará para 12, para poder ir matando das que já existiam...

  maxUnavailable: A quantidade máxima de pods que poderão ficar com status de inacessível...

  Obs: Podemos trocar estes valores por porcentagens (10% por exemplo)

Precisamos agora deletar o deployment que já existe para aplicar o novo deployment com o strategy

e então criá-lo novamente...

Depois de criá-lo vamos editar a versão para ver acontecer a estratégia nova de atualização...

'kubectl edit deploy giropops-v2'

Mudamos o que estava na versão 2 para versão 1...

Obs: Se quisermos ver de forma mais lenta o que acontece, basta mudarmos o 'maxSurge' para 2 e o 'maxUnavailable' para 3 utilizarmos o comando 
      "kubectl rollout status deployment giropops-v2" que poderemos ver o status da atualização....

      Waiting for deployment "giropops-v2" rollout to finish: 3 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 3 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 3 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 3 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 4 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 4 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 5 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 5 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 5 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 6 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 6 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 6 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 6 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 7 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 7 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 7 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 7 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 7 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 8 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 8 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 8 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 8 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 8 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 9 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 9 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 9 out of 10 new replicas have been updated...
      Waiting for deployment "giropops-v2" rollout to finish: 2 old replicas are pending termination...
      Waiting for deployment "giropops-v2" rollout to finish: 2 old replicas are pending termination...
      Waiting for deployment "giropops-v2" rollout to finish: 2 old replicas are pending termination...
      Waiting for deployment "giropops-v2" rollout to finish: 1 old replicas are pending termination...
      Waiting for deployment "giropops-v2" rollout to finish: 1 old replicas are pending termination...
      Waiting for deployment "giropops-v2" rollout to finish: 1 old replicas are pending termination...
      Waiting for deployment "giropops-v2" rollout to finish: 9 of 10 updated replicas are available...
      deployment "giropops-v2" successfully rolled out


Agora vamos deletar o deployment mais uma vez e mudar o 'maxSurge' e o 'maxUnavailable' para 5

Realizamos o create -f com o arquivo, e depois editamos novamente a versão no deployment....

Depois salvamos, saímos e realizamos o comando "kubectl rollout status deployment giropops-v2"

Waiting for deployment "giropops-v2" rollout to finish: 4 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 4 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 4 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 3 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 3 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 3 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "giropops-v2" rollout to finish: 5 of 10 updated replicas are available...
Waiting for deployment "giropops-v2" rollout to finish: 6 of 10 updated replicas are available...
Waiting for deployment "giropops-v2" rollout to finish: 7 of 10 updated replicas are available...
Waiting for deployment "giropops-v2" rollout to finish: 8 of 10 updated replicas are available...
Waiting for deployment "giropops-v2" rollout to finish: 9 of 10 updated replicas are available...
deployment "giropops-v2" successfully rolled out

A atualização foi muito mais rápida...

-*- Aula ao Vivo 30/05/20 - Descomplicando o Kubernetes - Day 3

-*- Preparatório CKA -Aula 01 - #BondeDoCKA

-*- Preparatório CKA -Aula 02 - #BondeDoCKA

-*- Preparatório CKA -Aula 03 - #BondeDoCKA


-*- Day 4

-*- Empty dir

Começamos criando um um yaml de um Pod para que pudéssemos testar o emptyDir...

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    name: busy
    command:
      - sleep
      - "3600"
    volumeMounts:
    - mountPath: /giropops
      name: giropops-dir
  volumes:
  - name: giropops-dir
    emptyDir: {}

Obs: As vezes ficamos confusos com o conteúdo do yaml de um pod, deployment etc, pois parece que estamos repetindo informações.
Mas na verdade sempre temos que ficar muito atentos com o escopo. No caso acima parece que estamos repetindo informações sobre
o volume, mas na verdade temos o 'volumeMounts' que é o volume que estamos criando dentro do container (atenção no escopo) e temos
o Volumes, que de fato cria o volume no nosso Pod...Ou seja, um para criação no volume no Pod, o outro para referenciar dentro
do container onde estará o 'bind' do mesmo.

NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          6s

root@elliot-01:~/day-4# kubectl describe pod busybox
Name:         busybox
Namespace:    default
Priority:     0
Node:         elliot-02/172.31.4.238
Start Time:   Sat, 25 Dec 2021 17:45:39 +0000
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.40.0.1
IPs:
  IP:  10.40.0.1
Containers:
  busy:
    Container ID:  docker://db9d05521db54ebf138e0ac0ada67206e42ce1ec994f32e4f15add929795f942
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:b5cfd4befc119a590ca1a81d6bb0fa1fb19f1fbebd0397f25fae164abe1e8a6a
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
      3600
    State:          Running
      Started:      Sat, 25 Dec 2021 17:45:41 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /giropops from giropops-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzpqc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  giropops-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-fzpqc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  15s   default-scheduler  Successfully assigned default/busybox to elliot-02
  Normal  Pulling    15s   kubelet            Pulling image "busybox"
  Normal  Pulled     14s   kubelet            Successfully pulled image "busybox" in 810.565223ms
  Normal  Created    14s   kubelet            Created container busy
  Normal  Started    14s   kubelet            Started container busy

Podemos ver na descrição acima no Pod, que o seu volume foi criado com o aviso de que o mesmo é efemero e será destruído junto
ao Pod caso aconteça a sua deleção (a temporary directory that shares a pod's lifetime)...Assim como o volume giropops dentro do container 
que mostra a informação que é leitura e escrita (rw)

Se entrarmos no container veremos o volume lá criado...

root@elliot-01:~/day-4# kubectl exec -it busybox -- sh
/ # ls
bin       dev       etc       giropops  home      proc      root      sys       tmp       usr       var
/ # cd giropops
/giropops # touch teste
/giropops # ls
teste
/giropops # 

Também aproveitamos para criar um arquivo de teste chamado 'teste'...


-*- Persistent Volume

Para verificarmos agora onde foi gravado o nó o diretório e arquivos que criamos...

root@elliot-01:~/day-4# kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          30m   10.40.0.1   elliot-02   <none>           <none>

Verificamos com este comando que o nosso pod 'busybox' está no Node 'elliot-02'...

Após entrarmos neste nós vamos procurar o diretório '/var/lib/kubelet/pods'

root@elliot-02:/var/lib/kubelet/pods# ls
14048a06-ccff-4bc9-8b18-17437008e5ae  e33face0-fa0b-4458-bdfc-7cc8f7225394
39bfa93d-7f06-461a-b415-9e024db46104

Vemos alguns diretórios criados...

Para encontrar o nosso giropops-dir:

root@elliot-02:/var/lib/kubelet/pods# find . -iname "giropops-dir"
./39bfa93d-7f06-461a-b415-9e024db46104/plugins/kubernetes.io~empty-dir/giropops-dir
./39bfa93d-7f06-461a-b415-9e024db46104/volumes/kubernetes.io~empty-dir/giropops-dir

Temos os subdiretórios 'plugins' e 'volumes' com giropops-dir dentro...Vamos verificar o 'volumes'...

root@elliot-02:/var/lib/kubelet/pods# cd ./39bfa93d-7f06-461a-b415-9e024db46104/volumes/kubernetes.io~empty-dir/giropops-dir

root@elliot-02:/var/lib/kubelet/pods/39bfa93d-7f06-461a-b415-9e024db46104/volumes/kubernetes.io~empty-dir/giropops-dir# ls
teste

Verificamos que nosso arquivo 'teste' está ali...

Podemos fazer o teste de, deletar o pod que haviamos criado e ir dando 'ls' no diretório do Node para ver que ele só existe enquanto o Pod existe...

Obs: Pra que utilizaríamos um 'emptydir', no caso um volume efemero? O exemplo de uso dado pelo instrutor é utilizar este diretório como agregador de Logs, onde
as informações era enviadas para o 'greylog'....

Porém como este recurso que vimos não persiste dados, é aí que entra os PVs e os PVCs...

'PV' ou 'Persistent Volume' é como se pegassemos um disco ou compartilhamento e aquilo pode ser usado pelo cluster, porém para que ele tenha utilidade ele precisa ser atachado a um pod
por isso precisamos fazer o 'PVC' ou 'Persistent Volume Claim'...

Utilizaremos o NFS (Network File System) para compartilhar volumes e diretórios entre os Nodes...

Dentro do node master:
# apt-get install nfs-kernel-server

Dentro dos nodes workers:
# apt-get install nfs-common


Obs: Criação de diretório/volume e configurações do export no Node Master
Criando o diretório que iremos utilizar para usar como file system..
# mkdir /opt/dados

Alterando permissões
# chamod 1777 /opt/dados

Agora dentro do arquivos exports vamos especificar qual volume, file system queremos exportar
# vim /etc/exports

# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#
/opt/dados *(rw,sync,no_root-squash,subtree_check)

Obs: Colocamos um asterísco pois queremos que qualquer Node da rede tenha acesso, mas poderíamos ali especificar um IP exclusivo para acesso...
Obs2: Na frente as opções sobre o volume...

Agora o comando para executar o que definimos no arquivo, é como se lessemos o arquivo e exportassemos o que está ali...
# exportfs -ar

Agora para verificar se tudo deu certo, podemos ir até a segunda máquina e executar o seguinte comando:
# showmount -e 172.31.7.162

Saída:

Export list for 172.31.7.162:
/opt/dados *

Para criar o nosso primeiro pv:
# vim primeiro-pv.yaml
(Este yaml se encontra na pasta day-4)


-*- Persistent Volume - Parte 02

Temos algumas coisas novas neste YAML, como por exemplo o valor '- ReadWriteMany' no atributo 'accessModes'
No 'AccessModes' poderiamos ter três valores diferentes...

- (RWX) ReadWriteMany - Volume pode ser montado por vários nós para Read & Write
- (RWO) ReadWriteOnce - Volume só pode ser montado por um Node para Read & Write
- (ROX) ReadOnlyMany - Volume pode ser montado por vários apenas para leitura

'VolumeReclaimPolicy', politica sobre o que acontece com o volume após a exclusão do mesmo...deixamos com o valor 'Retain', mas 
temos outros valores como: 'Delete', 'Recycle'.
Sobre 'Retain' estamos falando sobre os dados que foram gravados na pasta indicada...

Nas informações de 'nfs' para baixo estamos específicando qual é o tipo e onde está o volume...Flag 'false' no 'ReadOnly' pois
não será um volume apenas de leitura...

Tendo definido o YAML:
# kubectl create -f primeiro-pv.yaml

Vamos ver o que se passa com este pv:
# kubectl get pv

NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
primeiro-pv   1Gi        RWX            Retain           Available                                   45s

Damos um describe no PV:
# kubectl describe pv primeiro-pv

Saída:

Name:            primeiro-pv
Labels:          <none>
Annotations:     <none>
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    
Status:          Available
Claim:           
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   <none>
Message:         
Source:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    172.31.7.162
    Path:      /opt/dados
    ReadOnly:  false
Events:        <none>



Agora temos que criar o PVC - PersistentVolumeClaim (Arquivo primeiro-pvc.yaml na pasta Day-4)

Unica configuração peculiar é o request de 800 mega do recurso criado:

resources:
  requests:
    storage: 800Mi

Depois de adicionar o conteúdo ao arquivo...
# kubectl create -f primeiro-pvc.yaml

Verificamos o pvc criado:
# kubectl get pvc

NAME           STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
primeiro-pvc   Bound    primeiro-pv   1Gi        RWX                           17s

Obs: Podemos perceber que ele fez o 'claim' com o 'primeiro-pv' criando anteriormente

E então damos um describe nele para verificar se encontramos algum dado sobre isso:
# kubectl describe pvc primeiro-pvc

Name:          primeiro-pvc
Namespace:     default
StorageClass:  
Status:        Bound
Volume:        primeiro-pv
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      1Gi
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       <none>
Events:        <none>

Vemos que ele esta com status 'bound' com o 'primeiro-pvc'...

Esta mesma informação podemos verificar no 'primeiro-pv'

# kubectl get pv primeiro-pv

NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
primeiro-pv   1Gi        RWX            Retain           Bound    default/primeiro-pvc                           22m

Já está conectado com 'default/primeiro-pvc'

Se dermos um describe no PV agora...

# kubectl describe pv primeiro-pv

Name:            primeiro-pv
Labels:          <none>
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    
Status:          Bound
Claim:           default/primeiro-pvc
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   <none>
Message:         
Source:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    172.31.7.162
    Path:      /opt/dados
    ReadOnly:  false
Events:        <none>

Mesmas informações...

Agora vamos utilizar estes recursos, vamos criar um deployment...

# vim deployment-nfs-pv.yaml


-*- Persistent Volume - Parte 03

Criamos o Deployment com a declaração no POD do volume que irá ser montado...

spec:
containers:
- image: nginx
  imagePullPolicy: Always
  name: nginx
  volumeMounts:
  - name: nfs-pv
    mountPath: /giropops
  resources: {}
  terminationMessagePath: /dev/termination-log
  terminationMessagePolicy: File
volumes:
- name: nfs-pv
  persistentVolumeClaim:
    claimName: primeiro-pvc

O arquivo 'deployment-nfs-pv.yaml' está na pasta 'day-4'

Criamos o arquivo: # kubectl create -f deployment-nfs-pv.yaml

# kubectl get deployment

NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           4m2s

# kubectl get pods

NAME                     READY   STATUS    RESTARTS   AGE
nginx-5666597d96-dmtm5   1/1     Running   0          4m25s

# kubectl describe deployment nginx
Obs: Destacados entre aspas informações sobre volume e mount points...

Name:                   nginx
Namespace:              default
CreationTimestamp:      Mon, 27 Dec 2021 17:51:50 +0000
Labels:                 run=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               run=nginx
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:  run=nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    'Mounts:'
      "/giropops from nfs-pv (rw)"
  Volumes:
   nfs-pv:
    'Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)''
    'ClaimName:  primeiro-pvc'
    ReadOnly:   false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-5666597d96 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  4m40s  deployment-controller  Scaled up replica set nginx-5666597d96 to 1


# kubectl describe pod nginx-5666597d96-dmtm5

Obs: Destacado entre aspas informações sobre volumes e mount points

Name:         nginx-5666597d96-dmtm5
Namespace:    default
Priority:     0
Node:         elliot-02/172.31.4.238
Start Time:   Mon, 27 Dec 2021 17:51:50 +0000
Labels:       pod-template-hash=5666597d96
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.40.0.1
IPs:
  IP:           10.40.0.1
Controlled By:  ReplicaSet/nginx-5666597d96
Containers:
  nginx:
    Container ID:   docker://75a0ea2a9c2f7735c25245e143c55f9bcbdca0faeb981ba054cdfd34a34d97f4
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:366e9f1ddebdb844044c2fafd13b75271a9f620819370f8971220c2b330a9254
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Mon, 27 Dec 2021 17:51:56 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    'Mounts:''
      "/giropops from nfs-pv (rw)"
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-47jvt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  'nfs-pv:''
    'Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)''
    'ClaimName:  primeiro-pvc'
    'ReadOnly:   false'
  kube-api-access-47jvt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned default/nginx-5666597d96-dmtm5 to elliot-02
  Normal  Pulling    5m19s  kubelet            Pulling image "nginx"
  Normal  Pulled     5m15s  kubelet            Successfully pulled image "nginx" in 4.424821s
  Normal  Created    5m14s  kubelet            Created container nginx
  Normal  Started    5m14s  kubelet            Started container nginx

Vamos entrar no container e criar uns arquivos na pasta mapeada para o volume...

# kubectl exec -it nginx-5666597d96-dmtm5 -- sh

# ls
bin   dev                  docker-entrypoint.sh  giropops  lib    media  opt   root  sbin  sys  usr
boot  docker-entrypoint.d  etc                   home      lib64  mnt    proc  run   srv   tmp  var

Obs: Podemos verificar que a pasta giropops existe ali...Vamos criar os arquivos dentro dela...


# bash
root@nginx-5666597d96-dmtm5:/# cd giropops/
root@nginx-5666597d96-dmtm5:/giropops# touch TESTE GIROPOPS STRIGUS GIRUS

Agora vamos sair do container e verificar se os arquivos foram criados no Node na pasta /opt/dados, conforme especificamos
na criação do PV...

root@nginx-5666597d96-dmtm5:/giropops# exit
# exit

root@elliot-01:~/day-4# cd /opt/dados
root@elliot-01:/opt/dados# ls
GIROPOPS  GIRUS  STRIGUS  TESTE

Podemos ver que os arquivos foram criados...

Obs: Se executarmos o comando "kubectl get pods -o wide", veremos que o pod está rodando no Node elliot-02...

NAME                     READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES
nginx-5666597d96-dmtm5   1/1     Running   0          21m   10.40.0.1   elliot-02   <none>           <none>

Ou seja, o próprio Kubernetes montou o compartilhamento lá, nós mesmos só verificamos todas as criações e alterações no Node 1

Obs2: No Pod chamamos o diretório do volume de 'giropops', no Node de 'dados', mas o mapeamento que importa...


-*- Cronjobs


Primeiro para nos situar no agendamento (schedule) vamos ver a que os campos se referem...

"*/1 * * * *" = "Minutos Hora Dia Mes Dia-da-Semana Comando"

"*/1" = Executar a cada minuto (* = Qualquer minuto)

Minutos = 0-59
Horas = 0-23
Dia = 1-31
Mês = 1-12
Dia da Semana = 0-7 (0 - Domingo, 1 - Segunda, 2 - Terça, 3 - Quarta, 4 - Quinta, 5 - Sexta, 6 - Sábado, 7 - Domingo)

Exemplo simples:

Caso queiramos que uma tarefa execute a cada minuto, nas horas 1 e 2 da manhã, do dia 1 ao dia 10 de um determinado mês
Poderíamos definir assim os 'coringas'

"*/1 1,2 1-10"


Sobre o comando que específicamos no 'args' do arquivo 'primeiro-cronjob.yaml'

args:
- /bin/sh
- -c
- date; echo Bem Vindo ao Descomplicando Kubernetes - LinuxTips VAIIII; sleep 30

"- /bin/sh" "-c" Sempre serão os que executarão o comando...

- date (mostra a data) e depois vai imprimir a mensagem que está na frente e então, sleep 30 (espera 30 segundos antes de executar de novo)

Vamos criar o cronjob agora...

# kubectl create -f primeiro-cronjob.yaml

root@elliot-01:/# kubectl get cronjob
NAME            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
giropops-cron   */1 * * * *   False     0        <none>          9s


# kubectl describe cronjob giropops-cron

NAME            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
giropops-cron   */1 * * * *   False     0        <none>          9s


root@elliot-01:/# kubectl describe cronjob giropops-cron
Name:                          giropops-cron
Namespace:                     default
Labels:                        <none>
Annotations:                   <none>
Schedule:                      */1 * * * *
Concurrency Policy:            Allow
Suspend:                       False
Successful Job History Limit:  3
Failed Job History Limit:      1
Starting Deadline Seconds:     <unset>
Selector:                      <unset>
Parallelism:                   <unset>
Completions:                   <unset>
Pod Template:
  Labels:  <none>
  Containers:
   giropops-cron:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Args:
      /bin/sh
      -c
      date; echo Bem Vindo aao Descomplicando Kubernetes - LinuxTips VAIIII; sleep 30
    Environment:     <none>
    Mounts:          <none>
  Volumes:           <none>
Last Schedule Time:  Mon, 27 Dec 2021 18:54:00 +0000
Active Jobs:         giropops-cron-27343854
Events:
  Type    Reason            Age   From                Message
  ----    ------            ----  ----                -------
  Normal  SuccessfulCreate  83s   cronjob-controller  Created job giropops-cron-27343853
  Normal  SawCompletedJob   51s   cronjob-controller  Saw completed job: giropops-cron-27343853, status: Complete
  Normal  SuccessfulCreate  23s   cronjob-controller  Created job giropops-cron-27343854


Podemos ver se ele está ativo, se ja executou alguma vez...

# kubectl get jobs

Obs: O Cronjob executa os Jobs...

NAME                     COMPLETIONS   DURATION   AGE
giropops-cron-27343853   1/1           32s        3m19s
giropops-cron-27343854   1/1           32s        2m19s
giropops-cron-27343855   1/1           32s        79s
giropops-cron-27343856   0/1           19s        19s

Podemos colocar para observar quando os jobs são executados...

# kubectl get jobs --watch

Podemos verificar no Pod o log da tarefa que está sendo executada e após isto o pod "morre", pois completa a task...

root@elliot-01:/# kubectl get pods
NAME                              READY   STATUS      RESTARTS   AGE
giropops-cron-27343857--1-mkb6r   0/1     Completed   0          3m8s
giropops-cron-27343858--1-cpkj6   0/1     Completed   0          2m8s
giropops-cron-27343859--1-vsx5d   0/1     Completed   0          68s
giropops-cron-27343860--1-979jk   1/1     Running     0          8s

Pegamos os logs do Pod que ainda está 'Running'...

root@elliot-01:/# kubectl logs giropops-cron-27343860--1-979jk
Mon Dec 27 19:00:02 UTC 2021
Bem Vindo aao Descomplicando Kubernetes - LinuxTips VAIIII


-*- Secrets

Começamos criando um arquivo secret.txt...

# echo -n "giropops strigus girus" > secret.txt

Agora à partir deste arquivo vamos criar a nossa secret

# kubectl create secret generic my-secret --from-

Obs: Com este comando acima verificamos quais são as opções '--from'

--from-env-file= / --from-file= / --from-literal=

No caso neste primeiro exemplo utilizaremos o --from-file=

# kubectl create secret generic my-secret --from-file=secret.txt

# kubectl describe secret my-secret

Name:         my-secret
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
secret.txt:  22 bytes

Para pegarmos esta secret criada em forma de yaml

# kubectl get secret my-secret -o yaml

Utilizamos este arquivo para nos basear para criar nosso arquivos de secret...

Obs: Quando criamos secret o ideal é que criemos elas por linha de comando, pois se ficarmos criando arquivos de secret
estaremos resolvendo um problema criando outro...

Só que mesmo criando desta forma, por linha de comando, ainda temos um problema...
Se olharmos na saída do nosso ultimo comando, 'kubectl get secret my-secret -o yaml', veremos que na frente de secret.txt
nós temos uma chave codificada/token...

data:
  secret.txt: Z2lyb3BvcHMgc3RyaWd1cyBnaXJ1cw==

Se pegarmos esta chave e executarmos o comando:

# echo 'Z2lyb3BvcHMgc3RyaWd1cyBnaXJ1cw==' | base64 --decode

Ele vai mostrar a saída:

giropops strigus girus

Que nada mais é do que a chave que definimos no nosso arquivo 'secret.txt'

Agora vamos criar um pod para testar a secret...

# vim pod-secret.yaml

Obs: Este arquivo se encontra na pasta 'day-4'

# kubectl create -f pod-secret.yaml


-*- Secrets - Parte 02

Obs: Podemos reparar agora que o tipo de volume que estamos criando é 'secret'

Definimos este volume pois a secret será um arquivo no nosso container...'/tmp/giropops/'

Vamos criar o pod...

# kubectl create -f pod-secret.yaml

# kubectl get pods

NAME          READY   STATUS    RESTARTS   AGE
test-secret   1/1     Running   0          5s

# kubectl exec -it test-secret -- sh

/ # ls
bin   dev   etc   home  proc  root  sys   tmp   usr   var
/ # cd tmp
/tmp # ls
giropops
/tmp # cd giropops
/tmp/giropops # 
/tmp/giropops # ls
secret.txt
/tmp/giropops # cat secret.txt

"giropops strigus girus"


Agora vamos criar a secret do modo literal...

# kubectl create secret generic my-literal-secret --from-literal user=linuxtips --from-literal password=catota

root@elliot-01:~/day-4# kubectl describe secret my-literal-secret
Name:         my-literal-secret
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
user:      9 bytes
password:  6 bytes

Obs: Podemos observar que agora temos apenas o tamanho de 'user' e 'password' e mais nenhuma informação...

Vamos verificar como seria a saída do yaml desta secret...

# kubectl get secret my-literal-secret -o yaml

apiVersion: v1
data:
  'password: Y2F0b3Rh'
  'user: bGludXh0aXBz'
kind: Secret
metadata:
  creationTimestamp: "2021-12-27T20:08:03Z"
  name: my-literal-secret
  namespace: default
  resourceVersion: "608817"
  uid: 1cd1083a-d225-4269-824a-f08195223884
type: Opaque

Vamos verificar a saída com o decode...

# echo 'Y2F0b3Rh' | base64 --decode
catota

# echo 'bGludXh0aXBz' | base64 -- decode
linuxtips

Agora veremos uma nova forma de entregar secrets para os containers...

Começando no fim desta aula com a criação do arquivo 'pod-secret-env.yaml'


-*- Secrets - Parte 03


Temos uma diferença no manifesto agora...Nomeamos as variáveis no arquivo...

env:
  - name: MEU_USERNAME
    valueFrom:
      secretKeyRef:
        name: my-literal-secret
        key: user
  - name: MEU_PASSWORD
    valueFrom:
      secretKeyRef:
        name: my-literal-secret
        key: password

'MEU_USERNAME' e 'MEU_PASSWORD'...'valueFrom' => "Valor que é buscado de"

'secretKeyRef' => "Referente à uma secretKey"

'name' => "Nome da secret key" => "my-literal-secret" (vide: kubectl get secrets)

"key" => "user" e "password" => "Os campos definidos lá"

Vamos criar o pod...

# kubectl create -f pod-secret-env.yaml

root@elliot-01:~/day-4# kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
test-secret-env   1/1     Running   0          4s

Vamos acessar o container para verificar se correu tudo bem...

# kubectl exec -it test-secret-env -- sh

Dentro do container podemos executar o comando # set

Saída:

HISTFILE='/root/.ash_history'
HOME='/root'
HOSTNAME='test-secret-env'
IFS=' 
'
KUBERNETES_PORT='tcp://10.96.0.1:443'
KUBERNETES_PORT_443_TCP='tcp://10.96.0.1:443'
KUBERNETES_PORT_443_TCP_ADDR='10.96.0.1'
KUBERNETES_PORT_443_TCP_PORT='443'
KUBERNETES_PORT_443_TCP_PROTO='tcp'
KUBERNETES_SERVICE_HOST='10.96.0.1'
KUBERNETES_SERVICE_PORT='443'
KUBERNETES_SERVICE_PORT_HTTPS='443'
LINENO=''
MEU_PASSWORD='catota'
MEU_USERNAME='linuxtips'
OPTIND='1'
PATH='/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'
PPID='0'
PS1='\w \$ '
PS2='> '
PS4='+ '
PWD='/'
SHLVL='1'
TERM='xterm'

Podemos verificar na saída que as variáveis 'MEU_PASSWORD' e 'MEU_USERNAME' estão ali...

Obs: Caso não queria entrar no container para ver as variáveis basta executar externamente...

# kubectl exec teste-secret-env -c busy-secret-env -it -- printenv


-*- ConfigMaps


ConfigMaps são bem parecidos com as Secrets...São usados para adicionarmos configurações dentro do nosso POD/Container sem precisar
ficar buildando outras imagens...

Criamos uma pasta chamada frutas...

# mkdir frutas

Adicionados arquivos com valores

# echo amarela > frutas/banana
# echo morango > frutas/morango
# echo verde > frutas/limao
# echo "verde e vermelho" > frutas/melancia

Um arquivo na raiz da pasta 'day-4'...

# echo kiwi > predileta

Criamos um configmap com os parâmetros '--from-literal' e '--from-file'

# kubectl create configmap cores-frutas --from-literal uva=roxa --from-file=predileta --from-file=frutas/


env:
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: aws-credentials
          key: AWS_ACCESS_KEY_ID
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: aws-credentials
          key: AWS_SECRET_ACCESS_KEY

Foi criado também um manifesto de um pod para utilizarmos este configmap...

# vim pod-configmap.yaml

Este arquivo está na pasta 'day-4'...


-*- ConfigMaps - Parte 02

O que temos diferente dentro do arquivo?

env:
    - name: frutas
      valueFrom:
        configMapKeyRef:
          name: cores-frutas
          key: predileta

Dizemos que queremos pegar o valor 'frutas', mas não está ali o valor, iremos pegar de outros lugares...

Podemos notar a semelhança com quando queriamos pegar uma secret...'secretKeyRef', agora vamos pegar valores de um configmap...'configMapKeyRef'

O nomes que estamos chamando é 'cores-frutas', ou seja, temos que ter um 'configmap' com este nome...

# kubectl get configmap

NAME               DATA   AGE
cores-frutas       6      2d
kube-root-ca.crt   1      61d

Neste config map queremos o valor da chave predileta...'key: predileta'

# kubectl create -f pod-configmap.yaml

# kubectl get pods

# kubectl exec -it "nome do pod" -- sh

- Dentro do container...

# set

...
PATH='/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'
PPID='0'
PS1='\w \$ '
PS2='> '
PS4='+ '
PWD='/'
SHLVL='1'
TERM='xterm'
_='ls'
frutas='kiwi

Podemos ver que de fato ele só trouxe o valor da 'fruta predileta' que nos arquivos definimos como kiwi...

Agora iremos mudar no nosso manifesto do pod o 'env' para 'envFrom', pois agora queremos pegar mais de um valor...

Disto...

env:
- name: frutas
  valueFrom:
    configMapKeyRef:
      name: cores-frutas
      key: predileta
      
Para isto...

envFrom:
- configMapRef:
    name: cores-frutas

'envFrom' como dito anteriormente, porque queremos pegar todos os valores agora e não apenas 1...

Assim como o 'configMapKeyRef' para 'configMapRef', pois não queremos a referência de apenas uma chave...

E por isso agora também podemos passar somente o nome do configmap, 'cores-frutas', sem precisar passar o valor 'key'...


-*- ConfigMpas - Parte 03

# kubectl delete pod busybox-configmap

# kubectl create -f pod-configmap.yaml

# kubectl get pods

# kubectl exec -it 'nome do pod' -- sh

...
PATH='/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'
PPID='0'
PS1='\w \$ '
PS2='> '
PS4='+ '
PWD='/'
SHLVL='1'
TERM='xterm'
_='ls'
banana='amarela
'
limao='verde
'
melancia='verde e vermelho
'
morango='morango
'
predileta='kiwi
'
uva='roxa'

Agora temos todos os valores/variáveis de ambiente dentro do nosso container...

Vamos agora em vez de passar os valores como variáveis de ambiente, vamos passar em forma de arquivo...

# vim pod-configmap-file.yaml

Obs: Este arquivo e seu conteúdo se encontram na pasta 'day-4'...


-*- ConfigMaps - Parte 04

Agora a parte que temos diferente...

volumeMounts: <-Queremos um volume no Container->
    - name: meu-configmap-vol <-Nome do meu volume->
      mountPath: /etc/frutas <-Onde vamos montar este volume->
  volumes:
  - name: meu-configmap-vol <--A definição do volume no Pod>
    configMap: <O tipo do volume-->
      name: cores-frutas <-O nome do configMap que quero pegar->


# kubectl get pods

# kubectl delete pod busybox-configmap

# kubectl create -f pod-configmap-file.yaml

# kubectl get pods

root@elliot-01:~/day-4# kubectl exec -it busybox-configmap-file -- sh
/ # ls
bin   dev   etc   home  proc  root  sys   tmp   usr   var
/ # cd etc
/etc # ls
frutas       group        hostname     hosts        localtime    mtab         network      passwd       resolv.conf  shadow
/etc # cd frutas
/etc/frutas # ls
banana     limao      melancia   morango    predileta  uva
/etc/frutas # 

-- Verificando os valores...

/etc/frutas # cat banana
amarela
/etc/frutas # cat limao
verde
/etc/frutas # cat melancia
verde e vermelho
/etc/frutas # 

/etc/frutas # ls -lha
total 12K    
drwxrwxrwx    3 root     root        4.0K Dec 31 18:44 .
drwxr-xr-x    1 root     root        4.0K Dec 31 18:44 ..
drwxr-xr-x    2 root     root        4.0K Dec 31 18:44 ..2021_12_31_18_44_21.084356489
lrwxrwxrwx    1 root     root          31 Dec 31 18:44 ..data -> ..2021_12_31_18_44_21.084356489
lrwxrwxrwx    1 root     root          13 Dec 31 18:44 banana -> ..data/banana
lrwxrwxrwx    1 root     root          12 Dec 31 18:44 limao -> ..data/limao
lrwxrwxrwx    1 root     root          15 Dec 31 18:44 melancia -> ..data/melancia
lrwxrwxrwx    1 root     root          14 Dec 31 18:44 morango -> ..data/morango
lrwxrwxrwx    1 root     root          16 Dec 31 18:44 predileta -> ..data/predileta
lrwxrwxrwx    1 root     root          10 Dec 31 18:44 uva -> ..data/uva

Podemos verificar que foram criados links simbolicos...

Vamos verificar mais a fundo...

/etc/frutas # find / -iname predileta
/etc/frutas/predileta
/etc/frutas/..2021_12_31_18_44_21.084356489/predileta

/etc/frutas # cd ..2021_12_31_18_44_21.084356489/
/etc/frutas/..2021_12_31_18_44_21.084356489 # ls
banana     limao      melancia   morango    predileta  uva
/etc/frutas/..2021_12_31_18_44_21.084356489 # cat morango
morango
/etc/frutas/..2021_12_31_18_44_21.084356489 # cat predileta
kiwi
/etc/frutas/..2021_12_31_18_44_21.084356489 # cat uva
roxa/etc/frutas/..2021_12_31_18_44_21.084356489 #


-*- InitContainers

 Sempre que precisamos preparar um ambiente para que o container principal suba com uma determinada configuração pre-estabelecida ou
 uma tarefa inicial...Para isto servem os 'InitContainers'...

 É um outro container, que roda antes do container da aplicação propriamente dito...

 Vamos iniciar criando um arquivo chamado 'nginx-initcontainer.yaml'

 # vim nginx-initcontainer.yaml

 Obs: Este arquivo se encontra na pasta 'day-4'


-*- InitContainers - Parte 02 

Obs2: Apareceu o seguinte erro quando tentei utilizar o 'wget' para o site 'http://kubernetes.io'

Connecting to kubernetes.io (147.75.40.148:80)
Connecting to kubernetes.io (147.75.40.148:443)
wget: note: TLS certificate validation not implemented
wget: TLS error from peer (alert code 40): handshake failure
wget: error getting response: Connection reset by peer

Por isso fiz com o site do google mesmo 'wget -O /work-dir/index.html http://google.com' para funcionar com a imagem busybox
Porém fiz um novo teste com a imagem alpine, como ela tinha 'TLS support', foi possível baixar pro 'index.html' o site do kubernetes

Obs3: Usei os dois modelos na sintaxe do 'command' também...


command: ['wget', "-O", "/work-dir/index.html", 'http://google.com']

command:
- wget
- "-O"
- "/work-dir/index.html"
- http://kubernetes.io


-*- RBAC


Hoje vamos conhecer o básico do RBAC, junto com o Ingress, iremos criar a conta, o clusterrole, clusterrolebinding...
Hoje veremos apenas como criar um serviceaccount e dar permissões a um usuaŕio para ser o admin ao cluster.
Com o ingress daremos permissões específicas para usuários específicos...


Começando...

Para criar um serviceaccount basta fazermos: "kubectl create serviceaccount marlon"

root@elliot-01:~# kubectl describe serviceaccount marlon
Name:                marlon
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   marlon-token-lqvdq
Tokens:              marlon-token-lqvdq
Events:              <none>

Agora podemos associar esta "serviceaccount" a uma "clusterrole". Para listar as "clusterroles": "kubectl get clusterrole"

NAME                                                                   CREATED AT
admin                                                                  2021-10-31T16:58:45Z
cluster-admin                                                          2021-10-31T16:58:45Z
edit                                                                   2021-10-31T16:58:45Z
kubeadm:get-nodes                                                      2021-10-31T16:58:46Z
system:aggregate-to-admin                                              2021-10-31T16:58:45Z
system:aggregate-to-edit                                               2021-10-31T16:58:45Z
system:aggregate-to-view                                               2021-10-31T16:58:45Z
system:auth-delegator                                                  2021-10-31T16:58:45Z
system:basic-user                                                      2021-10-31T16:58:45Z
system:certificates.k8s.io:certificatesigningrequests:nodeclient       2021-10-31T16:58:45Z
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2021-10-31T16:58:45Z
system:certificates.k8s.io:kube-apiserver-client-approver              2021-10-31T16:58:45Z
system:certificates.k8s.io:kube-apiserver-client-kubelet-approver      2021-10-31T16:58:45Z
system:certificates.k8s.io:kubelet-serving-approver                    2021-10-31T16:58:45Z
system:certificates.k8s.io:legacy-unknown-approver                     2021-10-31T16:58:45Z
system:controller:attachdetach-controller                              2021-10-31T16:58:45Z
system:controller:certificate-controller                               2021-10-31T16:58:45Z
system:controller:clusterrole-aggregation-controller                   2021-10-31T16:58:45Z
system:controller:cronjob-controller                                   2021-10-31T16:58:45Z
system:controller:daemon-set-controller                                2021-10-31T16:58:45Z
system:controller:deployment-controller                                2021-10-31T16:58:45Z
system:controller:disruption-controller                                2021-10-31T16:58:45Z
system:controller:endpoint-controller                                  2021-10-31T16:58:45Z
system:controller:endpointslice-controller                             2021-10-31T16:58:45Z
system:controller:endpointslicemirroring-controller                    2021-10-31T16:58:45Z
system:controller:ephemeral-volume-controller                          2021-10-31T16:58:45Z
system:controller:expand-controller                                    2021-10-31T16:58:45Z
system:controller:generic-garbage-collector                            2021-10-31T16:58:45Z
system:controller:horizontal-pod-autoscaler                            2021-10-31T16:58:45Z
system:controller:job-controller                                       2021-10-31T16:58:45Z
system:controller:namespace-controller                                 2021-10-31T16:58:45Z
system:controller:node-controller                                      2021-10-31T16:58:45Z
system:controller:persistent-volume-binder                             2021-10-31T16:58:45Z
system:controller:pod-garbage-collector                                2021-10-31T16:58:45Z
system:controller:pv-protection-controller                             2021-10-31T16:58:45Z
system:controller:pvc-protection-controller                            2021-10-31T16:58:45Z
system:controller:replicaset-controller                                2021-10-31T16:58:45Z
system:controller:replication-controller                               2021-10-31T16:58:45Z
system:controller:resourcequota-controller                             2021-10-31T16:58:45Z
system:controller:root-ca-cert-publisher                               2021-10-31T16:58:45Z
system:controller:route-controller                                     2021-10-31T16:58:45Z
system:controller:service-account-controller                           2021-10-31T16:58:45Z
system:controller:service-controller                                   2021-10-31T16:58:45Z
system:controller:statefulset-controller                               2021-10-31T16:58:45Z
system:controller:ttl-after-finished-controller                        2021-10-31T16:58:45Z
system:controller:ttl-controller                                       2021-10-31T16:58:45Z
system:coredns                                                         2021-10-31T16:58:47Z
system:discovery                                                       2021-10-31T16:58:45Z
system:heapster                                                        2021-10-31T16:58:45Z
system:kube-aggregator                                                 2021-10-31T16:58:45Z
system:kube-controller-manager                                         2021-10-31T16:58:45Z
system:kube-dns                                                        2021-10-31T16:58:45Z
system:kube-scheduler                                                  2021-10-31T16:58:45Z
system:kubelet-api-admin                                               2021-10-31T16:58:45Z
system:monitoring                                                      2021-10-31T16:58:45Z
system:node                                                            2021-10-31T16:58:45Z
system:node-bootstrapper                                               2021-10-31T16:58:45Z
system:node-problem-detector                                           2021-10-31T16:58:45Z
system:node-proxier                                                    2021-10-31T16:58:45Z
system:persistent-volume-provisioner                                   2021-10-31T16:58:45Z
system:public-info-viewer                                              2021-10-31T16:58:45Z
system:service-account-issuer-discovery                                2021-10-31T16:58:45Z
system:volume-scheduler                                                2021-10-31T16:58:45Z
view                                                                   2021-10-31T16:58:45Z
weave-net          

Vamos descrever a "clusterrole" "cluster-admin"

"kubectl describe clusterrole cluster-admin"

Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [*]
             [*]                []              [*]

Podemos ver tudo o que a serviceaccount que estiver nesta clusterrole pode fazer, neste caso acima, TUDO...

Vamos ver outra clusterrole, a view

root@elliot-01:~# kubectl describe clusterrole view
Name:         view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  bindings                                     []                 []              [get list watch]
  configmaps                                   []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  events                                       []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  persistentvolumeclaims                       []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  pods                                         []                 []              [get list watch]
  replicationcontrollers/scale                 []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  replicationcontrollers                       []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  serviceaccounts                              []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  services                                     []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  daemonsets.apps                              []                 []              [get list watch]
  deployments.apps/scale                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  deployments.apps                             []                 []              [get list watch]
  replicasets.apps/scale                       []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  replicasets.apps                             []                 []              [get list watch]
  statefulsets.apps/scale                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  statefulsets.apps                            []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  cronjobs.batch                               []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  jobs.batch                                   []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  daemonsets.extensions                        []                 []              [get list watch]
  deployments.extensions/scale                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  deployments.extensions                       []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  ingresses.extensions                         []                 []              [get list watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  replicasets.extensions/scale                 []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  replicasets.extensions                       []                 []              [get list watch]
  replicationcontrollers.extensions/scale      []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  ingresses.networking.k8s.io                  []                 []              [get list watch]
  networkpolicies.networking.k8s.io            []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  poddisruptionbudgets.policy                  []                 []              [get list watch]

Podemos ver que nesta 'clusterrole' podemos realizar apenas as ações de 'get', 'list' e 'watch'...

Para conectar a 'serviceaccount' a uma 'clusterrole' precisamos de um recurso chamado 'clusterrolebinding'.

Para lista-las: 'kubectl get clusterrolebindings'

NAME                                                   ROLE                                                                               AGE
cluster-admin                                          ClusterRole/cluster-admin                                                          77d
kubeadm:get-nodes                                      ClusterRole/kubeadm:get-nodes                                                      77d
kubeadm:kubelet-bootstrap                              ClusterRole/system:node-bootstrapper                                               77d
kubeadm:node-autoapprove-bootstrap                     ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient       77d
kubeadm:node-autoapprove-certificate-rotation          ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   77d
kubeadm:node-proxier                                   ClusterRole/system:node-proxier                                                    77d
system:basic-user                                      ClusterRole/system:basic-user                                                      77d
system:controller:attachdetach-controller              ClusterRole/system:controller:attachdetach-controller                              77d
system:controller:certificate-controller               ClusterRole/system:controller:certificate-controller                               77d
system:controller:clusterrole-aggregation-controller   ClusterRole/system:controller:clusterrole-aggregation-controller                   77d
system:controller:cronjob-controller                   ClusterRole/system:controller:cronjob-controller                                   77d
system:controller:daemon-set-controller                ClusterRole/system:controller:daemon-set-controller                                77d
system:controller:deployment-controller                ClusterRole/system:controller:deployment-controller                                77d
system:controller:disruption-controller                ClusterRole/system:controller:disruption-controller                                77d
system:controller:endpoint-controller                  ClusterRole/system:controller:endpoint-controller                                  77d
system:controller:endpointslice-controller             ClusterRole/system:controller:endpointslice-controller                             77d
system:controller:endpointslicemirroring-controller    ClusterRole/system:controller:endpointslicemirroring-controller                    77d
system:controller:ephemeral-volume-controller          ClusterRole/system:controller:ephemeral-volume-controller                          77d
system:controller:expand-controller                    ClusterRole/system:controller:expand-controller                                    77d
system:controller:generic-garbage-collector            ClusterRole/system:controller:generic-garbage-collector                            77d
system:controller:horizontal-pod-autoscaler            ClusterRole/system:controller:horizontal-pod-autoscaler                            77d
system:controller:job-controller                       ClusterRole/system:controller:job-controller                                       77d
system:controller:namespace-controller                 ClusterRole/system:controller:namespace-controller                                 77d
system:controller:node-controller                      ClusterRole/system:controller:node-controller                                      77d
system:controller:persistent-volume-binder             ClusterRole/system:controller:persistent-volume-binder                             77d
system:controller:pod-garbage-collector                ClusterRole/system:controller:pod-garbage-collector                                77d
system:controller:pv-protection-controller             ClusterRole/system:controller:pv-protection-controller                             77d
system:controller:pvc-protection-controller            ClusterRole/system:controller:pvc-protection-controller                            77d
system:controller:replicaset-controller                ClusterRole/system:controller:replicaset-controller                                77d
system:controller:replication-controller               ClusterRole/system:controller:replication-controller                               77d
system:controller:resourcequota-controller             ClusterRole/system:controller:resourcequota-controller                             77d
system:controller:root-ca-cert-publisher               ClusterRole/system:controller:root-ca-cert-publisher                               77d
system:controller:route-controller                     ClusterRole/system:controller:route-controller                                     77d
system:controller:service-account-controller           ClusterRole/system:controller:service-account-controller                           77d
system:controller:service-controller                   ClusterRole/system:controller:service-controller                                   77d
system:controller:statefulset-controller               ClusterRole/system:controller:statefulset-controller                               77d
system:controller:ttl-after-finished-controller        ClusterRole/system:controller:ttl-after-finished-controller                        77d
system:controller:ttl-controller                       ClusterRole/system:controller:ttl-controller                                       77d
system:coredns                                         ClusterRole/system:coredns                                                         77d
system:discovery                                       ClusterRole/system:discovery                                                       77d
system:kube-controller-manager                         ClusterRole/system:kube-controller-manager                                         77d
system:kube-dns                                        ClusterRole/system:kube-dns                                                        77d
system:kube-scheduler                                  ClusterRole/system:kube-scheduler                                                  77d
system:monitoring                                      ClusterRole/system:monitoring                                                      77d
system:node                                            ClusterRole/system:node                                                            77d
system:node-proxier                                    ClusterRole/system:node-proxier                                                    77d
system:public-info-viewer                              ClusterRole/system:public-info-viewer                                              77d
system:service-account-issuer-discovery                ClusterRole/system:service-account-issuer-discovery                                77d
system:volume-scheduler                                ClusterRole/system:volume-scheduler                                                77d
weave-net                                              ClusterRole/weave-net                                                              77d

Temos uma 'clusterrolebinding' com o mesmo nome da 'clusterrole' que vimos anteriormente...

Vamos dar um describe nela: 'kubectl describe clusterrolebinding cluster-admin'

Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name            Namespace
  ----   ----            ---------
  Group  system:masters

Não temos ninguém associado além do grupo 'system:masters'

Então, ficamos entendidos que uma 'clusterrolebinding' é quando associamos uma role (clusterrole) a um usuário (serviceaccount)

Vamos criar uma associação agora: 'kubectl create clusterrolebinding toskeria --serviceaccount=default:marlon --clusterrole=cluster-admin'

clusterrolebinding.rbac.authorization.k8s.io/toskeria created

Vamos dar um describe para ver como ficou: 'kubectl describe clusterrolebinding toskeria'

Name:         toskeria
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind            Name    Namespace
  ----            ----    ---------
  ServiceAccount  marlon  default

Podemos ver em 'Role:' qual é a 'clusterrole' e em 'Subjects:' qual é a 'serviceaccount' que a 'clusterrole' foi ligada (bind)

Vamos ver agora um describe do nosso 'serviceaccount': 'kubectl describe serviceaccount marlon'

root@elliot-01:~# kubectl describe serviceaccount marlon
Name:                marlon
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   marlon-token-lqvdq
Tokens:              marlon-token-lqvdq
Events:              <none>
 
Podemos ver que sobre a 'clusterrole' e a 'clusterrolebinding' não temos nenhuma informação...

Vamos criar estes recursos agora com yaml's...

# vim admin-user.yaml

Obs: Arquivo na pasta 'day-4'

# kubectl create -f admin-user.yaml

serviceaccount/admin-user created

root@elliot-01:~/day-4# kubectl get serviceaccount -n kube-system
NAME                                 SECRETS   AGE
admin-user                           1         16s
attachdetach-controller              1         77d
...

Vamos criar agora a 'clusterrolebinding'

# vim admin-cluster-role-binding.yaml

Obs: Arquivo se encontra na pasta 'day-4'

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io # De autorização
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

O 'clusterrolebinding' associa um 'serviceaccount' a uma 'clusterrole', e basicamente é isso que vemos acima...
É interessante notar que o 'namespace' definido é o 'kube-system' que é o namespace de 'administração'...

Descrevendo em mais detalhes: Pegamos o nosso 'admin-user' que é do tipo 'serviceaccount' e ligamos a 'cluster-admin' que
é do tipo 'clusterrole' e tem as permissões que desejamos para este usuário que está no 'namespaces' 'kube-system'...

Agora vamos criar: # kubectl create -f admin-cluster-role-binding.yaml

Então: # kubectl get clusterrolebinding

NAME                                                   ROLE                                                                               AGE
'admin-user                                            ClusterRole/cluster-admin'                                                       2m53s
cluster-admin                                          ClusterRole/cluster-admin                                                          77d
...

Podemos dar um describe aqui: # kubectl describe clusterrolebinding admin-user

root@elliot-01:~/day-4# kubectl describe clusterrolebinding admin-user
Name:         admin-user
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind            Name        Namespace
  ----            ----        ---------
  ServiceAccount  admin-user  kube-system

Isso foi o básico de 'serviceaccount', 'clusterrole' e 'clusterrolebinding', veremos com mais detalhes na parte do ingress...


-*- Descomplicando Helm - Day 1 - Aulas Twitch

"A ideia do treinamento é descomplicar a utilização do Helm e facilitar o entendimento dos Helm Chats.
Com isso, teremos muito mais facilidade em realizar o deploy de nossas aplicações"

Site do Helm: https://helm.sh/

Obs: Lembrando que todo 'Descomplicando Helm' foi baseado na documentação, sendo assim, grande parte dos comandos que utilizaremos
foram tirados da documentação...

"Helm é o melhor jeito de encontrar, compartilhar e usar softwares construídos para Kubernetes"

- Instalação

# sudo snap install helm --classic

[nezzonarcizo@localhost helm]$ sudo snap install helm --classic
[sudo] senha para nezzonarcizo: 
helm 3.7.0 from Snapcrafters installed

Para fazermos o deploy da nossa aplicação no cluster com o Helm, precisaremos de um 'Chart', nele reuniremos tudo que precisarmos
para a nossa aplicação...'Deployment', 'Services', 'Ingress' etc...

Temos charts pré-definidos em: https://artifacthub.io/

Neste site teremos charts, como por exemplo, do nginx...

- Link para instalação no cluster kubernetes (ubuntu): https://helm.sh/docs/intro/install/

curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -
sudo apt-get install apt-transport-https --yes
echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

root@elliot-01:~/helm# helm version
version.BuildInfo{Version:"v3.7.2", GitCommit:"663a896f4a815053445eec4153677ddc24a0a361", GitTreeState:"clean", GoVersion:"go1.16.10"}


Para vermos os possíveis comandos com o Helm: # helm help


- Helm Chart Repository

Nada mais é que um repositório de charts, onde podemos subir nossos charts e termos acesso a eles futuramente...É como se fosse um 
repositório do 'apt-get'.

Adicionando o repositório...

# helm repo add stable https://charts.helm.sh/stable

Agora que adicionamos o repositório podemos procurar charts lá...

Temos dois comandos para isso...

# helm search hub 'search for charts in the Artifact Hub or your own hub instance' (artifacts)
e
# helm search repo 'search repositories for a keyword in charts'

Exemplo:

# helm search hub stable ou helm search repo stable

Vamos adicionar o repositório stable

# helm repo add stable https://charts.helm.sh/stable

Verificando um chart de uma aplicação conhecida...

# helm search hub nginx

Vamos instalar um chart de exemplo, mas antes vamos atualizar o repo...

Vamos fazer um 'helm repo update' para baixar os metadados para vermos tudo que tem neste repositório...

Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈

Obs: Atualmente na página 'quickstart' do 'helm.sh', temos exemplo com repo do bitnami, e na aula o Jeferson utilizou o stable...
...Vamos seguir o exemplo da aula...

# helm install stable/mysql --generate-name

```
WARNING: This chart is deprecated
NAME: mysql-1644685631
LAST DEPLOYED: Sat Feb 12 17:07:13 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
mysql-1644685631.default.svc.cluster.local

To get your root password run:

    MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql-1644685631 -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
    $ mysql -h mysql-1644685631 -p

To connect to your database directly from outside the K8s cluster:
    MYSQL_HOST=127.0.0.1
    MYSQL_PORT=3306

    # Execute the following command to route the connection:
    kubectl port-forward svc/mysql-1644685631 3306

    mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}
```

Desta maneira não estão criando e nem customizando, estamos apenas pegando um chart que ja existe e executando no cluster kubernetes.

Este chart que baixamos é 'deprecated', ou seja, se você para um ambiente real teríamos que utilizar outro chart...

Os outros são dados que já vimos...

```
WARNING: This chart is deprecated
NAME: mysql-1644685631
LAST DEPLOYED: Sat Feb 12 17:07:13 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
mysql-1644685631.default.svc.cluster.local
```

Na descrição, ele já nos trouxe como recuperar a senha do MySQL no cluster que está dentro de uma secret...

'MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql-1644685631 -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)'

Utilizando um pod como um client para conectar ao banco...

'kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il'

Ou podemos instalar diretamente o 'mysql cli' e conectar à instância...

'mysql -h mysql-1642965554 -p'

Para realizar a conexão de fora do cluster...

MYSQL_HOST=127.0.0.1
MYSQL_PORT=3306'

"
# Execute the following command to route the connection:
kubectl port-forward svc/mysql-1644685631 3306

mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}
"

Para vermos quais charts Helm temos instalados no nosso cluster...

# helm list ou helm ls

NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
mysql-1644684290        default         1               2022-02-12 16:44:53.546211351 +0000 UTC deployed        mysql-8.8.23    8.0.28 


Para ver o status do chart

# helm status "nome do chart"

Obs: Este é como se fosse o 'readme' do chart que baixamos do repositório...
    (Aquelas mesmas informações que vieram quando instalamos o chart)
    Nelas existem os dados sobre o chart em si e informações de conexão do banco (neste caso do chart do MySQL)


Para desinstalar algum chart

# helm uninstall 'nome do chart'

Listamos mais uma vez os charts que temos disponíveis no repositório 'stable'

# helm search hub nginx e helm search repo stable

Fizemos a instalação do minecraft service

# helm install stable/minecraft --generate-name

Obs: Ele nos dá um aviso que o "deployment vai estar incompleto até lermos o EULA do Minecraft linkado no README.md"

Para fazer isso ele imprime na tela o comando para usarmos: "helm upgrade minecraft-1644687759 \
--set minecraftServer.eula=true stable/minecraft"

Agora recebemos as informações sobre este chart...

WARNING: This chart is deprecated
Release "minecraft-1644687759" has been upgraded. Happy Helming!
NAME: minecraft-1644687759
LAST DEPLOYED: Sat Feb 12 17:44:57 2022
NAMESPACE: default
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
Get the IP address of your Minecraft server by running these commands in the
same shell:

!! NOTE: It may take a few minutes for the LoadBalancer IP to be available. !!

You can watch for EXTERNAL-IP to populate by running:
  kubectl get svc --namespace default -w minecraft-1644687759-minecraft

Fizemos testes com o chart do bitnami/nginx também...

# helm uninstall minecraft-1644687759

Para o "help" do helm...

# helm get -h

------- Charts

Criando a estrutura do chart...

https://helm.sh/docs/topics/charts/

https://helm.sh/docs/topics/library_charts/

# mkdir charts

# cd charts

# helm create giropops

A seguinte estrutura será criada:
"Chart.yaml  charts  templates  values.yaml"

Dica: para melhor visualização de diretórios no Linux instale o 'tree'

# sudo apt-get install tree -y

# ls
"
.
├── Chart.yaml
├── charts
├── templates
│   ├── NOTES.txt
│   ├── _helpers.tpl
│   ├── deployment.yaml
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── service.yaml
│   ├── serviceaccount.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml
"

Chart.yaml - Chart metadados - definição

diretorio charts - 

diretorio templates

  Tudo que precisamos para montar nossa aplicação, se precisarmos de mais algum YAML basta adicionarmos aqui...

values.yaml

  Principal arquivo, o que define os valores e preenche os arquivos templates

.helmignore

  Arquivo que contem os nomes dos arquivos que serão ignorados ao subirmos um chart para o repositório


Dentro do arquivo 'Chart.yaml' temos algumas informações, inclusive sobre os tipos de 'charts' que existem...

# Application charts are a collection of templates that can be packaged into versioned archives
# to be deployed.
#
# Library charts provide useful utilities or functions for the chart developer. They're included as
# a dependency of application charts to inject those utilities and functions into the rendering
# pipeline. Library charts do not define any templates and therefore cannot be deployed.


Temos diversos atributos para colocar no arquivo Charts.yaml

Neste link da documentação podemos verificar a lista completa
https://helm.sh/docs/topics/charts/


Teoricamente, as estruturas nos arquivos de template são as mesmas das que usamos normalmente nos nossos arquivos yaml, com a 
diferença de que são passadas por referencia...

Exemplo:

No arquivo 'deployment.yaml' temos o seguinte campo...

containers:
  - name: {{ .Chart.Name }}

Este valor está sendo preenchido utilizando a referencia do arquivo 'Chart.yaml' valor 'Name'....

apiVersion: v2
name: giropops
description: Primeiro chart do Nginx criado no treinamento
...

Com o campo da 'imagem' no 'deployment' acontece a mesma coisa, mas agora pegando de arquivo diferente...

containers:
- name: {{ .Chart.Name }}
  securityContext:
    {{- toYaml .Values.securityContext | nindent 12 }}
  image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"

Agora podemos ver que a referencia esta sendo feita ao arquivo 'values.yaml'

image:
  repository: nginx
  ...
  tag: "" # Está vazio, mas se quisessemos uma versão específica do Nginx aqui que deveríamos definir...

  Obs: O último campo 'default .Chart.AppVersion'
  Caso não passarmos um valor para a Tag, ele buscará a versão "padrão" no arquivo main/metadados 'Chart.yaml', '1.16.0'

  apiVersion: v2
  name: giropops
  description: Primeiro chart do Nginx criado no treinamento
  type: application
  version: 0.1.0
  'appVersion: "1.16.0"'


Descomentamos no 'values.yaml' os "limites de hardware" para a aplicação...
"
limits:
  cpu: 100m
  memory: 128Mi
requests:
  cpu: 100m
  memory: 128Mi
"

Reconfiguramos o 'autoscaling'...

autoscaling:
  enabled: true # Aqui deixamos true ao invés de false
  minReplicas: 3 # Aqui deixamos 3 réplicas ao invés de 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

Adicionamos um nome para uma futura 'service account' que iriamos utilizar...

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "giropops"


Dentro do 'deployment.yaml' podemos ver que temos também condicionais, como é o caso no 'spec'...

spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}

Ou seja, caso não definirmos como tru, aquela parte vista acima no 'autoscaling', ele buscará o valor no arquivo 'values.yaml'
no campo 'replicaCount' logo no início do arquivo...
"
# Default values for giropops.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
...
"

Mudamos o tipo de serviço de 'ClusterIP' para 'LoadBalancer'

"
service:
  type: LoadBalancer
  port: 80
"

Agora iremos fazer a instalação do helm...

Voltamos um nível de pasta, "/charts" ...e realizamos o seguinte comando...

 # helm install giropops giropops/ --values giropops/values.yaml

Nele estamos dizendo onde esta os nossos charts (giropops/) e onde está o arquivo values (giropops/values.yaml)

E assim estará feita a instalação do nosso chart...

Obs: Na hora que eu fiz o deploy a primeira vez, ele reclamou da versão do 'autoscaling' tive que muder da 'autoscaling/v2beta1'
      para 'autoscaling/v2beta2'

"
NAME: giropops
LAST DEPLOYED: Sat Feb 12 20:41:30 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace default svc -w giropops'
  export SERVICE_IP=$(kubectl get svc --namespace default giropops --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:80

# kubectl get svc

NAME         TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
giropops     LoadBalancer   10.98.230.3   <pending>     80:31813/TCP   4m47s
kubernetes   ClusterIP      10.96.0.1     <none>        443/TCP        104d

"
Obs: Não foi exibido o 'EXTERNAL-IP'

Agora iremos acrescentar algumas coisas ao nosso chart, por isso alteraremos a versão para 0.2.0 e a versão da aplicação para 1.19.9

"
apiVersion: v2
name: giropops
scription: Primeiro chart do Nginx criado no treinamento
type: application
version: 0.2.0
appVersion: "1.19.9"
...
"

Vamos alterar no 'values.yaml' o 'targetCPUUtilizationPercentage' para '50'
"
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 100
  targetCPUUtilizationPercentage: 50
  # targetMemoryUtilizationPercentage: 80
"

Versão do Nginx para '1.19.9'

image:
  repository: "nginx"
  pullPolicy: "Always"
  # Overrides the image tag whose default is the chart appVersion.
  tag: "1.19.9"


Agora com upgrade...# helm upgrade giropops giropops/ --values giropops/values.yaml 

"
W0212 21:42:26.521241  111813 warnings.go:70] autoscaling/v2beta1 HorizontalPodAutoscaler is deprecated in v1.22+, unavailable in v1.25+; use autoscaling/v2beta2 HorizontalPodAutoscaler
W0212 21:42:26.550201  111813 warnings.go:70] autoscaling/v2beta1 HorizontalPodAutoscaler is deprecated in v1.22+, unavailable in v1.25+; use autoscaling/v2beta2 HorizontalPodAutoscaler
W0212 21:42:26.610381  111813 warnings.go:70] autoscaling/v2beta1 HorizontalPodAutoscaler is deprecated in v1.22+, unavailable in v1.25+; use autoscaling/v2beta2 HorizontalPodAutoscaler
Release "giropops" has been upgraded. Happy Helming!
NAME: giropops
LAST DEPLOYED: Sat Feb 12 21:42:26 2022
NAMESPACE: default
STATUS: deployed
REVISION: 2
NOTES:
1. Get the application URL by running these commands:
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace default svc -w giropops'
  export SERVICE_IP=$(kubectl get svc --namespace default giropops --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:80
"

root@elliot-01:~/helm/charts# helm ls
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
giropops        default         2               2022-02-12 21:42:26.114351726 +0000 UTC deployed        giropops-0.2.0  1.19.9  

Agora neste caso temos duas revisões (Na aula já estavam na revisão 3)

Então, se quisermos voltar a versão podemos fazer o rollback...

# helm rollback giropops 1

NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
giropops        default         3               2022-02-12 21:47:12.937584816 +0000 UTC deployed        giropops-0.1.0  1.16.0

Voltamos para a versão 1.16.0


Mais uma alteração...

Na parte do 'ingress' no 'values.yaml', setei como 'true' (Coisa que não tinha feito antes)
"
ingress:
  enabled: true
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
"

Alterei também a versão da aplicação para 0.3.0

Para ver o histório de install, upgrades e rollouts...

# helm history giropops


-*- Descomplicando Helm - Day 2 - Aulas Twitch

# mkdir day-2
# mkdir k8s_files
# vim deployment.yaml
# vim services.yaml

Obs: Os arquivos estão disponíveis no diretório "day-4/helm/day-2/k8s_files/"

No dia 2 o Jefferson usou o conteúdo da estrutura do Chart do dia 1...

Podemos fazer o mesmo copiando o charts para o dia 2, ou podemos realizar o seguinte comando...

# helm create giropops

E preencher novamente os campos como estavam...
Basicamente replicamos as configurações dos arquivos 'deployment.yaml' e 'service.yaml' no 'Chart.yaml' e no 'values.yaml'

Nesta aula habilitamos o 'ingress'...

# kubectl create namespace ingress-basic

# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

"
# helm install nginx-ingress ingress-nginx/ingress-nginx \
    --namespace ingress-basic \
    --set controller.replicaCount=2 \
    --set controller.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set defaultBackend.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set controller.admissionWebhooks.patch.nodeSelector."beta\.kubernetes\.io/os"=linux
"

Obs: Nesta versão aplicada na aula somos alertados que está deprecated desde a versão 1.14...

# vim ingress.yaml

Obs: Arquivo dentro da pasta "day-4/helm/day-2/"

Um jeito diferente de configurar o ingress baseado no ambiente da "AZURE"...
Foi utilizado o seguinte link: https://docs.microsoft.com/en-us/azure/aks/ingress-tls?tabs=azure-cli
A configuração peculiar desta forma de implementar o ingress que o Jeferson mostrou é a seguinte...

# Public IP address of your ingress controller
IP="MY_EXTERNAL_IP"

# Name to associate with public IP address
DNSNAME="demo-aks-ingress"

# Get the resource-id of the public ip
PUBLICIPID=$(az network public-ip list --query "[?ipAddress!=null]|[?contains(ipAddress, '$IP')].[id]" --output tsv)

# Update public ip address with DNS name
az network public-ip update --ids $PUBLICIPID --dns-name $DNSNAME

# Display the FQDN
az network public-ip show --ids $PUBLICIPID --query "[dnsSettings.fqdn]" --output tsv

"
Obs: Quando temos o ingress, nao precisamos mais usar o tipo LoadBalancer, podemos mudar o service para ClusterIP...

Obs2: Todo o Values também tem que ser configurado de acordo com o Ingress criado

Lembrando que os valores que setamos no 'values.yaml' são valores utilizados pelos 'templates'...
Tempos templates pra criar 'deployments', 'services', 'ingress', 'hpa', 'serviceaccount'...

Aquele 'name: giropops' que adicionamos no ínicio do arquivo 'values.yaml' vai ser pego pelo templates, no arquivo 'deployment.yaml'
desta forma:

...
metadata:
  name: {{ .Values.name }}
...

Se fosse outro valor que estivesse dentro de alguma outra parte, teríamos que inclui-lá, exemplo:

...
spec:
  {{ .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
...

No caso de um campo dentro de outro campo, preencheriamos assim:
Obs: Como dentro do arquivo 'values.yaml' o campo imagem é separado da tag, teríamos que abrir duas chaves como abaixo...
...
image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
imagePullPolicy: {{ .Values.image.pullPolicy }}
...

Obs: O Jeferson também deu um exemplo de como instalar o cert-manager, mas como foi com o tutorial da Azure e eu nao tinha infra
      lá, não acompanhei.


-*- Helm (Aula antiga...Apenas a instalação é versão nova)

------Instalação

Helm atende a necessida de instalar aplicações através de pacotes completos baixados de um repositório. 
Instalamos as aplicações e as dependencias dela.

Existem duas formas de instalar o Helm...

No caso do Ubuntu:

# snap install helm --classic

Ou utilizando o wget:
Obs: Escolha a versão em: https://github.com/helm/helm/releases

Neste caso cliquei na versão: v3.8.0
https://github.com/helm/helm/releases/tag/v3.8.0

Na página, como estou usando Linux com amd64, copiar o link para o arquivo '*.tar.gz'
https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz

# wget https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz

Desempacotando:
# tar -zxvf 

Temos que entrar dentro da pasta linux-amd64 agora...
#root@elliot-01:~/helm/helm-treinamento# ls
helm-v3.8.0-linux-amd64.tar.gz  linux-amd64

# cd linux-amd64
#root@elliot-01:~/helm/helm-treinamento/linux-amd64# ls
LICENSE  README.md  helm

Dentro da pasta que descompactamos temos que encontrar o binário do Helm e mandar para '/usr/local/bin/helm'
# mv linux-amd64/helm /usr/local/bin/helm

Para testar e ver se a instalação deu certo
# helm help

ou

# helm version
version.BuildInfo{Version:"v3.8.0", GitCommit:"d14138609b01886f544b2025f5000351c9eb092e", GitTreeState:"clean", GoVersion:"go1.17.5"}


-------
Daqui pra baixo utiliza-se na versão antiga
-------
Obs: O Jeferson informou que quando estamos utilizando o Helm junto com o cluster construído pelo Kubeadm, temos que tomar alguns cuidados...
Quando fazemos com Kubeadm o RBAC já vem ativo por default...Como temos o RBAC, precisamos que o Helm esteja associado a um 'clusterolebinding'
Por isso precisamos de uma 'serviceaccount' associada ao 'tiller'.

Porém nesta versão que estou usando, e utilizei a docuementação para instalar, nao veio com o tiller...
-------

Precisamos inicializar o Helm na nossa máquina...
# helm init
Obs: Este comando também não é necessário na versão nova

Para criarmos uma 'serviceaccount' para o 'tiller':
# kubectl create serviceaccount --namespace=kube-system tiller
Obs: Deve ser no namespace do 'kube-system' pois ele irá interagir com todos os 'Pods'

Próximo passo criar um 'clusterrolebinding' para ligar a 'serviceaccount' ao 'clusterrole'
# kubectl create clusterrolebinding tiller-cluster-role --clusterrole cluster-admin --serviceaccount=kube-system:tiller
No comando acima estamos dizendo que queremos criar uma 'clusterrolebinding' com o nome de 'tiller-cluster-role' e queremos associar a 
'clusterrole' de nome 'cluster-admin', que é o grupo de permissões, e a conta que queremos associar a este grupo de permissões é a 'tiller'
que está no namespace 'kube-system' ou seja: 'kube-system:tiller'

A instalação do 'Helm' criou um deployment do 'Tiller', neste deployment temos que adicionar a referência da 'serviceaccount' para que tenha
as permissões necessárias para realizar os deploy no cluster...

Para esta tarefa temos um comando no Kubernetes chamado 'patch' que insere alterações no deployment...

# kubectl patch deployments -n kube-system tiller-deploy -p "{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}"

#root@elliot-01:~/helm/helm-treinamento# helm list
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

O comando usado em aula, 'helm search grafana', não trouxe nada pois ainda não instalei nenhum repositório


-*- Helm - Parte 02 (Aula antiga)

Nesta aula primeiro foi criado o 'namespace' 'monitoring':
# kubectl create namespace monitoring

Obs: Aqui nesta aula ficou explicado porque a instalação do Helm na aula ao vivo deu trabalho com os 'PVs' e os 'PVCs', deviam ser desabilitados
na instalação do Helm via linha de comando...

Precisamos adicionar um repositório do Helm:
# helm repo add bitnami https://charts.bitnami.com/bitnami


Obs: No caso já tinhamos uns instalados da aula ao vivo...
#root@elliot-01:~/helm/helm-treinamento# helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "bitnami" chart repository

Vamos procurar que pacotes temos do 'Prometheus'
# helm search repo stable

NAME                                    CHART VERSION   APP VERSION             DESCRIPTION                                       
stable/prometheus                       11.12.1         2.20.1                  DEPRECATED Prometheus is a monitoring system an...


Instalando Prometheus:
# helm install --namespace=monitoring --name=prometheus --version=7.0.0 --set alertmanager.persistentVolume.enabled=false,server.persistentVolume.enabled=false stable/prometheus

Obs: Flag '--name=' desconhecida, tentando nova sintaxe do comando...

# helm install prometheus stable/prometheus --namespace monitoring --version 11.12.1 --set alertmanager.persistentVolume.enabled=false,server.persistentVolume.enabled=false

Comando correto, porém:

WARNING: This chart is deprecated
Error: INSTALLATION FAILED: unable to build kubernetes objects from release manifest: [unable to recognize "": no matches for kind 
"ClusterRole" in version "rbac.authorization.k8s.io/v1beta1", unable to recognize "": no matches for kind "ClusterRoleBinding" in version 
"rbac.authorization.k8s.io/v1beta1"]

Verificar se a versão pura do Prometheus no repositório Bitnami não está deprecated:

# helm search repo bitnami

Deprecated também...
Praticamente todos os charts do Prometheus, tanto no stable quanto no bitnami estão deprecated...

A opção foi adicionar o repositório da própria comunidade do prometheus...
# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

Obs: Se tiver adicionado repositórios demais, você pode remove-los com:
# helm repo remove "repo"

Para ver quais os repositório que já tem adicionado:
# helm repo list

Procurando uma versão mais simples do Prometheus para instalar:
# helm search repo prometheus-community

NAME                                                    CHART VERSION   APP VERSION     DESCRIPTION
prometheus-community/prometheus                         15.5.2          2.31.1          Prometheus is a monitoring system and time seri...     

# helm install prometheus prometheus-community/prometheus --namespace monitoring --version 15.5.2 --set alertmanager.persistentVolume.enabled=false,server.persistentVolume.enabled=false

NAME: prometheus
LAST DEPLOYED: Tue Mar  1 21:32:23 2022
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local

... (Abaixo destas primeiras informações temos outras informações importantes também, como quando foi deployado, namespace, etc)

Como desativamos as opções para 'persistent volumes' para o 'alertmanager' e o 'server' ele nos dá o seguinte aviso:

#################################################################################
######   WARNING: Persistence is disabled!!! You will lose your data when   #####
######            the Server pod is terminated.                             #####
#################################################################################


Listando o Helm instalado no namespace monitoring (Isto não foi mostrado na aula, não sei se o Jeferson já estava no contexto no namespace monitoring)
#root@elliot-01:~/helm/helm-treinamento# helm list --namespace monitoring
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
prometheus      monitoring      1               2022-03-01 21:32:23.676253595 +0000 UTC deployed        prometheus-15.5.2       2.31.1


Listando os pods deployados pelo chart prometheus-community/prometheus
root@elliot-01:~/helm/helm-treinamento# kubectl get pods --namespace monitoring
NAME                                             READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-79848df6cc-chr29         2/2     Running   0          5m22s
prometheus-kube-state-metrics-6c44ff7fb6-qdnjc   1/1     Running   0          5m22s
prometheus-node-exporter-bncgw                   1/1     Running   0          5m22s
prometheus-node-exporter-j8rxf                   1/1     Running   0          5m22s
prometheus-pushgateway-6c68758cc5-m7mgg          1/1     Running   0          5m22s
prometheus-server-5dc5f8ff9f-8xx5c               2/2     Running   0          5m22s

Listando os services...
root@elliot-01:~/helm/helm-treinamento# kubectl get svc --namespace monitoring
NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager         ClusterIP   10.96.230.82     <none>        80/TCP     6m54s
prometheus-kube-state-metrics   ClusterIP   10.105.95.156    <none>        8080/TCP   6m54s
prometheus-node-exporter        ClusterIP   None             <none>        9100/TCP   6m54s
prometheus-pushgateway          ClusterIP   10.99.186.238    <none>        9091/TCP   6m54s
prometheus-server               ClusterIP   10.100.234.104   <none>        80/TCP     6m54s

Vamos editar o service 'prometheus-server', mudar de 'ClusterIp' para 'NodePort'

# kubectl edit service -n monitoring prometheus-server

...
selector:
    app: prometheus
    component: server
    release: prometheus
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
...

Se verificarmos de novo o serviço 'prometheus-server', veremos que temos uma porta exposta...

# kubectl get svc -n monitoring

NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager         ClusterIP   10.96.230.82     <none>        80/TCP         11m
prometheus-kube-state-metrics   ClusterIP   10.105.95.156    <none>        8080/TCP       11m
prometheus-node-exporter        ClusterIP   None             <none>        9100/TCP       11m
prometheus-pushgateway          ClusterIP   10.99.186.238    <none>        9091/TCP       11m
"prometheus-server               NodePort    10.100.234.104   <none>        80:32595/TCP   11m"

Basta utilizar o IP público de uma instância, liberar este porta (32595) no grupo de segurança e acessar pelo browser...

Agora faremos a instalação do 'Grafana'...

# helm search repo bitnami

NAME                                            CHART VERSION   APP VERSION     DESCRIPTION 
bitnami/grafana                                 7.6.16          8.4.2           Grafana is an open source metric analytics and ...

Para sorte, o 'Chart' do 'Grafana' não está deprecated no repositório bitnami (recomendado para início na documentação do Helm)

Para instalação do grafana precisamos de umas flags um pouco diferentes...

Comparando sintaxes...Verificamos algumas diferenças na sintaxes das flags da instalação do prometheus para o grafana, porém, fiz isso só
para verificar que as duas funcionam, o problema que deu a primeira vez foi pela forma que foi passado o nome e pelas versões deprecated do
prometheus...

De fato o nome não se passa mais assim '--name=', e quando a versão está deprecated corre muito risco de não ser instalada por causa das versões
dos yamls...

Sintaxe instalação Prometheus:
# helm install prometheus prometheus-community/prometheus --namespace monitoring --version 15.5.2 --set alertmanager.persistentVolume.enabled=false,server.persistentVolume.enabled=false

Sintaxe instalação Grafana:
# helm install grafana --namespace=monitoring --version=7.6.16 --set=adminUser=admin,adminPassword=admin,service.type=NodePort bitnami/grafana
"
NAME: grafana
LAST DEPLOYED: Tue Mar  1 22:00:37 2022
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: grafana
CHART VERSION: 7.6.16
APP VERSION: 8.4.2

** Please be patient while the chart is being deployed **

1. Get the application URL by running these commands:
    export NODE_PORT=$(kubectl get --namespace monitoring -o jsonpath="{.spec.ports[0].nodePort}" services grafana)
    export NODE_IP=$(kubectl get nodes --namespace monitoring -o jsonpath="{.items[0].status.addresses[0].address}")
    echo http://$NODE_IP:$NODE_PORT

2. Get the admin credentials:

    echo "User: admin"
    echo "Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
"

Vamos verificar se o Pod do Grafana já está rodando...
# kubectl get pod -n monitoring

NAME                                             READY   STATUS    RESTARTS   AGE
grafana-d556584c-7rmks                           0/1     Pending   0          7m9s

Foi necessário criar um 'PV' com nome 'grafana' para o Pod do Grafana...Características '10Gi' e 'ReadWriteOnce'...

# kubectl get pods -n monitoring
NAME                                             READY   STATUS              RESTARTS   AGE
grafana-7b7fbffc76-tq9bm                         0/1     ContainerCreating   0          9m34s

# kubectl get pods -n monitoring
NAME                                             READY   STATUS    RESTARTS   AGE
grafana-7b7fbffc76-tq9bm                         0/1     Running   0          9m51s

# kubectl get pods -n monitoring
NAME                                             READY   STATUS    RESTARTS   AGE
grafana-7b7fbffc76-tq9bm                         1/1     Running   0          10m


Listando os serviços mais uma vez...
# kubectl get svc -n monitoring
NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
grafana                         NodePort    10.111.101.122   <none>        3000:32582/TCP   12m


Basta usar o IP público de uma das instâncias do cluster e utilzar a porta '32582', veremos o painel do grafana de login
para logar usuário 'admin' e com o comando: kubectl get secret grafana-admin --namespace monitoring -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode
conseguiremos a senha...

Agora para testar o Prometheus e o Grafana utilizaremos a aplicação que estavamos utilizando nas aulas passadas...
O YAML é o seguinte: 'app-v1.yaml'...

Nele temos duas annotations: prometheus.io/scrape: "true" e prometheus.io/port: "32111"

Com estas annotations estamos passando informações para o nosso cluster Kubernetes e o serviço do Prometheus que está em execução fica
procurando por esta annotations nos deployments que estão em execução, ele vê a annotation '...scrap: true' ele vê que precisa pegar
as métricas deste deployment, a segunda annotation "(...port: 32111)" está dizendo por qual porta o Prometheus vai pegar as métricas...

Lembrando que na aplicação, na imagem tem que estar preparadas nesta porta para passar as métricas, chave e valor...

Vamos fazer o deploy da aplicação:
# kubectl create -f app-v1.yml

Obs: Tive que criar um servico para este deployment para conseguir ver as metricas pois ja tinha deletado tudo que tinha feito
das outas aulas...

# kubectl get svc
NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                        AGE
giropops-v1-65df785568-jpfx4   NodePort    10.97.229.29   <none>        80:32222/TCP,32111:32111/TCP   19h
kubernetes                     ClusterIP   10.96.0.1      <none>        443/TCP                       122d


Acesso ao IP publico de qualquer nó do cluster com a porta 32222 acessamos a aplicação
Acesso ao IP publico de qualquer nó do cluster com a porta 32111 acessamos o servico (para métricas = /metrics)

Com '/metrics' verificamos quais são as métricas que estão sendo coletadas.

No Prometheus conseguimos acessar qualquer uma destas métricas...

Também no Grafana escolhemos "Adicionar o Data Source", que no caso foi o Prometheus

Depois criamos um dashboard com a consulta abaixo...

Query do Grafana (Requisições HTTP, para a app 'giropops', a cada 5 minutos, e irá separar por versões)
sum(rate(nginx_http_requests{app="giropops"}[5m])) by (version)

Obs: A parte de fazer um deploy com a versão 2 e deixar o 'while true' rodando eu não fiz...



-*- Ingress - Parte 01

Precisamos do Ingress para poder expor nossas aplicações na núvem...

Para expor nossa aplicação normalmente utilizamos o LoadBalancer ou NodePort, aqui faremos diferente...

Utilizaremos o Ingress para que através de um endereço possamos expor vários serviços. Assim os serviços não precisam estar utilizando
um LoadBalancer ou um NodePort.

Utilizaremos o Ingress controller do Nginx.
Um detalhe que diferencia o Nginx dos outros IngressController é que ele tem um default backend que retorna um 404 quando a pessoa
tenta acessar um serviço que não existe.

No nosso exemplo teremos dois deployments, dois serviços, giropops e o strigus...

Criamos um deployment para cada uma 'app1' e 'app2' utilizando uma imagem do docker de exemplo que apenas imprime um nome na página.

'dockersamples/static-site'

O código destes deployments estão na pasta 'day-5-ingress'

A única diferença entre os YAMLs é que um é identificado em nomes e labels como app1 e o outro como app2. A variável de ambiente que
define o nome que é exibido na página através da imagem docker na app1 é "GIROPOPS" e na app2 é "STRIGUS".

tip: Substituindo conteúdo do yaml

:%s/app1/app2/g

Agora vamos criar estes deployments...

kubectl create -f app1.yaml
kubectl create -f app2.yaml

Então criamos os serviços...

O conteúdo dos arquivos dos services estão na pasta "day-5-ingress"

Depois...

kubectl create -f svc-app1.yaml
kubectl create -f svc-app2.yaml

Temos um comando onde podemos ver quais são os endpoints...

kubectl get endpoints ou kubectl get ep...Onde veremos quais são os IPs disponíveis para cada pode em determinado deployment.

root@elliot-01:~/day-5-ingress# kubectl get endpoints
NAME         ENDPOINTS                   AGE
appsvc1      10.40.0.1:80,10.44.0.1:80   4m38s
appsvc2      10.40.0.2:80,10.44.0.2:80   4m26s
kubernetes   172.31.7.162:6443           133d

Podemos fazer um curl nos endpoints fornecidos...

# curl 10.40.0.1.:80

Ao final do retorno veremos a identificação do deployment que acabamos de criar...

"...
<br></p>

<h1 id="toc_0">Hello GIROPOPS!</h1>

<p>This is being served from a <b>docker</b><br>
container running Nginx.</p>


</body>

</html>
"

Pegando todos os recursos desejados...

# kubectl get deploy,svc,ep,po

"NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/app1   2/2     2            2           7m12s
deployment.apps/app2   2/2     2            2           7m4s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/appsvc1      ClusterIP   10.108.120.82    <none>        80/TCP    10m
service/appsvc2      ClusterIP   10.103.103.220   <none>        80/TCP    10m
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   133d

NAME                   ENDPOINTS                   AGE
endpoints/appsvc1      10.40.0.1:80,10.44.0.1:80   10m
endpoints/appsvc2      10.40.0.2:80,10.44.0.2:80   10m
endpoints/kubernetes   172.31.7.162:6443           133d

NAME                        READY   STATUS    RESTARTS   AGE
pod/app1-587cbf8fbf-8gdw5   1/1     Running   0          7m12s
pod/app1-587cbf8fbf-bzxzr   1/1     Running   0          7m12s
pod/app2-d4d554b75-5jbxf    1/1     Running   0          7m3s
pod/app2-d4d554b75-fhzl6    1/1     Running   0          7m3s"


-*- Ingress - Parte 02

Sobre o 'default-backend' que falamos antes...Podemos customiza-lo...

vim default-backend.yaml

...Este arquivo se encontra dentro da pasta 'day-5-ingress'...

O conteúdo do arquivo do default-backend que vamos utilizar está disponível no repositório do Google...
Não criamos, só estamos usando...

Sobre atributos que não temos falado muito:

'terminationGracePeriodSeconds: 60' é o tempo em segundos que ele espera para o container ser finalizado (com educação rsss) 
usando o sinal '-15' do 'sigkill', se passar de 60 segundos e o container não terminar ele vai utilizar o 'kill -9'

'livenessProbe' "Prova de vida de tempos em tempos"

livenessProbe:
          httpGet: # Metodo
            path: /healthz # Caminho que responderá
            port: 8080 # Porta
            scheme: HTTP # Protocolo
          initialDelaySecond: 30 # Tempo de tolerância para iniciar a verificação, temos que dar um tempo pro container subir
          timeoutSeconds: 5 # Tempo de espera da resposta após começar a verificar

'readnessProbe' "Quando o container está pronto para receber requisições"

Para deployar o 'default-backend' criamos um outro namespace...'ingress'

# kubectl create namespace ingress

# kubectl create -f default-backend.yaml -n ingress

# kubectl get deploy -n ingress

Criamos o serviço 'backend-default' ...# vim svc-default-backend.yaml

Este arquivo está na pasta 'day-5-ingress'

# kubectl create -f svc-default-backend.yaml -n ingress

# kubectl get svc -n ingress

# kubectl get ep -n ingress

NAME              ENDPOINTS                       AGE
default-backend   10.40.0.3:8080,10.40.0.4:8080   3m23s

# curl 10.40.0.3:8080

'default backend - 404'

Agora vamos criar um 'configMap' para o nosso IngressController...

# vim nginx-ingress-controller-config-map.yaml

Adicionamos a seguinte variável no nosso config-map..."enable-vts-status: 'true'"

Isso irá ativar no nosso Nginx o recurso de capturar o estado dele, sobre requisição etc...

# kubectl create -f nginx-ingress-controller-config-map.yaml -n ingress

# kubectl get configmap -n ingress

NAME                            DATA   AGE
kube-root-ca.crt                1      21m
nginx-ingress-controller-conf   1      29s

Podemos descreve-lo para que possamos verificar se o valor foi setado corretamente...

# kubectl describe configmap -n ingress nginx-ingress-controller-conf

"
Name:         nginx-ingress-controller-conf
Namespace:    ingress
Labels:       app=nginx-ingress-lb
Annotations:  <none>

Data
====
enable-vts-status:
----
true

BinaryData
====

Events:  <none>
"

tip: Pegando uma quantidade determinada de últimos comandos...

# history | tail -n 20


-*- Ingress - Parte 03

Agora de fato entrando na estrutura do Ingress...

Primeira coisa que iremos fazer é criar uma 'ServiceAccount'

# vim service-account-nginx-ingress-controller.yaml

Segunda coisa iremos criar uma 'ClusterRole', ClusterRole é um conjunto de regras que determinados usuários podem fazer

# vim clusterrole-nginx-ingress-controller.yaml

Terceira coisa iremos criar uma 'ClusterRoleBinding', Isso vai associar uma 'ServiceAccount' a uma 'ClusterRole'...

# vim clusterrolebinding-nginx-ingress-controller.yaml


Aplicando os 3 recursos ao cluster...

# kubectl create -f service-account-nginx-ingress-controller.yaml -n ingress
# kubectl create -f clusterrole-nginx-ingress-controller.yaml -n ingress
# kubectl create -f clusterrolebinding-nginx-ingress-controller.yaml -n ingress

Arquivos se encontram na pasta 'day-5-ingress'

tip: Dentro do vim 'set nu', para numerar as linhas...


Agora faremos o deploy do "nginx ingress controller"

# vim nginx-ingress-controller-deployment.yaml

tip: Chave/Valor do selector sempre tem que bater com labels

tips: Pegar todos os recursos que ainda estão vinculados ao namespace
# kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n "namespace"

Obs: Os recursos para fazer o deploy do ingress-nginx da aula estavam muito antigos...Utilizei os recursos do github do k8s
https://github.com/kubernetes/ingress-nginx/blob/main/deploy/static/provider/aws/deploy.yaml



O deploy que acabamos de fazer é como se estivessemos extendido o Kubernetes, demos um novo componente à ele...

Agora iremos criaremos objetos do tipo ingress, podemos criar regras agora...

# vim nginx-ingress.yaml

O serviço está disponível, se disponibilizado através de um nodeport, através de qualquer IP do cluster,
mas para utilizarmos o ingress-controller, a rota através do ingress, temos que utilizar um dns de um dos hosts
ou um loadbalancer na regra.

ingressclass: k8s.io/ingress-nginx
