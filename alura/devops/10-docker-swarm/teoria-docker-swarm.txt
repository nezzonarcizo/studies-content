CMD na pasta de instalação do Virtual Box -> VBoxManage modifyvm "Virtual Machine Name" --nested-hw-virt on //Isto faz com que a opção "Habilitar VT-x/AMD-V Aninhado" habilite
No PowerShell como administrador -> bcdedit /set hypervisorlaunchtype off //Isto faz com que o Hyperviser (Hyper-V) desabilite para as VMs funcionarem, mas o WSL para de funcionar
Encurtando o caminho no Gitbash: PS1="${debian_chroot:+($debian_chroot)}\[\033[01;34m\]\W \[\033[31m\]\$\[\033[00m\] "

Aula 01: Cluster com Docker Swarm -------

    O que é Docker Swarm?

        Vamos entender porque utilizar o docker da forma que estavamos utilizando não é o suficiente.

        O que podemos fazer com o Docker? Subir vários containers, com diversos serviços. Subimos 3, 6, 9 e assim por diante...O que acontece quando temos um número
        imenso de containers? 30, 60, 90 etc?

        Vamos lembrar que cada container é um processo a mais na máquina (Docker host).

        Então, em cada container temos um serviço e externamente, podemos ter diversos processos o que pode fazer com que a nossa máquina/servidor não aguente pois
        todos estarão consumindo muitos recursos, sejam eles, memória, processamento etc. Não sabemos exatamente o que está sendo executado em cada um destes containers.

        O que pode ocorrer é que um dos container falhe, talvez outros containers dependam do serviço que estava rodando neste container que caiu. Isto faz com que os
        outros container parem de funcionar mesmo que de maneira indireta. Isto demandará intervensão manual para que o container volte a funcionar da forma que antes
        estava funcionando. É neste momento que o Docker Swarm entra para resolver as coisas de uma forma "elegante".

        Como assim? Vamos imaginar aqui que temos agora uma rede de computadores, e o que podemos fazer com uma rede de computadores? Posso criar um cluster, um conjunto
        de máquinas agora para dividir determinado processamento, então, por exemplo, no primeiro caso qual é o problema? Que toda uma única máquina ali tem que carregar
        todos aqueles outros containers de uma vez só e isso poderia acabar sobrecarregando ela.

        Agora tendo um cluster, o que podemos fazer? Podemos instalar o Docker em todas essas máquinas de um mesmo cluster, que vamos ver como vai ser criado, e dividir
        esses containers, toda essa carga entre essas máquinas dentro de um cluster, e agora não vamos sobrecarregar mais uma máquina em específico, mas vamos dividir 
        toda essa carga entre elas.

        Isso também significa que vamos conseguir resolver aquele problema, também queremos que caso um container pare de funcionar ele volte ao normal da forma que 
        deveria. Como resolveremos isso? Precisamos ter alguém que faça o papel de orquestrar, de fazer todo esse controle do que vai ser colocado e onde, como ele vai
        fazer os serviços voltarem a funcionar no caso de pararem.

        E qual é o papel do orquestrador da forma que conhecemos? É reger uma orquestra, que nesse caso vão ser os próprios containers, e quem vai ser o orquestrador?
        Será o Docker Swarm, ele vai fazer o papel de definir em qual máquina vai ser melhor rodado determinado container, se um container está pronto para ser reiniciado
        em caso de falha ou não, e assim por diante.

        O Docker Swarm vai agir como um orquestrador num Cluster de máquinas. Através de um dispatcher ele vai definir qual máquina é a mais apropriada para rodar 
        determinado container.

        Ele faz de maneira autônoma a escolha da máquina para rodar determinado container. Isso já é uma coisa bem interessante porque já conseguimos evitar aqueles
        problemas todos de sobrecarga que podem acontecer.

        E como dito antes, pode ser que algum container também por algum motivo falhe, e o que o Docker Swarm consegue fazer? Ele consegue através de políticas de restart
        definir que esse contêiner vai voltar a funcionar de maneira autônoma, diretamente sem nenhuma intervenção, então o que estava ali com problema, vai parar de ter
        problema porque um novo serviço vai ser instanciado ali para nós.

        Vamos entender como vamos começar a criar o primeiro cluster, começar a criar um sistema com várias máquinas.


    Usando a Docker Machine:

        Já vimos que o Docker Swarm serve para que tenhamos diversas máquinas para dividir o nosso processamento. O legal é que com o Docker Machine não precisaremos de diversas máquinas para
        fazer isso, trabalharemos com apenas uma máquina física e as demais serão virtuais.

        Ela não é ferramenta crucial para que utilizemos o Docker Swarm, mas nos ajudará muito neste curso para fins didáticos. As máquinas virtuais que o Docker Machine criará formará o nosso
        cluster.

        A questão agora é...Como iremos criar estas máquinas virtuais? Como vamos utilizar o Docker Machine? Instalaremos o Docker Machine de acordo com este link:

            https://docs.docker.com/machine/install-machine/
        
        Com o comando '$ docker-machine' vemos uma sequência de comandos e instruções da aplicação...Vários comandos são bem parecidos com os do próprio Docker...
        Por exemplo:

            $ docker-machine ls - Irá nos listar todas as máquinas virtuais que criamos até o momento com o 'docker-machine'...

            $ docker-machine ls
            NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS

        Agora se quisermos criar uma Docker Machine, uma máquina virtual pronta para rodar o Docker precisaremos rodar o comando:

            $ docker-machine create ...

            precisamos também informar o driver que o docker-machine utilizará para criar esta máquina virtual, então por exemplo, existem diversos drivers, mas o que vamos utilizar aqui neste
            curso, por ser o mais famoso e o mais tranquilo de se instalar, será o VirtualBox...

            Então além de instalarmos o Docker Machine , nós também vamos precisar instalar o VirtualBox (Já realizado em cursos anteriores), e como informaremos isso? Acrescentando um '-d'ao 
            comando junto com 'virtualbox', por fim, colocamos o nome que queremos dar a nossa máquina virtual, neste curso no caso estou usando o mesmo que o instrutor 'vm1', então damos 'Enter'
            e agora ele começará a criar esta máquina...

            $ docker-machine create -d virtualbox vm1
            
            A saída será parecida com esta:
            
                Running pre-create checks...
                Creating machine...
                (vm1) Copying C:\Users\nesso\.docker\machine\cache\boot2docker.iso to C:\Users\nesso\.docker\machine\machines\vm1\boot2docker.iso...
                (vm1) Creating VirtualBox VM...
                (vm1) Creating SSH key...
                (vm1) Starting the VM...
                (vm1) Check network to re-create if needed...
                (vm1) Windows might ask for the permission to configure a dhcp server. Sometimes, such confirmation window is minimized in the taskbar.
                (vm1) Waiting for an IP...
                Waiting for machine to be running, this may take a few minutes...
                Detecting operating system of created instance...
                Waiting for SSH to be available...
                Detecting the provisioner...
                Provisioning with boot2docker...
                Copying certs to the local machine directory...
                Copying certs to the remote machine...
                Setting Docker configuration on the remote daemon...
                
                This machine has been allocated an IP address, but Docker Machine could not
                reach it successfully.
                
                SSH for the machine should still work, but connecting to exposed ports, such as
                the Docker daemon port (usually <ip>:2376), may not work properly.
                
                You may need to add the route manually, or use another related workaround.
                
                This could be due to a VPN, proxy, or host file configuration issue.
                
                You also might want to clear any VirtualBox host only interfaces you are not using.
                Checking connection to Docker...
                Docker is up and running!
                To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: C:\Users\nesso\bin\docker-machine.exe env vm1

        Este processo demora um tempo, onde baixa todos os arquivos necessários...
        Se tudo der certo, ao final, somos informados que a máquina está rodando e que agora podemos nos conectar à ela...Podemos utilizar agora aquele comando para listar todas as máquinas
        virtuais em execução...

            $ docker-machine ls

            Agora nos é apresentada a seguinte, ou semelhante, saída:

            NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
            vm1    -        virtualbox   Running   tcp://192.168.99.106:2376           v19.03.12

            Temos as informações do nome da máquina virtual, o driver que ela está utilizando, a URL e a versão do docker que está instalada na máquina...
            Podemos nos conectar à esta máquina realizando o seguinte comando:

                $ docker-machine ssh vm1

                A seguinte saída nos é apresentada:

                    ( '>')
                   /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
                  (/-_--_-\)           www.tinycorelinux.net
        
        
            Se caso tivessemos criado a máquina mas ela não estivesse rodando, poderíamos iniciá-la com o comando:

                $ docker-machine start "nome da vm"

            Se utilizarmos este comando agora com a máquina que acabamos de criar, seremos informados que a máquina já está rodando...

                Starting "vm1"...
                Machine "vm1" is already running.

            Agora que já aprendemos o básico com docker-machine, vamos avançar com o curso...

        A partir de agora, iremos criando as nossas máquinas virtuais conforme formos precisando e, por fim, construir o nosso cluster...Só pra frizar mais uma vez, o que é um Cluster: Nada mais
        que um conjunto de máquinas, físicas ou não, que dividem o processamento de determinadas aplicações, criaremos o nosso Swarm a partir destas máquinas virtuais leves.
        
        Então, só para dar aquela finalizada aqui, se dermos um 'docker -v' dentro da máquina virtual que criamos, ela vai listar a mesma versão que foi listada pela Docker Machine com o comando
        '$ docker-machine ls', confirmando aqui que temos uma máquina virtual já com o Docker instalado. A ideia agora é ver como vamos criar o nosso primeiro cluster.

    
    Para saber mais: Cloud e Docker Machine:

        Durante o curso, utilizaremos a Docker Machine apenas para criarmos nosso ambiente com diversas máquinas isoladas e iniciarmos nosso cluster.
        Porém, a Docker Machine também é muito utilizada com provedores de serviço em nuvem, como a AWS! Podemos definir nossas credenciais, e, utilizando o driver amazonec2, temos a 
        possibilidade de criar diversas máquinas nos servidores da Amazon!        
        Caso tenha interesse, mais informações podem ser obtidas na documentação oficial.

    
    Criando o Cluster:

        A partir da primeira máquina virtual que criamos, iremos iniciar o nosso cluster...
        A partir de agora, sempre que formos nos referir ao nosso cluster, vamos chamar ele de Swarm, porque além de ser o nome da ferramenta, ele remete ao coletivo de animais em inglês,
        enxame, bando, alcateia, e tem toda essa ideia de trabalho coletivo, de divisão de trabalho assim como queremos fazer com o Docker Swarm.

        O que precisamos agora para criar o nosso Cluster? Precisamos acessar a nossa máquina virtual recém-criada, e já vimos que para fazer isso, executamos o comando:
            
            $ docker-machine ssh vm1

        Agora precisamos ececutar um comando bem simples que é:

            $ docker swarm init (Já dentro da máquina, nunca esquecer disso)

        Nos é apresentado o seguinte erro:

            docker@vm1:~$ docker swarm init
            Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 192.168.99.106 on eth1) 
            - specify one with --advertise-addr

        Se lermos o erro com calma, veremos que ele não conseguiu anunciar o IP que vai ser utilizado por essa máquina dentro do Swarm, ele tem tanto a possibilidade de usar o IP da máquina
        virtual quanto o IP da minha máquina física, que são os dois drivers que ele está se referenciando o eth0 (10.0.2.15) e o eth1 (192.168.99.106). Ele está dizendo que eu tenho que
        especificar o IP que eu quero através da '--advertise-addr'.

        Mesmo que não tivessemos este erro, é sempre boa prática usarmos essas flags da '--advertise-addr', porque eu preciso fixar um IP para a minha máquina criadora do meu Swarm, para que 
        todas as máquinas que foram entrar daqui para frente, elas consigam manter uma comunicação fixa e estável com quem criou o Swarm.

        Então agora vamos tentar iniciar o Swarm da forma correta com o comando:

            $ docker swarm init --advertise-addr "IP da máquina "Física""

        "Física", porque na verdade é o IP da máquina virtual que estamos utilizando...
        Agora temos a mensagem de que o Swarm foi inicializado:

            Swarm initialized: current node (uf6tlac8zhq0m8an77xqgkjhx) is now a manager.

            To add a worker to this swarm, run the following command:
            
                docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377
            
            To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

        Como temos a certeza de que esta máquina agora está em um Swarm?
        Utilizamos o comando:

            $ docker info

            Na parte 'Swarm:' ela deve estar 'active'

        Então todo o trabalho que teríamos se quiséssemos implementar essa ideia de divisão de carga e de políticas de restart na mão, já foi feita com um simples comando, e agora ele está
        falando que além do Swarm ter isdo incializado, o meu nó corrente tem esse ID (uf6tlac8zhq0m8an77xqgkjhx) e é um manager, e quem é esse sinal corrente? É exatamente essa máquina que
        criou o Swarm, porque a máquina que criou o Swarm, ela a partir desse momento vai ser considerada manager e líder desse Swarm.

        E agora o que ela precisa ter? Workers ou "Trabalhadores" para realizar as tarefas árduas que vão ser impostas daqui pra frente. A partir de agora precisamos ententer o que são esses
        tais de workers, como adicionar eles, qual o papel deles dentro do nosso Swarm e será que precisamos adicionar outros managers?

    
    Nesta aula, aprendemos:

        - O Docker Swarm é um orquestrador
        - O Docker Swarm é capaz de alocar e reiniciar containers de maneira automática
        - Como criar máquinas já provisionadas para utilizar o Docker com a Docker Machine utilizando comando docker-machine create
        - Um cluster é um conjunto de máquinas dividindo poder computacional
        - Como criar um cluster utilizando o Docker Swarm utilizando o comando docker swarm init

        
    
    Questões aula 01:

        01: O Docker Swarm apresenta uma série de vantagens em relação ao Docker usado de maneira tradicional. Marque as alternativas que contém essas vantagens:

            Selecione 2 alternativas

            R1: O Docker Swarm divide os containers entre múltiplas máquinas de um mesmo cluster de maneira automática.
                Alternativa correta! Através do dispatcher o Docker Swarm define a melhor máquina para executar algum container

            R2: O Docker Swarm consegue resetar containers automaticamente em caso de falhas.
                Alternativa correta! O Docker Swarm tem capacidade de reiniciar containers a fim de manter a aplicação funcionando.

        
        02: Vimos na última aula que a Docker Machine, por mais que não esteja relacionada diretamente ao Docker Swarm, pode nos ajudar bastante. Qual das alternativas abaixo contém uma 
            funcionalidade da Docker Machine?

            Selecione uma alternativa

            R: Ao utilizar a Docker Machine, podemos criar máquinas virtuais prontas para executar Docker.
               Alternativa correta! A Docker Machine cria máquinas virtuais bem leves já provisionadas com o Docker.

        
        03: Queremos criar nosso primeiro cluster para dividir os containers em diversas máquinas e não sobrecarregar uma única máquina. Qual dos comandos abaixo devemos utilizar para criar o 
            cluster e darmos o primeiro passo para atingir nosso objetivo?

            Selecione uma alternativa

            R: docker swarm init
               Alternativa correta! Além disso, a boa prática também seria utilizar a flag --advertise-addr.


Aula 02: Responsabilidade dos nós workers -------

    Criando o primeiro worker:

        Temos até agora o nosso Swarm, temos uma máquina fazendo parte desse Swarm, é um nó, que é o nosso manager, o nosso líder até então, mas precisamos agora de alguém para realizar os
        trabalhos, que carrega os contêiners e faça toda a aparte mais pesada de processamento.

        Ou seja, temos o manager que é a máquina principal, mas precisamos das outras que ajudarão a manager a gerenciar, para isto que serve o Swarm.

        Vamos ter o nosso líder, e ele terá seus "subordinados", e esses subordinados vão fazer exatamente o papel de carregar nossos containers enquanto o principal cordena tudo que vai
        acontecer, vimos nas primeiras aulas que o computador principal faz o papel de orquestrador enquanto teremos os caras que vão ser responsáveis pelo carregamento das informações,
        processamento de containers e tudo mais.

        A pergunta que fica agora se formos observar é, como vamos criar esses dois workers? Se formos pensar um pouco mais abstrato, estamos colocando duas novas máquinas, dois novos nós dentro
        desse Swarm, precisamos criar duas novas máquina virtuais, e para criarmos novas máquinas, já sabemos que é só utilizar aquele clássico comando 'docker-machine create -d virtualbox' e 
        aqui passamos o nome da VM. No caso segui a sequencia como o instrutor e coloquei 'vm2' e 'vm3'.

        A questão agora é como estes caras vão ser inseridos no Swarm, se buscarmos no terminal, quando criamos o nosso Swarm ele já deu esse comando. Rode o seguinte comando para adicionar mais
        nós ao Swarm (docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377).

           $ docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377

        Como ele sabe que o Swarm que tem que se conectar é o que criamos? Por causa do IP! 192.168.99.106 na porta 2377. Esta é a nossa manager, que é a porta que o Docker Swarm necessita para
        fazer a comunicação entre os nós que vão fazer parte de um Swarm.

        Após terminar de criar as máquinas teremos que acessá-las através do comando 'docker-machine ssh 'nomeDaMaquina''. Depois de estar dentro da máquina, basta usar o comando informado 
        na criação do Swarm, aquele que citamos acima, e colar na linha de comando, dentro da máquina virtual acessada. Veremos a seguinte saída:

            docker@vm2:~$ docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377
            This node joined a swarm as a worker.

        O comando é grande, mas não precisamos decorá-lo. Tecnicamente não teriamos mais aquele comando disponível no terminal, após executar um clear, exit etc...
        O que precisamos fazer se quisermos recuperar aquele comando para que possamos adicionar mais workers? É simples...Basta que executemos o seguinte comando dentro da máquina manager:

            docker@vm1:~$ docker swarm join-token worker
            To add a worker to this swarm, run the following command:
            
                docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377

        Ali ele nos retornou o comando necessário para adicionar novos workers...

        Então agora precisamos entender, depois que nós já adicionamos esse primeiro líder e agora temos nosso workers, como esses caras vão conseguir se comunicar, como conseguimos listar o
        que é que tem dentro do nosso Swarm. Precisamos arrumar um pouco a casa para entender que no fim das contas esse líder vai fazer o papel de orquestrar, de definir tudo que vai acontecer
        aqui dentro, enquanto nossos workers vão fazer o trabalho de carregar os nossos containers.

    Listando e removendo nós:

        Até o momento aprendemos a criar o nosso Swarm com um manager dentro, conseguimos já adicionar workers também a esses Swarms, mas ainda precisamos saber como listar quais são os nós e
        quantos são eles dentro do Swarm e como conseguimos remover também os nós a partir do momento que eles estão fazendo parte de um Swarm.

        Apesar de nós sabermos no momento que adicionamos um manager e dois workers, totalizando três nós, precisamos de uma forma sucinta para consultar isso, por isso temos o comando:

            $ docker node ls

            Ele lista para nós o ID, o hostname, o status, se está disponível, se é lider ou não do swarm e a versão da "engine" instalada, ou seja, a versão do docker que eles estão rodando.

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12
            tycqgy7kndfjaznweg11xprsg     vm3                 Ready               Active                                  19.03.12

        Então por exemplo, ele listou os IDs dos nossos nós e no nó que rodou o comando ele colocou um asterisco. Vemos que é nossa vm1 que rodou o comando, o status é 'ready', que significa
        que está rodando sem nenhum problema.

        O que os outros nós diferem do primeiro? Que eles não tem a informação em 'MANAGER STATUS', se eles não tem nada de manager status dentro deles, significa para o Swarm que eles são
        workers, então esses dois caras aqui por não terem informações nessa coluna, significa que são apenas workers do Swarm.

        Mais uma outra pergunta que ficaria: Será que eu posso executar esse 'docker node ls' em qualquer nós do meu Swarm? Se conectarmos ao nosso segundo nó, 'vm2', e tentarmos executar o 
        mesmo comando, teremos a seguinte saída: 
        
            "Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the 
                current node to a manager."

            Ele detalha tudo aqui perfeitamente para nós, ele fala "Esse nó não é um manager do Swarm. Workers nnão podem ser usados para visualizar ou alterar o estado do seu cluster. Por favor
            rode este comando em um nó manager ou promova o nó, no qual você se encontra, para manager."

        Isto significa que se tentassemos na nossa vm3 ou em outros nós que não sejam manager teremos a mesma mensagem de erro.

        Uma coisa que falta ainda, agora que conseguimos adicionar e listar os nossos nós, precisamos fazer esse papel de remoção. Podemos tentar remover um nó primeiro executando o comando:

            $ docker node ls

            Então verificamos qual o é o ID do nó que queremos remover, e então executamos o comando:

            $ docker node rm "ID do nó"

        Receberemos a seguinte saída:
                    
            "Error response from daemon: rpc error: code = FailedPrecondition desc = node tycqgy7kndfjaznweg11xprsg is not down and can't be removed"

            Aqui diz que:

            "Este nó não está down e não pode ser removido"

        Então, para que possamos deixar nossa vm3 como down, temos que nos conectar à ela e executar o seguinte comando:

            $ docker swarm leave

            Receberemos a seguinte saída:

                "Node left the swarm."

        Apesar de que um nó não tem autonomia para alterar o Swarm, ele é responsável pela mudança de seu status, ação pré-requerida para que seja removido do Swarm pelo nó líder...
        Sabendo disso agora voltamos ao nosso nó principal e executamos novamente o comando: $ docker node rm "ID do nó"

        Primeiro vamos executar o comando: "$ docker node ls" para verificar se realmente o status do nós 3 foi modificado

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12
            tycqgy7kndfjaznweg11xprsg     vm3                 Down                Active                                  19.03.12

            Podemos verificar que agora o STATUS do nó 3 está Down...Vamos ao comando de remover o nó agora...

            $ docker node rm tycqgy7kndfjaznweg11xprsg
            
            Agora se executarmos mais uma vez o comando: "$ docker node ls" veremos a seguinte saída...

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12

            Confirmamos que o nó 3 foi removido...Quem realmente executou a exclusão do nó foi o nosso 'Leader'
        
        Agora vamos readicionar o nó 3 ao Swarm...
        Realizamos o comando "docker swarm join-token worker" para pegar o comando com a chave necessária para adição...

            Nossa saída...

            To add a worker to this swarm, run the following command:

            docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377

        Utilizamos este comando no nó 3 novamente...

            "This node joined a swarm as a worker."

            Executamos agora o comando "docker node ls" novamente no nó líder...

            E...O nó 3 está presente mais uma vez...

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12
            u1hquwd0mnvktpn9d7wcqvogu     vm3                 Ready               Active                                  19.03.12

        Até o momento então aprendemos como:

            Listar, adicionar, criar um cluster e outras operações básicas dentro do Swarm.

    
    Subindo um serviço:

        Chegou o momento em que vamos criar o nosso primeiro container dentro do nosso Swarm. Onde iremos subir este container? O ideal é que façamos isso em um dos nosso workers, ou seja, na
        vm1 ou vm3, pois é o trabalho deles executar os container, fazer toda a carga pesada, enquanto, a princípio, o trabalho de orquestração e de leitura de escritas vai ser realizada pelo
        nosso manager.

        Se queremos subir um container, já aprendemos isso no curso de docker, podemos ir no terminal do nó que desejamos, por exemplo, vm2 que é um dos nossos workers e executar aquele comando
        "docker container run" passando a porta em que queremos fazer o binding (Mapear atributos em requisições HTTP para objetos ou mapas), no caso a porta "8080" da nossa vm2 com a porta "3000"
        do container que criamos, colocamos para isso o "-d" para não travar o nosso terminal, e por fim colocamos "aluracursos/barbearia" que é o nome da imagem que iremos utilizar...

            $ docker container run -p 8080:3000 -d aluracursos/barbearia

            O download da imagem irá iniciar...

        Após o download podemos executar um "docker container ls" para verificar que o nosso container está em execução, e podemos conferir que ele está fazendo binding da porta 8080 com a 3000.

            CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                    NAMES
            e851f40e17bf        aluracursos/barbearia   "/bin/sh -c 'node se…"   2 minutes ago       Up 2 minutes        0.0.0.0:8080->3000/tcp   laughing_blackbur

        Como é que acessamos essa aplicação agora? A ideia inicial é irmos até o browser e coloquemos "localhost:8080", mas fazendo isso teremos uma mensagem de erro...Isto porque o nosso
        container está executando em nossa vm2 e não na nossa máquina local propriamente dito...Esta vm2 tem seu próprio IP e suas próprias configurações básicas. A nossa máquina física nesse
        curso, está sendo usada só para emular as máquinas que estamos criando para colocar no nosso Swarm.

        Para acessar a aplicação que acabamos de subir com o container, precisamos descobrir o endereço IP da vm2. Ela por ser um nó do nosso Swarm, podemos conseguir informações detalhadas dela
        pelo Leader. Assim como o Docker, usando ele Stand Alone que fizemos no curso anteior, conseguíamos entender e inspecionar diversas informações sobre um container, sobre uma imagem, sobre
        um volume...Faz todo o sentido que agora consigamos também ver informações bem mais detalhadas sobre um determinado nó.

        Para inspecionar um nó, usaremos o seguinte comando dentro do nosso manager que é a nossa vm1:

            $ docker node inspect vm2

            Teremos a seguinte saída:

            [
                {
                    "ID": "j9wyh6w89cpk2eh1hx749n7kv",
                    "Version": {
                        "Index": 35
                    },
                    "CreatedAt": "2021-03-25T00:56:22.33036158Z",
                    "UpdatedAt": "2021-03-31T18:11:50.593637781Z",
                    "Spec": {
                        "Labels": {},
                        "Role": "worker",
                        "Availability": "active"
                    },
                    "Description": {
                        "Hostname": "vm2",
                        "Platform": {
                            "Architecture": "x86_64",
                            "OS": "linux"
                        },
                        "Resources": {
                            "NanoCPUs": 1000000000,
                            "MemoryBytes": 1033252864
                        },
                        "Engine": {
                            "EngineVersion": "19.03.12",
                            "Labels": {
                                "provider": "virtualbox"
                            },
                            "Plugins": [
                                {
                                    "Type": "Log",
                                    "Name": "awslogs"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "fluentd"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "gcplogs"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "gelf"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "journald"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "json-file"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "local"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "logentries"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "splunk"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "syslog"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "bridge"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "host"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "ipvlan"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "macvlan"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "null"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "overlay"
                                },
                                {
                                    "Type": "Volume",
                                    "Name": "local"
                                }
                            ]
                        },
                        "TLSInfo": {
                            "TrustRoot": "-----BEGIN CERTIFICATE-----\nMIIBajCCARCgAwIBAgIUKEOhSAjR7FPZKcH+iPMMA2T27i8wCgYIKoZIzj0EAwIw\nEzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMjEwMzIwMTkyODAwWhcNNDEwMzE1MTky\nODAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH\nA0IABHOI4VRH807sqAc2IA+v7Y5KmpSEn41cCC9eKxp5LqV682AWrLutBZBaoMFY\nvVdiOyPONXH9B8AzdJD6VE3ifdyjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB\nAf8EBTADAQH/MB0GA1UdDgQWBBQaMA1e0wbub/ig0gbb7etkuyyWPTAKBggqhkjO\nPQQDAgNIADBFAiA316oy+8K/wQWS6d4r84liL9t8ulOKxw776XLMIEkMCQIhAI00\ndVqnQmslCaafl4GeQJ7t/huE2m6BKP5T+8Rs6Dg6\n-----END CERTIFICATE-----\n",
                            "CertIssuerSubject": "MBMxETAPBgNVBAMTCHN3YXJtLWNh",
                            "CertIssuerPublicKey": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEc4jhVEfzTuyoBzYgD6/tjkqalISfjVwIL14rGnkupXrzYBasu60FkFqgwVi9V2I7I841cf0HwDN0kPpUTeJ93A=="
                        }
                    },
                    "Status": {
                        "State": "ready",
                        "Addr": "192.168.99.107"
                    }
                }
            ]

            Muitas informações sobre o nosso nó foram exibidas, a informação que precisamos está no último bloco "Status", ele mostra o endereço IP, que é 192.168.99.107, se formos agora 
            novamente no browser e colocarmos o endereço 192.168.99.107:8080, veremos o site da "Barbearia Alura" ser exibido.

        Mas...Vamos pensar na estrutura do nosso Swarm...Será que a nossa vm1 e vm3 enxergam este container com a aplicação que está executando na vm2? Será que sabem de algum modo da existência
        dele? Porque a função do Swarm além de toda divisão do trabalho é manter nossa aplicação sempre disponível.

        A ideia é que todos os nós saibam o que está sendo feito para que eles consigam se comunicar de maneira correta, para não ter nenhum tipo de telefone sem fio dentro do Swarm.

        E para confirmarmos de uma maneira bem simples que os nossos nós sabem da existência desse container, é irmos em cada um deles, por exemplo, e executar um "docker container ls"...
        Verificamos que não foi exibido nada em nenhum dos outros nós (vm1, vm3), enquanto na vm2 está sendo exibidos os dados deste container...

        Por quê isto está acontecendo? Porque com o comando "docker container run" na vm2 fizemos com que o container e sua aplicação rodassem em escopo local, e não no escopo do Swarm como
        queremos. Como então podemos criar um container e rodar ele no escopo do Swarm? O primeiro passo será remover o container que criamos...

            $ docker container rm e8 --force

            removido...

        Agora queremos subir esse container de uma forma que todos saibam da existência dele, e como podemos fazer isso? Como queremos subir um container no Swarm de forma que todos vejam
        não trataremos mais como um simples container, mas sim como um serviço.

        Para criarmos um serviço dentro do nosso Swarm, temos um comando específico para isso, que é o "docker service create", após isso a sintaxe é bem parecida com a da criação de um container
        comum.

        Inserimos o nosso "-p 8080:3000", não precisamos mais do "-d", porque este novo comando por padrão não trava o terminal, e, por fim, a imagem que queremos utilizar, "aluracursos/barbearia"...
        Agora como resultado temos aquele mesmo erro que vimos aquela hora que tentamos alterar o Swarm através de um simples worker:
        "Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. 
            Please run this command on a manager node or promote the current node to a manager.", o motivo é o mesmo, como agora estamos tentando criar um serviço
        no escopo do Swarm e não um simples container local, não temos autonomia para realizar esta alteração à partir de um simples worker.

        Sendo assim para que possamos realizar com sucesso esta tarefa precisamos executar este comando dentro do nosso manager, a nossa vm1...

            docker@vm1:~$ docker service create -p 8080:3000 aluracursos/barbearia

        Temos a seguinte saída:

            lrtj4rn0a8nkpl6pysqg3wwfx
            overall progress: 1 out of 1 tasks
            1/1: running   [==================================================>]
            verify: Service converged
        
        Agora ele carregou tudo certinho e informou que o serviço convergiu.


    Tarefas e o Routing Mesh:

        Serviço convergido nada mais é do que serviço criado. Agora temos um container sendo executado no escopo do Swarm. Agora como listamos os serviços/containers? Podemos realizar o famoso
        comando do Docker:

            $ docker container ls

            Nossa saída...

            CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS               NAMES
            6c866926aa53        aluracursos/barbearia:latest   "/bin/sh -c 'node se…"   11 minutes ago      Up 11 minutes                           strange_hypatia.1.owo4fl9prahzfrejlr7rtvvph

        Vamos usar também o comando:

            $ docker service ls

            Nossa saída

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            lrtj4rn0a8nk        strange_hypatia     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp
            
        Temos o ID do container, o nome, etc...Vamos entender agora o que quer dizer replicado....
        Podemos ver que ele está fazendo binding na porta 8080 com a 3000, mas nós não sabemos uma coisa ainda, em qual máquina ele está sendo executado? Será que á a vm1, vm2 ou vm3? Porque
        temos um asterisco na frente da porta, não sabemos de qual vm é...

        Se voltarmos nas informações do console, veremos que ele também criou uma tarefa...

            "overall progress: 1 out of 1 tasks"

        ...e o que é essa tal de tarefa? O que isso quer dizer? Uma tarefa nada mais é do que a instância de um serviço que foi criado, então vamos parar para pensar, temos um serviço que criamos,
        uma ideia a ser realizada e criamos logo depois de uma tarefa que é a instância desse serviço na prática, se parar para pensar, é como se o serviço fosse meioo que a receita e a task que
        é só a execução desa receita propriamente dita.

        Se quisermos ver aonde esta tarefa essa tarefa está sendo executada, podemos vir aqui e dar um "docker service ps" e por fim, botar o ID do serviço que queremos ver o detalhe, quais tasks
        esse serviço tem.

        Vamos usar aqui "lr", que é o inicio da ID, dando um 'Enter', ele mostra diversas informações, como por exemplo o ID dessa tarega e o nome. Repare que é o mesmo nome do nosso serviço
        seguido de '.1' a imagem que ele está executando. Ele está utilizando a vm1, o estado desejado é que ele esteja rodando e ele realmente está e não vemos nenhum erro...

            docker@vm1:~$ docker service ps lr
            ID                  NAME                    IMAGE                          NODE                DESIRED STATE       CURRENT STATE             ERROR                              PORTS
            owo4fl9prahz        strange_hypatia.1       aluracursos/barbearia:latest   vm1                 Running             Running 26 minutes ago

        Então perfeito, sabemos que agora o nosso serviço, a nossa tarega está sendo executada na nossa vm1, ou seja, tem um container sendo executado na nossa vm1, só que agora ele está no
        escopo do Swarm, e o que isso muda? Porque agora é legal ter isso no escopo do Swarm? Se voltarmos na nossa aplicação e dermos F5 (192.168.99.106:8080), ainda estará funcionando, mas 
        agora o que conseguimos fazer?

        Para verificar as principais diferenças, vamos executar um "docker node inspect ...", para ver o IP da nossa vm2 e vm3...Verificamos que o IP da vm2 é '192.168.99.107'
        e da vm3 é '192.168.99.108'. Se colocarmos estes IPs no browser na porta 8080 será que funciona? Se fizermos os testes veremos que sim.

        Então, por mais que o serviço esteja sendo executado, a princípio o container esteja sendo mantido pela vm1, qualquer outra máquina consegue acessar esse container agora.

        E como isso acontece? Isso acontece porque agora quando fazemos essa requisição para a porta 8080 para o nosso Swarm, não mais para um nó específico, através de um cara chamado
        Routing Mesh, ele consegue fazer esse redirecionamento, e ele vai procurar se tem algum aplicação sendo executada em alguma porta 8080 de algum nó dentro do nosso Swarm, ele vai ver,
        nesse caso, que temos um serviço na vm1, então qualquer outra máquina, qualquer outro nó que recebe uma requisição para a porta 8080, esta requisição vai ser redirecionada para o nó que
        tem o serviço de fato. Isto significa que a porta 8080 está sendo utilizada no escopo todo do Swarm.

        E se tentarmos subir outra aplicação na porta 8080? Teremos um erro...

            Error response from daemon: rpc error: code = InvalidArgument desc = port '8080' is already in use by service 'strange_hypatia' (lrtj4rn0a8nkpl6pysqg3wwfx) as an ingress port

        O erro mostra que já temos o serviço 'strange_hypatia', utilizando a porta desejada, e que não pode ser utilizada como uma porta de ingresso.

        Só para vermos um pouco mais do que o Docker Swarm é capaz de fazer, se viermos na nossa vm1 novamente, e executarmos o comando "docker container ls"...

            CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS               NAMES
            6c866926aa53        aluracursos/barbearia:latest   "/bin/sh -c 'node se…"   About an hour ago   Up About an hour                        strange_hypatia.1.owo4fl9prahzfrejlr7rtvvph

        ...veremos que no fim das contas temos realmente um container sendo executado aqui, e esse container foi criado pela tarefa que veio lá do serviço que criamos.

        Se executarmos o comando "docker container rm" e colocarmos o ID do container que está executando o nosso serviço, início 6c, acompanhado de '--force' ao final, removemos esse container,
        podemos verificar com "docker container ls", que o container foi removido. 
        
            Obs: Como estamos na máquina 'leader' notamos que já existe outro container mas com o ID diferente, ou seja, 
            o antigo container foi removido, isto aconteceu porque o Swarm criou o serviço no mesmo nó. Este teste funciona melhor em máquinas que são simplesmente workers, pois de fato o 
            container some. Veremos mais pra frente no curso como deixar o nó leader apenas como gerenciador dos outros nós, e não sendo mais um nó no qual o Swarm pode alocar serviços.

        Como executamos o "docker container ls" na máquina leader, e vimos que foi criado um novo container, podemos agora executar o comando "docker service ps" acrescentando o ínicio do ID do
        serviço 'lr' veremos que agora o serviço está rodando no nó vm2, o serviço foi realocado automaticamente pelo Swarm em outro nó.

        Agora o "docker container ls" na vm1 não mostra mais nada...Podemos fazer mais vezes este mesmo teste, indo na nossa vm2 e removendo o container para ver o processo se repetindo.
        Vimos que da forma que está configurado o serviço acabou caindo dentro da nossa manager...E estudaremos sobre isso mais a frente. Avançaremos nos estudos sobre a manager.


        Nesta aula, aprendemos:

        - Que nós workers são responsáveis por executar containers
        - Comandos de leitura e visualização de nós, como o docker node ls
        - Comandos que leem ou alteram o estado do swarm só podem ser executados em nós managers
        - O comando docker container run sobe containers em escopo local e o docker service create cria serviços em escopo do swarm
        - Tarefas são instâncias de serviços
        - Portas são compartilhadas entre nós do swarm e são acessíveis a partir de qualquer nó graças ao routing mesh
        
        
    Questões aula 02:

        01 - Agora queremos adicionar nós ao swarm. Qual das alternativas abaixo é realmente uma responsabilidade dos nós workers dentro do swarm?

            Selecione uma alternativa

            R: São responsáveis pela execução dos containers dentro do swarm.

            Alternativa correta! Como o nome diz, eles são os trabalhadores, responsáveis por rodar containers.

        
        02 - Para executarmos comandos de leitura, como por exemplo docker node ls, e/ou alteração no estado do swarm, temos uma condição. Qual das alternativas abaixo contém essa condição?

            Selecione uma alternativa

            R: Podem ser executados apenas em nós managers.

            Alternativa correta! Tais comandos ficam restritos apenas aos nós managers dentro do swarm.

        
        03 - Vimos na última aula uma diferença bem importante entre os comandos docker service create e docker container run. Qual é essa diferença?

            Selecione uma alternativa
            
            R: O comando docker service create no final cria um container em escopo do swarm e o docker container run em escopo local.

            Alternativa correta! Como vimos, essa é uma grande diferença entre os dois comandos.

        
        04 - Qual artifício do Docker Swarm permite que nós possamos acessar quaisquer serviços a partir do IP de qualquer nó dentro do swarm, apenas informando a porta?

            Selecione uma alternativa

            R: Routing Mesh.

            Alternativa correta! Graças ao Routing Mesh conseguimos acessar diferentes serviços a partir de qualquer IP pertencente ao swarm.


Aula 03: Gerenciando o cluster com managers -------

    O papel do manager:

        Nessa aula agora vamos ter um estudo um pouco mais teórico, mas ainda também prático sobre o papel efetivo dos managers dentro do nosso Swarm.

        Na aula passada estudamos sobre worker, como subimos agora um serviço para que consigamos subir um container no escopo do Swarm e essas coisas. Mas agora queremos saber qual é o papel
        do manager, será que ele só orquestra e administra o container? O que pode acontecer de diferente, o que pode ser causado de problemas para nós? Vamos imaginar um caso mais real...

        Vamos supor que sejam três máquinas reais, máquinas físicas, temos por exemplo máquina 1, máquina 2 e máquina 3, assim como estão dispostos nossos nós, elas estão em algum lugar do mundo.

        Aí de repente a nossa máquina 1 pega fogo, queima, sei-lá, é destruída, o que vai acontecer? A minha máquina 1 vai parar de funcionar, consequentemente eu perdi o meu manager, eu não tenho
        mais um manager no meu Swarm, o que isso pode acarretar de problema para mim? Muitas coisas.

        Ainda temos o nosso serviço e nossos nós funcionando da aula anterior, e se tentarmos simular esta situação onde o nó manager é perdido? Temos o comando "docker swarm leave", se tentarmos
        executar este comando, o próprio Docker nos enviará a mensagem:

        "Error response from daemon: You are attempting to leave the swarm on a node that is participating as a manager. Removing the last manager erases all current state of the swarm. 
                        Use `--force` to ignore this message."

        Esta mensagem diz: Você está tentando deixar o Swarm a partir de um nó manager. Removendo o último manager (Pois no caso criamos apenas um) apagará todo o estado atual do Swarm.
        
        Ou seja, todas as nossas configurações serão perdidas...Mesmo assim ele ainda sugere o comando "--force" para o caso de ser isso mesmo que queremos fazer.

            docker@vm1:~$ docker swarm leave --force
            Node left the swarm.

        Realizar este passo acarretará na construção do Zero do nosso Swarm, mas é importante para ver o que pode acontecer em algum momento se não tomarmos conta e não observarmos o estado
        do nosso Swarm desde o início.

        Agora que não temos um nó manager, as nossas workers estão perdidas, por quê? Porque como esse cara não faz parte do nosso Swarm mais, se eu fizer qualquer comando aqui respectivo ao 
        antigo Swarm, por exemplo um "dsocker node ls", ele irá dizer que este nó não é nenhum Swarm manager e nem faz parte de um Swarm, faz todo sentido...

            docker@vm1:~$ docker node ls
            Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.

        Eu não consigo mais executar comandos do escopo do Swarm nesta máquina agora, não faz parte mais de um Swarm. Significa que se eu tentar subir algum serviço novo, por exemplo, 
        "docker service create -p 8080:3000 aluracursos/barbearia"....

            Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.

        Não vamos conseguir porque esse cara ainda é um worker, assim como a nossa vm3. Significa que nós não vamos mais conseguir ler e nem alterar nenhuma informação do nosso Swarm, porque
        o nosso único manager, o único cara que era responsável por isso não existe mais.

        Porém uma coisa interessante é que o serviço nos outros dois nós (vm2 e vm3) ainda está executando, podemos verificar no browser. Não funciona mais apenas na vm1.
        O que isso quer dizer? Em certo ponto é uma coisa boa, pois, por mais que agora não tenhamos mais a administração do nosso Swarm, todos os serviços que lá estavam continuam funcionando
        sem nenhum problema, por mais que não tenha mais um manager. O lado negativo é que não conseguimos mais alterar o estado do nosso Swarm e não conseguimos mais ver nenhuma informação.

        Veremos agora como contornar este problema...

    
    Como fazer backup do Swarm:

        Como poderíamos ter evitado que toda a configuração do Swarm desaparece-se? Como conseguiremos fazer o backup do nosso Swarm de maneira correta?
        Executamos o comando para fazer as outras máquinas deixarem o Swarm também...

            $ docker-machine ssh vm3 /depois/ $ docker swarm leave
            $ docker-machine ssh vm2 /depois/ $ docker swarm leave

        Agora tudo o que tinhamos no Swarm foi apagado...

        Vamos recriar o Swarm do 0 agora...Na vm1 executaremos o comando:

            $ docker swarm init --advertise-addr 192.168.99.106 

            Agora entrar em cada um dos workers e adicionar ao Swarm com o código - 
                (docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377)

        Agora vamos recriar o serviço à partir da máquina 1...

            $ docker service creater -p 8080:3000 aluracursos/barbearia

            ipqucvqhh4bev9dlccenkog0z
            overall progress: 1 out of 1 tasks
            1/1: running   [==================================================>]
            verify: Service converged

            Serviço convergiu, verificamos com:

            $ docker service ls

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            ipqucvqhh4be        blissful_lamarr     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

        Como é que podemos fazer um backup do nosso Swarm em determinados momentos para evitar que alguma tragédia aconteça e tenhamos um ponto de retorno para não ter que fazer tudo do Zero
        novamente? Poderíamos ter feito o seguinte, copiado todos os nossos logs, todo o nosso conteúdo do Swarm para uma outra pasta de backup.

        E aonde fica todos esse conteúdo que queremos fazer backup? Poderíamos ver na documentação, mas temos já em mãos, ele fica na pasta "/var/lib/docker/swarm".
        Se tentarmos com o usuário comum teremos o seguinte erro:

            docker@vm1:~$ cd /var/lib/docker/swarm
            -bash: cd: /var/lib/docker/swarm: Permission denied

        Teremos que elever a permissão com "sudo su" ou "sudo -s", agora sim conseguiremos entrar na pasta...

            docker@vm1:~$ sudo -s
            root@vm1:/home/docker# cd /var/lib/docker/swarm
            root@vm1:/var/lib/docker/swarm#

        E agora se usarmos um "ls" ele nos mostra todos os conteúdos, tudo que está sendo gerenciado pelo nosso Swarm...
        
            root@vm1:/var/lib/docker/swarm# ls
            certificates       raft               worker
            docker-state.json  state.json
        
        ...significa que queremos fazer o que? Queremos copiar toda esta pasta para
        onde? Para algum outro lugar de onde possamos recuperá-la depois...

        Por exemplom, vamos copiar toda essa pasta...Voltaremos para "home", faremos um "cp", que é o comando de cópia no terminal. Utilizaremos...

            root@vm1:/home# cp -r /var/lib/docker/swarm/ backup

        Acima vemos uma cópia do comando utilizado para copiar o conteúdo da pasta 'swarm' que se encontra em '/var/lib/docker', para a pasta backup (crida no comando) que está na home.

        Como tudo isso acima está em uma pasta, utilizamos o parâmetro '-r' de cópia recursiva, que copia a pasta e tudo que ela tem dentro...
        Se utilizarmos o 'ls' agora na pasta backup que está em 'home', veremos o conteúdo dentro dela...

            root@vm1:/home/backup# ls
            certificates       raft               worker
            docker-state.json  state.json

        Agora, vamos imaginar o que vai acontecer no nosso caso lá no Swarm, tudo queimar novamente, vamos dizer que o Swarm todo parou de funcionar, como é que conseguiríamos recuperar ele
        agora? Vamos repetir o nosso erro, vamos executar o comando "docker node rm", vamos remover os dois nós adicionados ao Swarm, "vm2 --force", 

            root@vm1:/home/backup# docker node rm vm2 --force

            root@vm1:/home/backup# docker node rm vm3 --force

            Agora se dermos um...

            $ docker node ls

            Só temos a vm1, e agora utilizaremos o:

            $ docker swarm leave --force

        Se formos agora em "/var/lib/docker/swarm", o que irá acontecer?

            $ cd /var/lib/docker/swarm

        Estará vazio...então todo aquele controle que tínhamos foi perdido, por isso que no momento em que criamos um novo Swarm ainda a pouco, não tinha nada como tinha antes, tudo tinha
        sido apagado...

        E agora o que queremos fazer? Queremos criar um novo swarm usando as configurações que salvamos no nosso backup, então vamos voltar aqui para a nossa home e vamos fazer o seguinte...
        Vamos utilizar o comando "cp" de novo junto com o "-r", e vamos copiar todo o conteúdo que está dentro de "backup/*", para onde? Para "/var/lib/docker/swarm"

            root@vm1:/home# cp -r /backup/* /var/lib/docker/swarm/

        E agora o que precisamos fazer? Já copiamos todo o conteúdo para lá de novo, agora precisamos recirar o nosso swarm, e ele vai utilizar essas configurações que estão copiadas dentro
        dessa pasta, mas se eu for ali no terminal e simplesmente fazer um "docker swarm init --advertise-addr 192.168.99.106", qual é o IP que ele irá sugerir?

        Ele não necessariamente vai utilizar essas configurações, eu preciso forçar ele a recarregar todas as configurações que estão dentro desse caminho aqui "/var/lib/docker/swarm"...
        E como é que forçamos ele a usar essas configurações? Vamos colocar uma outra flag antes, que se chama "--force-new-cluster" e agora dando um 'Enter', o que ele faz? Ele recria o Swarm...

            root@vm1:/home# docker swarm init --force-new-cluster --advertise-addr 192.168.99.106
            Swarm initialized: current node (j8owlnxj5wmrz3v8wq6dtwahp) is now a manager.
            
            To add a worker to this swarm, run the following command:
            
                docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377
            
            To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

        Se voltarmos nas nossas páginas e dermos um 'F5', tudo estará funcionando normalmente...Se utilizarmos também o "docker node ls", dentro da nossa manager, o que ele mostrará? Que nós já
        temos os nossos nós, a nossa vm2 e a nossa vm3 já estão dentro do Swarm de novo.

        Se dermos um "docker service ls"...

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            ipqucvqhh4be        blissful_lamarr     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

            ...o serviço está rodando, se dermos um "docker service ps ip (Que é o início do id do serviço)"

            ID                  NAME                    IMAGE                          NODE                DESIRED STATE       CURRENT STATE           ERROR                              PORTS
            muj9lor14r1n        blissful_lamarr.1       aluracursos/barbearia:latest   vm1                 Running             Running 5 minutes ago
            5fulhdsz10ht         \_ blissful_lamarr.1   aluracursos/barbearia:latest   vm1                 Shutdown            Failed 5 minutes ago    "No such container: blissful_l…"

            ...ele vai mostrar uma coisa bem interessante...Que tem uma tarefa, ele tentou subir uma vez e falhou, na segunda ele conseguiu rodar esse serviço, que é a nossa barbearia da Alura,
            inclusive na vm1, lembra o que foi dito? A nossa vm manager não é o ideal para carregar e executar container, nós ainda vamos estudar isso, como isso pode ser evitado.

            Então é isso...Acabou de criar o Swarm com os workers desejados, faça um backup, para caso de algum problema que faça o Swarm parar de funcionar termos o backup dele...Antes que
            percamos todos os nossos managers ou alguma coisa mais absurda, tenhamos sempre o conteúdo mais atual dele e só precisa recriar ele com o:

                $ docker swarm init --force-new-cluster

                ...copiando nosso backup para dentro da pasta "/var/lib/docker/swarm"

    Criando mais managers:

        De quais outras formas poderíamos ter evitado aquele desastre que aconteceu? É claro que pudemos evitar isso fazendo o backup, mas como poderíamos ter evitado de ter que usar o nosso
        backup de maneira tão rápida? Será que podemos deixar o nosso Swarm mais robusto? Que ele seja mais inteligente, que ele consiga lidar melhor com essas falhas? Porque veja bem, tínhamos
        um manager até então, será que poderíamos ter mais um, dois, três, cinco, dez? Podemos!

        Como faremos isso? Como podemos deixar o nosso Swarm mais inteligente? Menos suscetível a erros na verdade? Removeremos novamente os nós do Swarm com o comando:

            $ docker swarm leave --force

            Entraremos nas vm's 2 e 3 com o comando "$ docker-machine ssh vmx" e removeremos as máquina virtuais do Swarm com o comando acima.

        E agora o que vamos fazer? Mais uma vez iremos recriar o Swarm na vm1 (Não se esqueça que ela também deve ser removida do Swarm ou então um erro será exibido):

            $ docker swarm init --advertise-addr ...

            e iremos colocar 192.168.99.106, clicaremos no 'Enter'. Ele recriou aqui mais uma vez o swarm, reparem que não usamos o force-new-cluster, não queremos usar nenhum backup, estamos
            criando ele do zero.

        E agora ele me dá o comando de adicionar um worker, mas agora queremos adicionar um manager, e reparem, ele já até dá a resposta aqui no final...Tínhamos aquele comando:

            $ docker swarm join-token worker --- para poder adicionar um worker ao nosso Swarm.

        Se quisermos adicionar um manager, é só botar "docker swarm join-token manager", e ele vai dar o output aqui para nós mostrando o quê? O comando para adicionar um manager ao Swarm.
        Reparem que o comando é o mesmo, toda a primeira parte pelo menos, até o último hífen. O que ele troca no final é só o token que vai definir se o nó vai entrar como um worker ou como um
        manager no Swarm.

            $ docker swarm join-token worker  -> docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377
            $ docker swarm join-token manager -> docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.106:2377
            
        Agora pegamos esse comando e colamos nas nossas vm2 e vm3. A saída deverá ser: "This node joined a swarm as a manager."
        Agora temos o que? Agora temos um Swarm com três managers, mas precisamos também adicionar os nossos worker para fazer o nosso trabalho duro.

        Por mais que tenhamos visto que o nosso Swarm consegue atribuir container, serviços aos nossos managers, precisamos ainda ter os nossos worker porque vamos entender mais para frente
        qual é a definição, do que devemos ou não colocar no manager.

        Mas vamos lá, precisamos agora criar mais workers. Para criá-los utilizaremos o comando:

            $ docker-machine create -d virtualbox vm4
            $ docker-machine create -d virtualbox vm5

        Se formos na nossa vm1 e executarmos o comando "docker node ls", ela vai exibir algumas informações para nós.

            Obs: No curso o instrutor quis ensinar como melhorar a visualização logo em seguida, já que estava utilizando o terminal do Windows.
                Como estou utilizando o VSCode desde o ínicio, não tive este problema.

        Veremos como exibir apenas alguns atributos, como exemplo pegaremos apenas o 'HOSTNAME' e o 'MANAGER STATUS'...Utilizaremos o comando:

            $ docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"

            Teremos a seguinte saida:

            vm1 Leader
            vm2 Reachable
            vm3 Reachable

        Podemos ver que a nossa vm1 agora ainda é a líder, mas reparem que a nossa vm2 e a nossa vm3 são 'Reachable', ou seja, elas não tem a ausência de informação que tinham anteriormente, 
        quando eram vazias, eram workers, agora elas são manager.

        Vamos adicionar agora as nossas vm4 e vm5 aqui no nosso Swarm. Vamos acessá-las com "docker-machine ssh vm4" e "docker-machine ssh vm5" e vamos adicioná-las como worker e só para
        mostrar que podemos, vamos ir na vm3 e vamos executar o comando:

            $ docker swarm join-token worker

        E agora como ela é uma manager, ela vai conseguir dar esse output para nós que é bem legal:
        
        docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.108:2377

            Copiamos o comando, agora iremos até a vm4 de fato, e vamos executá-lo...Faremos o mesmo na vm5. Se executarmos na nossa vm1 novamente o comando para formatar tudo certinho, ele
            vai exibir vm1, vm2, vm3, vm4 e vm5, mas só que mais uma vez, só com aquela formatação de exibir apenas o hostname e o manager status:

                $ docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"

                Saída:

                vm1 Leader
                vm2 Reachable
                vm3 Reachable
                vm4
                vm5

        Então se não tem nenhuma informação da vm4 e da vm5, significa que elas são workers, se ele está exibindo esse reachable na vm2 e na vm3 significa que elas são managers, e como ele
        está exibindo esse leader na vm1, significa que além de manager, ela é uma líder, ou seja, ela é a liderança máxima que tem dentro desse Swarm.

        Mas o que podemos pensar agora? Agora que temos um 'Leader' e mais dois membros managers, 'Reachable'...O que poderia acontecer? Vamos parar e pensar no seguinte, temos a nossa vm1,
        e se ela parar de funcionar, o que vai acontecer? Será que vamos ter que decidir entre a vm2 e a vm3 quem vai ser o nosso novo líder? Exatamente!

        E como isso vai acontecer? Vamos fazer um teste, vamos na vm1 e mais uma vez, vamos simular uma catastrofe, vamos executar um "docker swarm leave --force", removendo ela do Swarm de 
        forma abrupta. Se formos agora na nossa vm2 ou na nossa vm3 que também são managers ainda, reparem que nós ainda conseguimos dar um "docker node ls", ou seja, nós ainda temos status, 
        conseguimos visualizar todas as informações do nosso cluster.

        Se utilizarmos novamente o "docker node ls --format", só para ficar mais apresentável, colocarmos "{{.Hostname}}" e depois "{{.ManagerStatus}}", ele vai exibir que a vm1 está com 
        estado de 'Unreachable', ou seja, está inacessível, a vm3 ainda é manager porque ela é 'Reachable'.

        E agora a vm2 é a nossa nova líder, ou seja, nós sempre temos um novo líder, a pergunta que fica é: "Como esse cara foi eleito como líder?", veremos nos próximos capítulos sobre o
        consenso, como o Docker faz essa votação.

    
    Algoritmo de consenso RAFT:

        O que acabou de acontecer, no que diz respeito da escolha do novo Leader? Foi feita uma eleição e foi decidido no caso aqui da minha máquina, do nosso Swarm que a vm2 é a nova líder,
        pode ser que na sua máquina tenha sido eleita a vm3, se você estiver fazendo isso que nem estou fazendo, mas a ideia aqui é ver que aconteceu uma eleição que vai variar o resultado
        conforme o nosso sistema, conforme a nossa máquina.

        Mas a pergunta que fica é: Como foi feita essa eleição? Quem fez essa eleição? Será que ela sempre funciona? Para começarmos a responder isso, precisamos entender que foi aplicado na
        verdade um algoritmo de consenso e esse algoritmo de consenso é um cara chamado RAFT, o simbolo usado no exemplo é como uma canoa, uma jangada, mas o que importa mesmo não é o símbolo
        e sim o que ele faz e como ele faz.

        Sempre temos um só líder, e o que acontece quando este líder falha? O líder falhou e a partir de agora, entre todos os outros managers que tem o nosso Swarm, vai ser feita uma votação,
        uma eleição, e cada um vai votar em quem eles querem que seja o novo líder.

        No meu caso, foi eleita a vm2, mas no momento dessa votação, que eles estão "depositando" o voto, poderia ter sido eleita tanto a vm2 quanto a vm3, o que improta é, temos algumas
        regrinhas, temos algumas restrições para que o consenso possa ser atingido.

        E quais são as regras para que o RAFT consiga funcionar? Na verdade é bem simples, precisamos do quê? Precisamos ter o número máximo de falhas que vamos conseguir aguentar e
        consequentemennte, vamos ter um quórum mínimo para que a eleição seja feita. Porque vamos imaginar, se abrissemos a votação e tivessemos só uma pessoa, não faria sentido ter uma votação.

        Então vamos lá, o que acontece? Tínhamos três managers, tanto faz se um era líder ou não, o que importa é que no fim das contas todos os três são managers, mas o que acontece é que, 
        vamos suportar no caso, N - 1 falhas, no caso aqui nosso N é o número de managers, então vamos suportar ((3 - 1) / 2) (Três menos um igual a dois, dividido por dois), então ele só 
        suporta uma falha nesse caso aqui que temos três managers, e conseguentemente, o nosso quórum mínimo vai ser N dividido por 2 que é 1, resto 1.
        
        E aqui no caso vamos ter N sobre 2 mais 1, ou seja, nosso quórum tem que ser no mínimo 2, então estamos já no nosso limite nesse caso aqui, se tivéssemos cinco managers o que aconteceria?
        Suportaríamos 5 - 1 = 4, dividido por 2 = 2 falhas, e o nosso quórum mínimo teria que ser N sobre 2 daria 2 mais 1 = 3, que é o nosso quórum ali para ter nossa eleição.

        Número de falhas = (N - 1) / 2 (Onde N é o número de máquinas)
        Quórum = N / 2 (+ 1) (Onde N é o número de máquinas...E (/) para dividir e ignorar o resto)

        Então repara, quanto mais managers temos, maior a nossa capacidade de suportar falhas e o nosso quórum vai estar lá sempre acompanhando esse número de falhas, por exemplo, por que não
        adicionamos logo então 300 milhões de managers no nosso Swarm, o problema estará resolvido? Porque na verdade, quanto mais managers adicionamos no nosso Swarm, maior é o nosso tempo
        de processamento de leitura e escrita, e isso vai reduzir o nosso desempenho cada vez mais.

        Então quanto mais managers nós temos, vamos ter um desempenho menor nesse longo prazo adicionando mais managers, e o que acontece? O próprio Docker, a própria empresa em si, recomanda que
        nós sempre tenhamos três, cinco ou sete managers, porque tendo esses números ímpares, nós conseguimos sempre ter um número a mais ali para segurar o nosso Swarm.

        E sempre vai ter um consenso, no caso 9 ainda é aceitável, mas nunca mais do que 10, porque aí já começamos a ter um número muito elevado de managers e vamos começar a ter uma taxa de
        leitura e escrita muito grande, nosso desempenho vai começar a cair.

    
    Nesta aula, aprendemos:

        - Que nós managers são primariamente responsáveis pela orquestração do swarm
        - A importância e como realizar o backup do swarm
        - Que podemos ter mais de um nó manager no swarm
        - A importância do Leader dentro do swarm
        - Como é feita a eleição de um novo Leader em caso de falhas
        - Os requisitos para funcionamento do RAFT


    Questões aula 03:

        01 - Caso nosso único manager pare de funcionar, podemos ter problemas. O que acontecerá com o nosso swarm em caso de perda do manager?

            Selecione 2 alternativas

            R1: As tarefas em execução em outros nós serão mantidas sem problemas.

            Alternativa correta! A ausência do manager não afetará as tarefas de outros nós.

            R2: Não conseguiremos mais executar comandos de leitura e/ou criar novos serviços.

            Alternativa correta! Com a ausência do manager, não teremos mais nós capazes de executar comandos administrativos.

        
        02 - É muito importante fazermos backup de todo o nosso swarm para evitarmos desastres. Por padrão, em qual diretório fica armazenado o conteúdo do nosso swarm?

            Selecione uma alternativa

            R: /var/lib/docker/swarm

            Alternativa correta! Nesse diretório temos todas as configurações de estado do nosso swarm.


        03 - Ao utilizarmos o comando docker node ls, como podemos identificar quais nós são managers dentro do nosso swarm?

            Selecione uma alternativa

            R: Basta olhar a coluna Manager Status e ver quais nós tem o valor Reachable ou Leader.

            Alternativa correta! Nós com esses status são managers dentro do nosso swarm.

        
        04 - Vimos que em caso de falhas do Leader do swarm, temos uma eleição entre os nós managers para definir o novo líder. Se tivéssemos um swarm com 7 nós managers, qual seria o nosso 
            quórum necessário e número máximo de falhas para realização da eleição?

            Obs: Caso não lembre das regras, reveja o último vídeo a partir dos 4:00.
        
            Selecione uma alternativa

            R: 4 para o quórum e 3 falhas no máximo.

            Alternativa correta! Como nosso quórum é (N / 2) + 1 e o número máximo de falhas é (N - 1) / 2, temos o valor esperado.


Aula 04: Separando as responsabilidades -------

    Readicionando um manager:

        Então vamos fazer o que agora? Vamos entender como vamos diviri as responsabilidades aqui dentro do nosso Swarm. Porque viemos falando desde o início que papel dos nossos managers
        é fazer orquestração e ler operação de leitura e escrita dentro do Swarm enqaunto os workers carregam e executam os containers, as tarefas, agora por assim dizer.

        Mas vamos parar e pensar no seguinte, em certo momento do nosso curso foi alocada uma tarefa dentro da vm1, ou seja, o próprio Swarm não se opôs de colocar uma tarefa para ser executada
        dentro de um manager, ao invés de colocar ela em um worker, mas a ideia é que evitemos que isso aconteça e nós só vamos querer rodar determinados containers, determinadas tarefas quando
        quisermos que sejam executadas e não que dê essa liberdade de colocar por exemplo alguns serviços quaisquer dentro daquela manager.

        E como é que faremos isso? Vamos voltar no nosso terminal e vamos fazer o seguinte, vamos dar um "docker node ls" mais uma vez e utilizar de novo o "{{.Hostname}}" e também o 
        "{{.ManagerStatus}}" para ver nossa situação.

            $ docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"

            Saída:
                vm1 Unreachable
                vm2 Reachable
                vm3 Leader
                vm4
                vm5

            A nossa vm1 ainda está unreachable, vamos adicionar ela novamente, quais seriam os passos? Vamos primeiro utilizar o "docker node rm vm1"

                    Saída: Error response from daemon: rpc error: code = FailedPrecondition desc = node o6npnn8j8tf10169sxlee4d5c is a cluster manager and is a member of the raft cluster. 
                                It must be demoted to worker before removal

            Que erro é esse? Ele diz que: "Este nó é um manager do cluster e é um membro do raft cluster", ou seja, mesmo que ele esteja "Unreachable", ainda é um membro.

        Ele deve ser "demoted" para o worker antes de ser removido, ou seja, ele tem que ser rebaixado, abaixar o nível dele para que ele seja removido, e como é que fazemos isso? Antes de
        mais nada vamos utilizar o "docker node demote vm1"...

            Saída: Manager vm1 demoted in the swarm.

        ...agora a vm1 por mais que ela ainda esteja "Unreachable", ela é uma worker comum dentro do nosso Swarm, e podemos remover ela agora sem nenhum problema.

        Vamos utilizar o comando para remove-lá:

            $ docker node rm vm1

        Agora para tornar nosso cluster ao estado original, vamos readicionar a vm1 como manager, para a dica utilizamos o comando "docker swarm join-token manager". Acessamos via ssh a vm1
        e colocamos o comando "docker swarm join --token XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 192.168.99.107:2377" e adicionamos
        "--advertise-addr 192.168.99.106" só para mantermos o nosso IP original lá.

            Saída: This node joined a swarm as a manager.

            Utilizamos o comando:

                $ docker node ls

                Teremos a saída:

                    ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
                    nhuzr9tiek2za2q26dcbwuwdl *   vm1                 Ready               Active              Reachable           19.03.12      
                    in3t87hcu77qnk13ln538rg29     vm2                 Ready               Active              Reachable           19.03.12
                    mr4jjwio2une2dn51od448g9q     vm3                 Ready               Active              Leader              19.03.12
                    jh0vk0wrgiruzrejjhfor8l3u     vm4                 Ready               Active                                  19.03.12
                    rzmb7ke49zeocehp5liaq97ez     vm5                 Ready               Active                                  19.03.12

        Tudo voltou a funcionar! A nossa Leader agora é a vm3. O que faremos agora? Vamos voltar para a nossa vm2, vamos questionar o seguinte, se nós subirmos algum Serviço aqui com
        o "docker service create" mais uma vez, "-p 8080:3000 aluracursos/barbearia", o que vai acontecer aqui? Ele vai criar a nossa task, o nosso serviço.

        E vamos ver o que vai acontecer...

            $ docker service create -p 8080:3000 aluracursos/barbearia

            Saída:

                jb8m98lf24u8rg7p9svtvrg9b
                overall progress: 1 out of 1 tasks
                1/1: running   [==================================================>]
                verify: Service converged

        Utilizamos mais uma vez o "docker service ls"

            Saída:

                ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
                jb8m98lf24u8        kind_feynman        replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

                ...temos ele aqui rodando, o ID, o início é "jb", vamos utilizar "docker service ps jb" (Só precisamos colocar os primeiros dois dígitos do ID do serviço).

                Saída:

                    ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
                    p82rpw90dm5v        kind_feynman.1      aluracursos/barbearia:latest   vm1                 Running             Running 4 minutes ago

                Ele subiu aonde? Na vm1, também podemos utilizar o "--format" aqui, e fazermos o que agora? Colocamos o que? O que queremos ver! No caso o "{{.Node}}".

                    $ docker service ps jb --format {{.Node}}

                    Saída:

                        vm1

        Então sabemos que esse servilom essa tarefa, foi atribuída para vm1, mas a vm1 é uma manager, e é aquela questão que acabamos de falar...Será que devemos evitar que quaisquer serviços,
        quaisquer tarefas sejam executadas dentro da manager? E mais, como é que evitamos que isso aconteça? Como é que só rodamos serviços que nós queemos que rode dentro de manager no momento
        em que queremos?

    
    Restringindo nós:
        
        Como é que podemos evitar que serviços sejam rodados na manager? Porque veja bem, o serviço que subimos ainda pouco está sendo executado em manager porque por padrão Docker, o Swarm
        permite que serviços sejam executados dentro da manager.

        Mas a ideia é restringirmos os serviços que queremos que sejam executados em managers ou não, mas vamos ver na próxima aula daqui a pouco em quais momentos devemos ou não executar um
        serviço dentro de uma manager, quais serviços devem fazer isso. Mas antes disso, precisamos aprender como restringimos o comportamento de um serviço ou de um nó para ele ser executado
        dentro de um manager ou não.

        Vamos começar fazendo aquele velho procedimento, vamos para mais uma vez o serviço que está em execução, e como paramos um serviço? Podemos listar o serviço, criar serviço, podemos
        remover serviço. Vamos utilizar "docker service rm", e para removermos basta passar aqui o ID do serviço que ele quer remover.

        Mas uma outra possibilidade é, podemos remover todos os serviços de uma vez, só para vermos como faz isso, utilizando a técnica que vimos lá no curso anterior (de Docker) que é passar
        dentro de parênteses, que é seguido de um cifrão o comando que queremos usar com uma entrada para esse comando aqui.

        Qual o comando que eu quero usar como entrada para o comando de remover? Eu quero que ele use como entrada o ID de todos os serviços que estão em execução, e como eu pego os serviços
        que estão em execução? Com o "docker service ls", para eu pegar só os IDs, executamos um "docker service ls -q", e damos um Enter aqui, e ele remove.

            $ docker service rm $(docker service ls -q)

            Podemos utilizar um "docker service ls" em seguida, para ver se realmente todos os serviços foram removidos.

        Vamos lá então se subirmos mais uma vez o serviço, será teimosia, o que poderá acontecer?

            $ docker service create -p 8080:3000 aluracursos/barbearia

            Ele vai começar a construir, mas uma vez vai tentar fazer os testes de convergência...

                oo3fmgtoiczzl5uvtzt04rfhm
                overall progress: 1 out of 1 tasks 
                1/1: running   [==================================================>]
                verify: Service converged

            ...se tudo der certo ele vai contruir, se dermos então, o "docker service ls"...

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            oo3fmgtoiczz        nervous_dhawan      replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

            ...depois "docker service ps oo"

            ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS
            nihqij3fvo7r        nervous_dhawan.1    aluracursos/barbearia:latest   vm2                 Running             Running about a minute ago

        Vemos que ele o serviço foi alocado com esse ID "oo3fmgtoiczz", quando utilizamos o segundo comando pudemos verificar que a tarefa foi gerada na vm2, e a vm2 também é um manager.

        Temos duas possibilidades de como evitar que algum serviço seja executado dentro de um determinado nó, vamos ver a primeira, como é que podemos chegar e falar "Eu quero que a minha vm2,
        a minha manager, não rode nenhum serviço", eu vou restringir o meu nó para que ele não execute nenhum serviço, e só faça papel administrativo.
        
        E como restrinjo a capacidade desse nós de executar serviço? Se dermos um "docker node ls" aqui, o que vai acontecer? Ele vai listar os nossos nós aqui, e mais, ele está mostrando essa
        coluna aqui de disponibilidade, nesse 'AVAILABILITY', e está mostrando o status de ativo para todos os nós, inclusive a nossa vm2.

        E o que significa esse availability que vimos lá na primeira aula? Significa que esse nós está disponível para rodar serviços e se eu quero tornar ele indisponível para rodar qualquer
        tipo de servilo, eu preciso trocar esse status para indisponível.

        E como é que vamos fazer isso? Temos um comando bem interessante que é o "docker node update", ou seja, vamos atualizar o estado de um nó, e o que queremos atualizar? O availability,
        e podemos atualizar ele com esse "--availability" aqui, e agora colocamos o status que quereos colocar nesse estado eu quero que ele esteja com esse status de "drain", que é o Status
        que por padrão não vai permitir que algum serviçlo seja executado dentro desse nó.

        Por fim, eu preciso informar qual é o nó que eu quero atualizar, porque esse comando aqui, pode ser executado dentro de qualquer manager, e por fim, eu preciso falar q  ue eu quero
        atualizar a minha vm2, ele deu um output aqui na própria vm2, então significa que tudo correu bem.

            $ docker service ls  
            
                ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
                oo3fmgtoiczz        nervous_dhawan      replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

            $ docker service rm oo

            $ docker node update --availability drain vm2
            
            $ docker node ls
                ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
                q5duspz3ugglfw664ln3xr18n     vm1                 Ready               Drain               Leader              19.03.12      
                in3t87hcu77qnk13ln538rg29 *   vm2                 Ready               Drain               Reachable           19.03.12      
                mr4jjwio2une2dn51od448g9q     vm3                 Ready               Active              Reachable           19.03.12
                jh0vk0wrgiruzrejjhfor8l3u     vm4                 Ready               Active                                  19.03.12
                rzmb7ke49zeocehp5liaq97ez     vm5                 Ready               Active                                  19.03.12
    
        No caso, logo acima, utilizamos mais uma vez "docker node ls", está com o status de drain agora. Significa que, se dermos um "docker service ps" para ver como está essa tarega que
        estava neste nó vm2, ele foi realocado agora para outro nó, não mais aqui na nossa vm2.

        Ou seja, so movemos o problema de lugar, estamos meio que em uma caça ao rato agora, não queremos mais que seja rodado na vm2, mas ele tentou e fez na vm3, então para cada novo nó
        que tivermos no nosso Swarm que não queremos que seja executada como um serviço nele, vamos ter que ficar nessa brincadeira de ficar mudando o status dele para drain.

        E mais, com essa mudança, tornamos o nosso nó completamente incapaz de rodar algum serviço e como falamos ainda a pouco, a ideia não é que nenhum serviço nunca vá ser executado dentro
        de um manager, mas com o drain, tornamos esse nó incapaz de rodar qualquer serviço.

        Essa foi a primeira possibilidade, vamos coloca o status destes nós de volta, "--availability" "Active" "vmx". 

            $ docker node update --availability Active vm"X"

        Se utilizarmos de novo o "docker node ls" veremos que todos os nós estão como Active novamente...Porém se utilziarmos de novo o "docker service ls" e "docker service ps 'id serviço'"
        veremos que a tarefa continua no nó vm4, não é porque reestabelecemos a disponibilidade dos nós vm1 e vm2 que o serviço volta para onde estava.

        Se formos no navegador verificar, veremos que o serviço está rodando sem nenhum problema. Vimos a primeira possibilidade que é restringindo nosso nó, criamos um limite ali para dentro
        do nosso nó, para que ele não rode serviços. A outra possibilidade que é um pouco menos invasiva.

    
    Restringindo serviços:

        Vamos então agora à nossa segunda possibilidade, estamos começando logo de onde paramos no ponto anterior. O serviço teve a nova tarefa criada na vm4, mas a pergunta que fica é:
        Como eu posso falar agora para o serviço que eu não quero que ele seja executado em determinados tipos de nós, ou seja, como é que eu restrinjo um serviço e não mais um nó? Será que
        dá para fazer isso? É claro que sim!

        Mas como é que podemos atualizar uma restrição, como é que podemos impor um limite a um serviço? Assim como tínhamos o nosso "docker node uptate" para atualizar o estado de um nó,
        também temos o nosso "docker service update" para atualizar o estado de um serviço.

        E o que precisamos fazer? Precisamos adicionar uma nova restrição a esse serviço, e como é que faremos? Se nós queremos fazer como mexemos na "--availability" de um nó, adicionaremos uma
        restrição, uma  "--constraint-add", e qual é a restrição que queremos? Queremos que este serviço, que criamos ainda a pouco, só rode em nós que tenham o papel de worker no nosso Swarm.
        Então adicionamos qual é a regra desta restrição logo em seguida com "node.role"

        E quais nós tem esse papel? A nossa vm4 e a nossa vm5, não precisamos listar aqui para ele em qual hostname, em qual nó por nome queremos fazer isso, basta explicitarmos o papel desse nó
        dentro do Swarm, então queremos que ele funcione apenas dentro de nós que tenham um papel de "worker", por isso colocamos em forma de comparação "==worker" (Encostado em "node.role")...

            $ docker service update --constraint-add node.role==worker "ID serviço"

        Se dermos um 'Enter' agora, ele irá atualizar o estado do serviço...

            4d
            overall progress: 1 out of 1 tasks
            1/1: running   [==================================================>]
            verify: Service converged

        Se utilizarmos o comando "docker service ps" novamente, passar o ID do serviço, veremos que ele foi criado em outro nós que é apenas worker, a vm4...

            ID                  NAME                      IMAGE                          NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS
            oudob38vx2r1        intelligent_chaum.1       aluracursos/barbearia:latest   vm4                 Running             Running 25 seconds ago
            4c3809jhtmty         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Shutdown 26 seconds ago
            0yukvri4q7zy         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 10 minutes ago     "task: non-zero exit (137)"
            198nayz45gli         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 18 minutes ago     "task: non-zero exit (137)"
            h9aq2arwxrtk         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm4                 Shutdown            Failed 40 minutes ago     "task: non-zero exit (255)"

            Obs: Já tinha encerrado na vm4 anteriormente o serviço para fazer o teste, e atualmente estava rodando na vm2 que é um manager, repare pelo histórico.

        Vamos ver se o serviço está rodando normalmente, vamos pegar o IP e colocar no browser...Caso não lembre o IP, basta sair do nós e executar um "docker-machine ls"...
        Bom, vimos que tudo continua funcionando perfeitamente. Agora podemos fazer o teste derrubando o serviço na vm4 para ver em qual nó o serviço será levantado novamente...
        Vamos até a vm4, executar um "docker container ls", utilizaremos o ID do container para derrubá-lo junto com o comando "docker container rm "ID container" --force".

        Vamos ver em qual nó foi levantado o serviço que derrubamos junto com o container. Primeiro precisamos ir para um nó que seja manager...No caso vm1, vm2 ou vm3...
        Então executar os comandos:

            $ docker service ls

            Pegar pelo menos o início do ID...

            $ docker service ps "id"

            ID                  NAME                      IMAGE                          NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS
            u5p5002sml28        intelligent_chaum.1       aluracursos/barbearia:latest   vm5                 Running             Running 46 seconds ago
            oudob38vx2r1         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm4                 Shutdown            Failed 2 minutes ago      "task: non-zero exit (137)"
            4c3809jhtmty         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Shutdown 12 minutes ago
            0yukvri4q7zy         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 22 minutes ago     "task: non-zero exit (137)"   
            198nayz45gli         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 30 minutes ago     "task: non-zero exit (137)"

            Podemos observar que o serviço foi realocado no nó "vm5", que também é um worker. Se ficarmos derrubando e levantando containers o serviço ficará alternando entre os nós que são
            apenas workers, de acordo com a restrição e regra que criamos.

        Talvez você esteja se perguntando, "E se essa restrição que utilizamos não for aplicável? E se não tiver nenhuma possibilidade para ele rodar esse serviço? Se colocarmos uma condição
        que ele não consiga atender?"...

        Vamos utilizar, por exemplo, uma restrição que não exista, exemplo "worker123"...

            $ docker service update --constraint-add node.role==worker123 "ID serviço"

            ...é um papel que não existe dentro do nosso Swarm, se utilizarmos este comando acima por exemplo, reparem, ele vai começar a tentar agendar essa tarefa, mas ele vai informar que
            não tem nenhum nó disponível que satisfaça essa restrição entre os cinco nós que temos.

                overall progress: 0 out of 1 tasks
                1/1: no suitable node (scheduling constraints not satisfied on 5 nodes)

        Significa que se formos lá no browser acessar o serviço ele vai ter parado de funcionar, porque o nosso serviço como um todo, a nossa tarefa não está alocada em nenhum nó porque
        nenhum nó está disponível, então sempre temos que tomar cuidado em restringir o nosso serviço, mas também de uma maneira em que nós sempre atendamos isso em algum nó.

        Vimos essas duas possibilidades, tanto restringindo um nó quanto o serviço, e cada caso dependendo da nossa situação, vai variar, será que vale a pena eu restringir o meu nó? Será
        que vale a pena eu restringir o meu serviço? O que vai custar menos e o que vai ser mais fácil de eu manter? É isso!

        Vimos agora como conseguimos restringir também os nossos serviços, não mais os nossos nós, essa técnica é um pouco menos invasiva, mas precisamos entender agora mais um pouco quando
        e quais tipos de serviço vamos executar em um manager, será que realmente vale a pena? Que tipo de serviço podemos fazer isso? Vamos descobrir isso no proximo vídeo.

    Para saber mais:

        No último vídeo vimos como podemos adicionar restrições a um serviço utilizando o comando:

        docker service update --constraint-add
        Utilizamos o comando acima para restringir serviços a funcionarem apenas em nós managers ou workers.
        Porém, também podemos impor outros tipos de restrições, como id, hostname e o próprio role. Vamos ver alguns exemplos!
        
        Caso quiséssemos restringir o serviço de id ci10k3u7q6ti para funcionar apenas em um nó com id t76gee19fjs8, poderíamos utilizar o comando:
        docker service update --constraint-add node.id==t76gee19fjs8 ci10k3u7q6ti
        
        Se o objetivo fosse fazer o serviço rodar apenas em nossa vm4 por exemplo, uma possibilidade seria utilizar:
        docker service update --constraint-add node.hostname==vm4 ci10k3u7q6ti
        
        Por fim, podemos também remover restrições criadas utilizando o comando de atualização passando a flag --constraint-rm. Para remover as duas restrições anteriores:
        docker service update --constraint-rm node.id==t76gee19fjs8 ci10k3u7q6ti        
        docker service update --constraint-rm node.hostname==vm4 ci10k3u7q6ti
        
        Após esse momento, quaisquer novas réplicas criadas para esse serviço poderão ser alocadas sem restrição alguma!

    
    Nesta aula, aprendemos:

        - Como readicionar um manager posterior a uma falha
        - Restringir nós de executarem quaisquer serviços utilizando o docker node update --availability drain
        - Restringir serviços de serem executados em determinados nós utilizando o docker service update --constraint-add


    Questões aula 04:

        1 - No último vídeo, removemos um nó manager do swarm a partir de outro manager. Quais os procedimentos que devemos executar para realizar essa remoção?

            Selecione uma alternativa

            R: Devemos primeiramente rebaixar o cargo do nó com o comando docker node demote e depois removê-lo com o comando docker node rm.

            Alternativa correta! É necessário transformar o nó manager em worker e depois remover.

        2 - Quando o nosso objetivo é restringir o comportamento de nós de um swarm, podemos utilizar o comando:

            docker node update --availability drain

            Qual das alternativas abaixo contém o comportamento esperado do nó sobre o qual foi aplicado esse comando?
            
            Selecione uma alternativa

            R: O nó ficará indisponível para executar tarefas.

            Alternativa correta! Com a disponibilidade em drain, não conseguiremos executar mais tarefas/containers nesse nó.

        
        3 - Podemos também aplicar restrições em serviços utilizando o comando:

            docker service update --constraint-add [outras infos]

            Se quisermos restringir o comportamento para um serviço ser rodado apenas em nós workers, o que mais precisamos informar para o comando acima?
            
            Selecione uma alternativa

            R: node.role == worker

            Alternativa correta! Informando o role e fazendo a comparação com ==, não teremos problemas.


Aula 05: Serviços globais e replicados -------

    Serviços replicados:

        Já vimos que nós managers também podem comportar serviço, por isso tivemos que usar regras de restrições para que o serviço ao ser criado não instânciasse tarefas nos nossos managers.

        Mas o ponto agora é o seguinte, quais serviços nós vamos querer rodar em nós do tipo manager? Vamos rodar serviços do tipo monitoramento, serviços de segurança, ou seja, serviços que
        são críticos, que dependemos na nossa aplicação.

        Porque queremos sempre estar monitorando o estado de qualquer nó do nosso Swarm, nós sempre queremos que eles estejam seguros, queremos sempre garantir que a nossa aplicação vai estar
        funcionando da maneira que esperamos, então como é que no Docker, aqui no Swarm, podemos fazer com que determinados serviços rodem em todos os nós da nossa aplicação? Até então estamos
        em cinco.

        Como é que eu posso garantir que, subindo de uma vez o serviço, ele rode em torno desses cinco nós? É uma pergunta a ser respondida, vamos responder isso daqui a pouco, mas antes de
        respondermos isso precisamos entender coo funciona os tipos de serviços do Docker Swarm.

        Nós tinhamos parado o serviço que estava rodando na ultima aula, vamos colocar ele para rodar novamente...

            $ docker service create -p 8080:3000 aluracursos/barbearia

        Vamos entender o que está acontecendo aqui, vamos começar a entender agora quais são os tipos de serviços que o Swarm tem para nós. Se dermos um "docker service ls" o que ele vai
        mostrar aqui?

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            ts9vbn0i3cen        trusting_edison     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

        Temos o ID, o nome e agora sim o modo..."RELICATED"

        O modo aqui está como 'replicated' de replicado e tem uma réplica funcionando no total de uma réplica, o que isso significa? Significa que um dos modos do Docker aqui do Swarm replicado,
        e o outro é o que vamos ver na próxima aula, no próximo vídeo é o modo global, que é como podemos rodar um serviço em todos os nós.

        Mas o que é esse tal de modo replicado? Para começarmos a entender, esse modo replicado nada mais é que o tipo padrão de um serviço do Swarm e esse cara funciona do seguinte modo:
        Até então, o que estava acontecendo? Se dermos um "docker service ps "inicio id servico""...

            ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
            vv8r8q9nwyre        trusting_edison.1   aluracursos/barbearia:latest   vm2                 Running             Running 5 minutes ago

        ...ele mostra que tem uma tarefa rodando na vm2 desse serviço que é o trusting_edison.

        Mas, vamos imaginar um caso real, vamos imaginar que aqui na nossa vm2 esses serviços estivessem rodando, tivessem recebendo diversas requisições, todas elas estariam sendo
        redirecionadas para vm2, por mais que a carga de receber pudesse ser balanceada por conta dos nossos outros nós através do Routing Mesh, tudo seria tratado no fim das contas pela vm2.

        A ideia é que possamos replicar alguns tipos de serviço para que eles sejam copiados, ter diversas cópias dele rodando em diversas máquinas, como assim? Ao invés de termos uma tarefa
        para esse serviço sendo executada, vamos ter na verdade quantas quisermos espalhadas pelo nosso Swarm para que todos os nós consigam dividir esse trabalho que vai ser executado.

        Até então temos uma tarefa, se eu quiser criar uma cópia de quatro tarefas para esse serviço, o que podemos fazer? Podemos simplesmente e utilizar o comando "docker service update",
        pois queremos mais uma ver atualizar o estado do nosso serviço, queremos atualizar o número de réplicas , então "--replicas", agora queremos definir um número de réplicas, por exemplo,
        quatro, e agora qual o serviço que queremos atualizar? Esse que acabamos de criar que é "ts9vbn0i3cen"...

            $ docker service update --replicas 4 ts9

            Saída:

                overall progress: 4 out of 4 tasks
                1/4: running   [==================================================>]
                2/4: running   [==================================================>]
                3/4: running   [==================================================>]
                4/4: running   [==================================================>]
                verify: Service converged

        Repare o que ele fez, alocou quatro tarefas para esse serviço, significa que agora se dermos um "docker service ps" com o id do serviço novamente...
        
            ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS
            vv8r8q9nwyre        trusting_edison.1   aluracursos/barbearia:latest   vm2                 Running             Running 15 minutes ago
            40sbs4yh8njy        trusting_edison.2   aluracursos/barbearia:latest   vm4                 Running             Running about a minute ago
            0p2a0g0hgmgc        trusting_edison.3   aluracursos/barbearia:latest   vm3                 Running             Running about a minute ago
            k6cbg0axfsxg        trusting_edison.4   aluracursos/barbearia:latest   vm1                 Running             Running about a minute ago
        
        ...existem quatro tarefas que estão em execução para esse serviços espalhadas entre as nossas vms, 2, 4, 3, 1. Isso quer dizer que a partir desse momento, tem quatro réplicas desse
        mesmo serviço rodando espalhados pelo nosso Swarm.

        Então quando requisitarmos agora esse serviço, pode ser que ele caia tanto na vm1, quanto 2, 3 ou 4.

        Isso é bem legal porque agora efetivamente vai ter essa divisão de carga entre os nossos nós. Vamos dividir o serviço em mais réplicas para testarmos...

        Por exemplo, se atualizassemos de novo para 8 réplicas, o que o Swarm vai fazer? Ele vai alocar 8 cópias para essa tarefa desse serviço, e como eu só tenho cinco máquinas, alguma
        máquina vai ter mais de uma réplica não é verdade? 

            $ docker service update --replicas 8 ts9

            Saída:

                overall progress: 8 out of 8 tasks
                1/8: running   [==================================================>]
                2/8: running   [==================================================>]
                3/8: running   [==================================================>]
                4/8: running   [==================================================>]
                5/8: running   [==================================================>]
                6/8: running   [==================================================>]
                7/8: running   [==================================================>]
                8/8: running   [==================================================>]
                verify: Service converged

            $ docker service ps ts9

            Saída:

                ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
                vv8r8q9nwyre        trusting_edison.1   aluracursos/barbearia:latest   vm2                 Running             Running 23 minutes ago
                40sbs4yh8njy        trusting_edison.2   aluracursos/barbearia:latest   vm4                 Running             Running 10 minutes ago
                0p2a0g0hgmgc        trusting_edison.3   aluracursos/barbearia:latest   vm3                 Running             Running 10 minutes ago
                k6cbg0axfsxg        trusting_edison.4   aluracursos/barbearia:latest   vm1                 Running             Running 10 minutes ago
                zlwle2js7h0u        trusting_edison.5   aluracursos/barbearia:latest   vm5                 Running             Running 2 minutes ago
                rkuc4eqhn8fn        trusting_edison.6   aluracursos/barbearia:latest   vm5                 Running             Running 2 minutes ago
                o4lg23vdkgz4        trusting_edison.7   aluracursos/barbearia:latest   vm1                 Running             Running 2 minutes ago
                mtnugys2q76o        trusting_edison.8   aluracursos/barbearia:latest   vm2                 Running             Running 2 minutes ago
        
        Podemos ver que temos as mesmas réplicas de antes, e mais uma na vm1, mais uma na vm2 e mais duas na vm5.

        Ou seja, ele criou diversas cópias desses serviços, diversas tarefas, que agora elas estão espalhadas pelo nosso Swarm, então se em algum momento algum desses caras não conseguir lidar
        com determinado número de requisições, outros também vão poder receber, porque esse serviços está espalhado pelo nosso Swarm.

        Isso é uma das possibilidades, esse é uma das possibilidades de distribuir um serviço pelo nosso Swarm, a outra possibilidade é, quando queremos um serviço rode em todos os nós do nosso
        Swarm, nós vamos querer trabalhar com serviço global, não vale a pena criarmos um serviço replicado com um alto número de réplicas só para garantir que ele vai estar em todos os nossos
        nós, isso já é meio que trapaça e vai diminuir nosso desempenho. A maneira correta para fazer isso é criando um serviço global.


    - Para saber mais: service scale

        Vamos supor que temos um serviço com id ci10k3u7q6ti. Como podemos escalar esse serviço para ter 5 réplicas?

        Aprendemos uma das possibilidades de alterar o número de réplicas de um serviço utilizando o comando docker service update --replicas 5 ci10k3u7q6ti, mas esse não é o único meio!
        
        Para isso também temos o comando docker service scale. Utilizando o id, podemos atualizar com o comando:
        
            $ docker service scale ci10k3u7q6ti=5

        Nesse caso, definimos 5 réplicas para o serviço. Os dois comandos produzem o mesmo resultado, o segundo é apenas uma forma resumida do primeiro comando.

    
    Serviços globais:

        Então vamos falar agora sobre os nossos serviços que vão ser criados em modo global, vimos ainda há pouco os serviços replicados, o que significa, como eles funcionam e tudo mais,
        mas agora vamos ver o nosso serviço global, que já falamos no início do vídeo passado que são os serviços que vão ter uma tarefa, uma instância desse serviço rodando em cada nó do
        nosso Swarm.

        E para que queremos isso? Queremos isso por algum motivo específico, por exemplo, garantir monitoramento, garantir segurança, queremos um serviço que seja crítico para ser executado
        em todos os nós, todos os nós no nosso caso vão depender da estância desse serviço, então queremos garantir que ele seja executado em todos esses nós.

        E como é que fazemos isso? Vemos que por padrão, se dermos o nosso comando "docker service create" e criar novamente o serviço o que acontece? Já sabemos que ele vai criar um réplica
        dele, vai criar ele no modo replicado e vai alocar ele aleatoriamente em uma máquina virtual, em algum nó do nosso Swarm.

        Mas e se quisermos criar esse serviço em modo global? Se quisermos que esse serviço tenha uma instância dele rodando em cada nó do nosso Swarm, como é que nós podemos garantir isso?
        Basta que, no meio do comando, antes de nós explicitarmos a imagem que queremos utilizar, podemos definir o modo em que esse serviço vai ser executado.

        Poderíamos colocar "--mode replicated", que é o padrão, ele já cria um serviço em modo replicado, ou podemos colocar "global", ou seja, global ele vai criar esse serviço em modo global
        e o que vai acontecer agora? Qual vai ser a grande diferença? A grande diferença é que de uma só vez ele vai começar a criar cinco instância desse serviço, cinco tarefas em diferentes
        nós, em todos os nós do Swarm.

            $ docker service create -p 8080:3000 --mode global aluracursos/barbearia

            Saída:

                overall progress: 5 out of 5 tasks
                in3t87hcu77q: running   [==================================================>]
                q5duspz3uggl: running   [==================================================>]
                rzmb7ke49zeo: running   [==================================================>]
                jh0vk0wrgiru: running   [==================================================>]
                mr4jjwio2une: running   [==================================================>]
                verify: Service converged

        Se dermos um "docker service ls", o que ele irá mostrar?

            ID                  NAME                  MODE                REPLICAS            IMAGE                          PORTS
            ck9nppwfl8wr        compassionate_raman   global              5/5                 aluracursos/barbearia:latest   *:8080->3000/tcp

        Ele está mostrando que tem cinco réplicas no total de cinco esperadas funcionando, mas agora o modo é global. Se utilizarmos agora o "docker service ps "id serviço""...

            ID                  NAME                                            IMAGE                          NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
            h2qr8wffwhzb        compassionate_raman.rzmb7ke49zeocehp5liaq97ez   aluracursos/barbearia:latest   vm5                 Running             Running 12 minutes ago
            97ka1hotzs1x        compassionate_raman.q5duspz3ugglfw664ln3xr18n   aluracursos/barbearia:latest   vm1                 Running             Running 12 minutes ago
            oon8gbxhb7y4        compassionate_raman.jh0vk0wrgiruzrejjhfor8l3u   aluracursos/barbearia:latest   vm4                 Running             Running 12 minutes ago
            xxb127sp3ten        compassionate_raman.mr4jjwio2une2dn51od448g9q   aluracursos/barbearia:latest   vm3                 Running             Running 12 minutes ago
            5qf60a5h3iob        compassionate_raman.in3t87hcu77qnk13ln538rg29   aluracursos/barbearia:latest   vm2                 Running             Running 12 minutes ago

        Ele mostra que em todos os nossos nós está rodando um tarefa, uma instância desse serviço.

        Isso significa que no dispatcher, no momento em que definimos qual nó vai carregar determinado serviço, nesse ponto vai ser diferente, o dispatcher vai dizer "como o serviço é global
        eu quero que todos os nós carreguem uma réplica desse serviço", ou seja, o serviço vai ser global e todos os nós vão ter uma instância dele.

        Então se caso for alguma aplicação crítica, ela vai estar sendo executada em todos os nossos nós, que talvez seja o que queiramos, ou não, depende, mas no fim das contas, usar só o
        serviço global ou só serviço replicado para tudo não é a solução, no mundo real, utilizaremos num mesmo Swarm serviços globais e serviços replicados.
        
    
    Nesta aula, aprendemos:

        - Serviços replicados rodam em um ou mais nós do swarm
        - Serviços globais rodam em todos os nós do swarm
        - Nós managers por padrão trabalham como workers
        - Serviços como monitoramento e segurança são bons exemplos de serviços globais
        - Como definir o modo que o serviço será criado utilizando a flag --mode no momento da criação do serviço

    
    Questões aula 05:

        1 - No último vídeo, aprendemos diversos aspectos sobre serviços replicados. Quais das alternativas abaixo são verdadeiras sobre esse tipo de serviço?

            Selecione 2 alternativas

            R1: Serviços replicados podem rodar em apenas um nó.

                Alternativa correta! Basta definirmos para o serviço ter apenas uma réplica.

            R2: Serviços por padrão são criados no modo replicado.

                Alternativa correta! Quando não informamos o modo desejado, criamos serviços replicados por padrão.

        
        2 - No último vídeo, aprendemos diversos aspectos sobre serviços globais. Quais das alternativas abaixo são verdadeiras sobre esse tipo de serviço?

            Selecione uma alternativa

            R: Bons exemplos de serviços globais são serviços de monitoramento e segurança.

            Alternativa correta! Serviços que são críticos à aplicação como um todo podem e devem ser executados como globais para que todos os nós possam ser devidamente monitorados e 
            estejam seguros.


Aula 06: Driver Overlay -------

    A rede ingress:

        Agora vamos continuar nesse aprofundamento que vamos tendo do Swarm, precisamos entender o que agora? Vimos sobre todas as partes que o Swarm consegue aos nós mas não entendemos de
        certa forma como isso acontece.

        Quem é o grande responsável por toda essa comunicação que acontece entre os diferentes nós de um Swarm? A questão aqui é a seguinte, se viermos aqui e utilizamos o comando...

            $ docker service ls //Vemos que estamos sem serviços, paramos tudo o que tinha da aula anterior, se não utilize o comando "docker service rm "id serviço""

        Agora utilizaremos aquele comando para listas as nossas redes...

            $ docker network ls

            Saída...

                NETWORK ID          NAME                DRIVER              SCOPE
                68a44a487ecc        bridge              bridge              local
                d26db2c1ecac        docker_gwbridge     bridge              local
                cc604e07f99f        host                host                local
                ks43cn60dvn7        ingress             overlay             swarm
                eb687475fca7        none                null                local

        Ele listou as redes em que a nossa máquina, neste caso a vm1, está conectada.
        Vemos que temos uma rede com o nome de bridger e a semelhante a ela docker_gwbridge, utilizando os drivers bridge, host.

        E o que cada um desses caras faz? Vamos dar aquela lembrada rapidinha, o driver bridge permite a comunicação entre diferentes containers em um mesmo host, então se nós temos 10
        containers em uma máquina só, todos eles conseguem se comunicar por causa desse driver bridge.

        E o driver host, para que ele serve? O driver host, serve para fazer a comunicação entre container e máquina, então a máquina que está carregando esse container se comunica com o IP
        desse container por causa desse driver host.

        Mas o que tem de novo então? A novidade aqui para nós é esse cara aqui, o ingress, se repararmos os escopos de todas as outras redes, veremos que é local, mas esse cara não, ele 
        utiliza um driver diferente, o overlay, tem o escopo de Swarm, e é exatamente isso que vamos analisar agora.

        Esse cara aqui overlay, esse driver que tem a rede ingress, é ele que faz toda mágica acontecer, faremos o seguinte, utilizaremos a vm1 e também a vm3.

        Se utilizarmos o comando "docker network ls" na vm3, o que teremos? A mesma coisa? Mais ou menos...ele vai nos mostrar que as redes de escopo local da vm3...

            NETWORK ID          NAME                DRIVER              SCOPE
            6812a7a731fd        bridge              bridge              local
            6b6c8d294be4        docker_gwbridge     bridge              local
            5a75dd387479        host                host                local
            ks43cn60dvn7        ingress             overlay             swarm
            9c001dc45fb3        none                null                local

        Ou seja, todas as redes de escopo "scope" local, tem ids diferentes daqueles que vimos na VM1...

        Mas a rede que é a overlay aqui, a nossa ingress, ele tem um mesmo ID, ou seja, todos os nosso nós do nosso Swarm, seja ele manager ou worker, eles estão todos dentro dessa mesma rede
        que é a ingress, quando um nó entra em um Swarm por padrão ele entra nessa rede ingress, essa rede é criada junto com o nosso cluster lá no comando do "docker swarm init" e todos que
        entrarem vão entrar por padrão nessa rede.

        Então qualquer nó que fizermos esse comando aqui, vai nos mostrar o mesmo ID da rede ingress, seja ele um manager ou um worker...Vamos tentar com a vm5...

            NETWORK ID          NAME                DRIVER              SCOPE
            5bea722cc3a7        bridge              bridge              local
            f7bd28c990d5        docker_gwbridge     bridge              local
            c4b80b840b6d        host                host                local
            ks43cn60dvn7        ingress             overlay             swarm *
            fdb8ca29ebe2        none                null                local

        Mas o que isso tudo quer dizer? Isso tudo quer dizer que agora que temos essa única rede entre todos os nossos nós, conseguimos fazer aquela comunicação que vem sendo feita de maneira
        muito mais direta, e o que acontece agora? Essa comunicação agora em aspectos mais técnicos, é feita de maneira completamente criptografada.

        Se utilizarmos o comando "docker node inspect" na nossa vm1, por exemplo, colocar no final do comando um "--pretty" para ele exibir bonito, não em formato de lista, ele vai "parciar"
        o JSON, o que ele vai mostrar?

            $ docker node inspect vm1 --pretty

            Saída: 

                ID:                     q5duspz3ugglfw664ln3xr18n
                Hostname:               vm1
                Joined at:              2021-04-13 03:27:51.01200965 +0000 utc
                Status:
                State:                 Ready
                Availability:          Active
                Address:               192.168.99.106
                Manager Status:
                Address:               192.168.99.106:2377
                Raft Status:           Reachable
                Leader:                No
                Platform:
                Operating System:      linux
                Architecture:          x86_64
                Resources:
                CPUs:                  1
                Memory:                985.4MiB
                Plugins:
                Log:           awslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog
                Network:               bridge, host, ipvlan, macvlan, null, overlay
                Volume:                local
                Engine Version:         19.03.12
                Engine Labels:
                - provider=virtualbox
                TLS Info:
                TrustRoot:
                -----BEGIN CERTIFICATE-----
                MIIBajCCARCgAwIBAgIUfwk5k4j8LKc1DrFTKxLl2Iv7aXwwCgYIKoZIzj0EAwIw
                EzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMjEwNDEwMDA0ODAwWhcNNDEwNDA1MDA0
                ODAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH
                A0IABFZP1F4lbH91QCnQ6pulI8D9loAaz9cnLeN1IGaJgp+OR1MJQdLVgxsyrITk
                vgTlLZWd/8LXS1+lUqdHor0wjt+jQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB
                Af8EBTADAQH/MB0GA1UdDgQWBBT+IoF3Y1n5aJ2dTDAEo5DmgHUTtzAKBggqhkjO
                PQQDAgNIADBFAiEApHYMm97UXu9dsED2UKosYyXQ884vQnTpodQF2pqjZqkCIFUd
                58U+9+DAgntlXc15eys2x/nF9+2DGkLqUgtjbnku
                -----END CERTIFICATE-----
                
                Issuer Subject:        MBMxETAPBgNVBAMTCHN3YXJtLWNh
                Issuer Public Key:     MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEVk/UXiVsf3VAKdDqm6UjwP2WgBrP1yct43UgZomCn45HUwlB0tWDGzKshOS+BOUtlZ3/wtdLX6VSp0eivTCO3w==

        Essa última parte aqui "BEGIN CERTIFICATE" - "END CERTIFICATE", é exatamente um certificado, toda a nossa comunicação entre todos os nossos nós é criptografada.

        Temos essa comunicação completamente segura, mas se começarmos a olhar as outras informações do nosso "docker node inspect", veremos o que? Além de aspectos como recurso sendo
        consumido, temos IP, hostname e todas as outras informações que já tinhamos, como o nosso endereço de manager (com porta) e tudo mais.

        O ponto é o seguinte, como estamos em diferentes máquinas, tecnicamente eu estou com cinco máquinas diferentes abertas aqui, da vm1 até a vm5, se nós tentarmos no final fazer um
        comando clássico de dar um ping. Vamos descobrir o ping da nossa vm4, que nós quase não usamos...

            $ docker node inspect vm4 --pretty

            Saída:

                ID:                     jh0vk0wrgiruzrejjhfor8l3u
                Hostname:               vm4
                Joined at:              2021-04-10 01:30:00.912753049 +0000 utc
                Status:
                State:                 Ready
                Availability:          Active
                Address:               192.168.99.109
                Platform:
                Operating System:      linux
                Architecture:          x86_64
                Resources:
                CPUs:                  1
                Memory:                985.4MiB
                Plugins:
                Log:           awslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog
                Network:               bridge, host, ipvlan, macvlan, null, overlay
                Volume:                local
                Engine Version:         19.03.12
                Engine Labels:
                - provider=virtualbox
                TLS Info:
                TrustRoot:
                -----BEGIN CERTIFICATE-----
                MIIBajCCARCgAwIBAgIUfwk5k4j8LKc1DrFTKxLl2Iv7aXwwCgYIKoZIzj0EAwIw
                EzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMjEwNDEwMDA0ODAwWhcNNDEwNDA1MDA0
                ODAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH
                A0IABFZP1F4lbH91QCnQ6pulI8D9loAaz9cnLeN1IGaJgp+OR1MJQdLVgxsyrITk
                vgTlLZWd/8LXS1+lUqdHor0wjt+jQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB
                Af8EBTADAQH/MB0GA1UdDgQWBBT+IoF3Y1n5aJ2dTDAEo5DmgHUTtzAKBggqhkjO
                PQQDAgNIADBFAiEApHYMm97UXu9dsED2UKosYyXQ884vQnTpodQF2pqjZqkCIFUd
                58U+9+DAgntlXc15eys2x/nF9+2DGkLqUgtjbnku
                -----END CERTIFICATE-----
                
                Issuer Subject:        MBMxETAPBgNVBAMTCHN3YXJtLWNh
                Issuer Public Key:     MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEVk/UXiVsf3VAKdDqm6UjwP2WgBrP1yct43UgZomCn45HUwlB0tWDGzKshOS+BOUtlZ3/wtdLX6VSp0eivTCO3w==

            Descobrimos que o IP da vm4 é 192.168.99.109...Se tentarmos pingar da vm1 a vm4 o que vai acontecer?

                $ ping 192.168.99.109

                Saída:

                    PING 192.168.99.109 (192.168.99.109): 56 data bytes
                    64 bytes from 192.168.99.109: seq=0 ttl=64 time=0.351 ms
                    64 bytes from 192.168.99.109: seq=1 ttl=64 time=0.452 ms
                    64 bytes from 192.168.99.109: seq=2 ttl=64 time=0.464 ms
                    64 bytes from 192.168.99.109: seq=3 ttl=64 time=0.459 ms

        Ela respondeu ao ping! Ou seja, ele está recebendo, está conseguindo enviar informações para a nossa vm4, por mais que sejam máquinas diferentes, nós conseguimos nos cominicar agora
        por causa de quem? Exatamente, da nossa rede overlay.

        E ela também quer fazer todos esse papel de ajudar a nós com aquele Routing Mesh, porque antes de batermos em qualquer um dos nossos nós quando enviamos uma requisição, essa requisição
        passa pela rede overlay, que é meio que um filtro, ela vai agir antes de passar para qualquer nó, e ela vai saber como resolver isso para nós, mas ela por padrão, da maneira que ela
        vem, ela já ajuda a nós de grande formar, mas ainda conseguimos tirar ainda mais proveito dela, só que isso vamos ver na nossa próxima aula.

    
    Service Discovery:

        Vamos continaur agora com o nosso estudo sobre o driver overlay. O que mais ele pode fazer de interessante? Um conceito bem famoso no dia de hoje, bem moderno, é um cara chamado Service
        Discovery, o que é o Service Discovery? Se formos traduzir literalmente é descobrir um serviço, mas é descobrir esse serviço à partir do nome dele.

        Ou seja, não precisamos ficar chamando um serviço ou outro pelo IP dele e sim chamar ele simplesmente pelo nome, que é uma maneira bem mais prática de conseguirmos comunicar diferentes
        serviços.

        Então como é que será, se dermos um "docker network ls" aqui, como é que será que utilizando esse driver overlay conseguimos fazer isso?

            NETWORK ID          NAME                DRIVER              SCOPE
            9a68a33c3af7        bridge              bridge              local
            d26db2c1ecac        docker_gwbridge     bridge              local
            cc604e07f99f        host                host                local
            ks43cn60dvn7        ingress             overlay             swarm
            eb687475fca7        none                null                local
        
        Vamos subir um serviço para fazermos esse experimento, e como é que criamos um serviço? utilizamos o comando "docker service create", daremos um nome para esse serviço, colocaremos
        "--name" e então o nome que queremos para o serviço, vamos chama-lo de "servico" mesmo, só vamos traduzir.

        Queremos também criar duas réplicas desse serviço, por que duas réplicas? Porque assim vamos ter dois containers, vamos fazer esses dois tentar comunicar esses dois containers, e
        para podermos criar um serviço já definido com duas réplicas, basta colocarmos no momento da criação dele a flag "--replicas" e informar o número de réplicas que queremos. O número de
        tarefas a ser gerado definimos como 2 neste exemplo, então serão gerados dois containers para esse cara.

        E por fim, a imagem que vamos utilizar, que é no caso o "alpine", que é uma distribuição Linux bem leve, só vamos utilizar ela para testar o terminal. E agora precisamos adicionar um
        comando para que o nosso novo container rode um pequeno processo para que assim que seja criado não caia, "sleep 1d", poderia ser qualquer comando que travasse o terminal.

            $ docker service create --name servico --replicas 2 alpine sleep 1d

            Saída:

                lbr10f60qe45usk1ttne8bvub
                overall progress: 2 out of 2 tasks
                1/2: running   [==================================================>]
                2/2: running   [==================================================>]
                verify: Service converged

        Depois de apertarmos o Enter, vimos que subiram duas tarefas, ele vai colocar a princípio nós aleatórios porque não definimos nenhum constraint para esse serviço. Vamos ver onde foi
        criado...

            $ docker service ls

            Saída:

                ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
                lbr10f60qe45        servico             replicated          2/2                 alpine:latest

            Temos agora o ID do serviço, vamos ver em quais nós as tarefas deste serviço foram alocadas...

                $ docker service ps lbr

                Saída:

                    ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
                    do8fouae0iyp        servico.1           alpine:latest       vm2                 Running             Running 6 minutes ago
                    e61oy5cq6lm5        servico.2           alpine:latest       vm4                 Running             Running 6 minutes ago

            Podemos ver que as tarefas deste serviço foram alocadas nos nós 2 e 4, ou seja, foi criado um container na nossa vm2 e um container na nossa vm4. Qual experimento podemos fazer
            agora? Temos dois containers, vamos dar um "docker container ls" na nossa vm2...

                $ docker-machine ssh vm2

                $ docker container ls

                Saida:

                    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
                    3e4f999123fa        alpine:latest       "sleep 1d"          10 minutes ago      Up 10 minutes                           servico.1.do8fouae0iypwnoeojypf2s8s

            Agora temos o nosso container do nosso nó vm2, nome/ID - servico.1.do8fouae0iypwnoeojypf2s8s, faremos o mesmo procedimento para pegar as informações do nosso nó vm4...
            Nosso nó vm4 tem o container com nome/ID - servico.2.e61oy5cq6lm5n4jok83ltz3m6.

        Eu quero fazer um experimento para tentar comunicar esses dois conteiners via nome, então primeiramente vamos acessar o container...Como é o comando para acessar um container?
        "docker exec", colocaremos também o "-it" do modo interativo, que vimos no curso de Docker, e vou informar o container que eu quero fazer isso, que é esse servico.1.do8...
        E o comando que queremos rodar junto para interagir com o container é o "sh", que é do terminal alpine.

            $ docker exec -it servico.1.do8fouae0iypwnoeojypf2s8s sh

            / #

            Estou dentro do terminal do alpine, dentro do meu container. Vou fazer a mesma coisa na vm4.

            $ docker exec -it servico.2.e61oy5cq6lm5n4jok83ltz3m6 sh

            / #

        Eu vou tentar agora conectar esses dois containers, fazer uma comunicação entre eles via nome, ou seja, eu vou tentar à partir de um container acessar o outro. Primeiro vamos tentar
        um pint, do nó 4 para o nó 2...

            $ ping servico.1.do8fouae0iypwnoeojypf2s8s

            Saída:

                ping: bad address 'servico.1.do8fouae0iypwnoeojypf2s8s'

        Repare que ele está falando que foi "bad address", ele não conseguiu se comunicar aquele container.

        Mas se vier aqui e utilizasse algum comando por exemplo, de IP, desse um "ifconfig" por exemplo, teremos o que?

            / # ifconfig

            Saída:

                eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02
                inet addr:172.17.0.2  Bcast:172.17.255.255  Mask:255.255.0.0
                UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
                RX packets:23 errors:0 dropped:0 overruns:0 frame:0
                TX packets:4 errors:0 dropped:0 overruns:0 carrier:0        
                collisions:0 txqueuelen:0
                RX bytes:1946 (1.9 KiB)  TX bytes:274 (274.0 B)
    
                lo        Link encap:Local Loopback
                inet addr:127.0.0.1  Mask:255.0.0.0
                UP LOOPBACK RUNNING  MTU:65536  Metric:1
                RX packets:0 errors:0 dropped:0 overruns:0 frame:0
                TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
                collisions:0 txqueuelen:1000
                RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

        Temos o IP dele que é 172.17.0.2, vamos utilizar esse IP para tentar um ping agora do nosso nó vm2...

            / # ping 172.17.0.2

            Saída:

                PING 172.17.0.2 (172.17.0.2): 56 data bytes
                64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.044 ms
                64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.064 ms
                64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.065 ms
                64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.062 ms

        O que aconteceu agora? Ele está pingando! Ou seja, eles conseguem se comunicar, mas a princípio sem comunicação via nome, não conseguimos chamar um serviço pelo nome dele até então.

        Ou seja, temos uma restrição aqui na nossa overlay até agora. Vamos ver como conseguimos contornar este tipo de problema.

    
    User-Defined Overlay:

        Como faremos agora o Docker fazer e entender esse Service Discovery, como é que vamos conseguir fazer a comunicação entre diferentes containers, diferentes tarefas à partir
        simplesmente do nome dessa tarefa, desse container, e não necessariamente à partir do IP.

        Eu já parei o serviço que estava em execução anteriormente, se dermos um "docker service ls" ele não está mais rodando, e vamos fazer o que agora? Vamos dar um "docker network ls",
        e ver mais uma vez nossas redes.

            NETWORK ID          NAME                DRIVER              SCOPE
            f4ac554b54af        bridge              bridge              local
            d26db2c1ecac        docker_gwbridge     bridge              local
            cc604e07f99f        host                host                local
            ks43cn60dvn7        ingress             overlay             swarm
            eb687475fca7        none                null                local

        No outro curso de Docker estavamos trabalhando com Stand Alone, em um único host, podíamos criar as nossas próprias redes utilizando driver bridge, as "user defined bridge" que podíamos
        criar, agora a ideia vai ser bem parecida, nós criamos as nossas próprias redes utilizando drive overlay, e vamos ver o que isso vai ajudar a nós.

        Como é que criamos uma rede no Docker? Já sabemos que é com o comando "docker network create", e precisamos informar qual o driver que queremos utilizar, para isso colocamos o "-d" e
        informamos que queremos utilizar o driver "overlay". Por fim precisamos colocar o nome da rede que vai ser criada, que eu vou chamar de "my_overlay", vamos lá, Enter...
        
            $ docker network create -d overlay my_overlay

            (sq8zp7u7i7u9gea1k6up75wx8)
        
        ...e se dermos um "docker network ls", veremos a nossa rede recém-criada.
        
            NETWORK ID          NAME                DRIVER              SCOPE
            f4ac554b54af        bridge              bridge              local
            d26db2c1ecac        docker_gwbridge     bridge              local
            cc604e07f99f        host                host                local
            ks43cn60dvn7        ingress             overlay             swarm
            sq8zp7u7i7u9        my_overlay          overlay             swarm *******
            eb687475fca7        none                null                local

        Se formos na nossa vm3 e na nossa vm2 que são as outras managers que tem no nosso Swarm, e executarmos o "docker network ls", o que acontece?
            
            NETWORK ID          NAME                DRIVER              SCOPE
            sq8zp7u7i7u9        my_overlay          overlay             swarm

        Está aparecendo a mesma rede com o mesmo ID, a rede que criamos na nossa manager vm1, se fizermos isso nos nós workers vm4 e vm5, podemos reparar que não tem a nossa "my_overlay".

        Isso porque vem o primeiro segredo, quando criamos a rede com o driver overlay no Swarm, elas são criadas de maneira "lazy", então todos os managers a princípio vão saber da existência
        dessa rede, mas os workers só vão listar essa rede aqui com o "docker network ls", à partir do momento em que alguma tarefa, algum container for colocado e utilize essa rede dentro 
        desse nó.

        Então vamos fazer exatamente isso agora. Vamos voltar na nossa vm1 e agora vamos subir dois serviços no nosso worker. Já sabemos como sobe o serviço "docker service create", e vamos
        fazer o que? Eu quero que esse serviço tenha mais uma vez aqui o nome de "servico", isso não é nada novo, é uma questão semântica para nós.

        Agora vem o ponto chave, queremos que ele utilize o driver overlay e que utilize a rede que criamos, que é a "my_overlay". Então repara no momento da criação em que estamos criando um
        serviço, nós podemos definir a rede em que esse serviço vai rodar, "--network", então além de ele ser executado por padrão aqui na ingress, ele também vai ser executado na my_overlay, duas redes
        diferentes.

        E agora vamos fazer o mesmo passo que fizemos antes, eu quero que ele tenha duas "--replicas" e vai utilizar o "alpine" aqui, com um "sleep" de um dia...

            $ docker service create --name servico --network my_overlay --replicas 2 alpine sleep 1d...

            Saída:

                nxz1pc9h88sswb5a9785bjph0
                overall progress: 2 out of 2 tasks
                1/2: running   [==================================================>]
                2/2: running   [==================================================>]
                verify: Service converged

            ...ele criou duas tarefas.

        Vamos utilizar o comando "docker service ps "id_servico"", para ver onde ele alocou...

            $ docker service ps nxz

            Saída:

                ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
                up1px3miblaj        servico.1           alpine:latest       vm2                 Running             Running 3 minutes ago
                riurwcvh32i3        servico.2           alpine:latest       vm4                 Running             Running 3 minutes ago

            ...ele está rodando na vm4 que é um worker e na vm2 que é um manager. Se dermos um "docker network ls" na vm4, o que vai acontecer?

                NETWORK ID          NAME                DRIVER              SCOPE
                6b0d4ff43713        bridge              bridge              local
                547eeeb28baa        docker_gwbridge     bridge              local
                68094332e67e        host                host                local
                ks43cn60dvn7        ingress             overlay             swarm
                sq8zp7u7i7u9        my_overlay          overlay             swarm *******
                da3087950007        none                null                local

            Repare que agora ele está listando a my_overlay...

        Porque, se dermos um "docker container ls", o que está acontecendo aqui?

            CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
            741cdf2a1a16        alpine:latest       "sleep 1d"          7 minutes ago       Up 7 minutes                            servico.2.riurwcvh32i3kgmzzeynzuel

        Ele tem um container, ele tem uma tarefa alocada, uma tarefa alocada utilizando a rede "my_overlay", se agora utilizarmos o comando "docker network inspect"...

            $ docker network inspect my_overlay

            Saída:

                [
                    ...
                        },
                        "ConfigOnly": false,
                        "Containers": {
                            "741cdf2a1a165853427ea1a9bd4dee5cb81a9e58e29e07f036839e1b26072a29": {
                                "Name": "servico.2.riurwcvh32i3kgmzzeynzuelp",
                                "EndpointID": "ea988b4aad6ec413325dfdeeda6679092cc2b9095a2e6689f00b815a2706cf8a",
                                "MacAddress": "02:42:0a:00:01:03",
                                "IPv4Address": "10.0.1.3/24",
                                "IPv6Address": ""
                            },
                            "lb-my_overlay": {
                                "Name": "my_overlay-endpoint",
                                "EndpointID": "bd36c24d642cce4d3abf7c86fed27e3a2366a889c3238b185abdd911409efe6f",
                                "MacAddress": "02:42:0a:00:01:05",
                                "IPv4Address": "10.0.1.5/24",
                                "IPv6Address": ""
                            }
                        }
                    ...
                ]

        Podemos ver o que? Exatamente o container que foi criado utilizando a rede "my_overlay", o serviço dois aqui, mas se repararmos bem, temos duas tarefas que foram criadas rodando nessa
        rede my_overlay, porque ele só mostra um container? Esse abaixo do container é o endpoint da rede, porque ele só mostra um container? Que é o serviço 2, cadê o serviço 1 que foi criado?
        É porque até então, o "docker network inspect", só vai mostrar os containers que estão nessa rede neste container.

        Dentro do nó que executou esse comando, então quando executamos o "docker network inspect" dentro da vm4, ele mostrou o container que está nessa rede dentro apenas da vm4.

        Mas agora o ponto é o seguinte, vamos executar um "docker exec -it" nesse serviço 2, e acessar o terminal do alpine neste container...

            $ docker exec -it servico.2.riurwcvh32i3kgmzzeynzuelp sh

            ...pegamos o nome do container, a outra réplica, que está sendo executada na vm2, servico.1.up1px3miblajtqh5n9h40u9em. Agora se tentarmos realizar um ping da vm4 utilizando o nome
            do container que está na vm2...

                / # ping servico.1.up1px3miblajtqh5n9h40u9em

                Saída:

                    PING servico.1.up1px3miblajtqh5n9h40u9em (10.0.1.4): 56 data bytes
                    64 bytes from 10.0.1.4: seq=0 ttl=64 time=0.587 ms
                    64 bytes from 10.0.1.4: seq=1 ttl=64 time=0.500 ms
                    64 bytes from 10.0.1.4: seq=2 ttl=64 time=0.578 ms
                    64 bytes from 10.0.1.4: seq=3 ttl=64 time=0.620 ms

            ...nós conseguimos fazer uma requisição de um container para outro em nós diferentes apenas pelo nome do serviço, nós não precisamos em momento algum procurar pelo IP desse cara,
            nós só passamos o nome e o Docker resolveu isso para nós chamando o serviço com esse nome.

        Isso é muito legal, é uma coisa muito interessante que está sendo muito utilizada no mercado, que é o conceito de "Service Discovery". Agora veremos com o é feito o processo de produção
        de serviços com o Docker.

    Para saber mais: Containers e Overlay:

        Por mais que o driver overlay seja responsável por comunicar múltiplos hosts em uma mesma rede, também podemos conectar containers em escopo local criados com o comando "docker container 
        run" em redes criadas com esse driver.

        Para isso, basta no momento da criação da rede utilizarmos a flag --attachable:
        
            $ docker network create -d overlay --attachable my_overlay

        Com o comando acima, conseguiremos conectar tanto serviços como containers "standalone" em nossa rede my_overlay.

    O que aprendemos?

        Nesta aula, aprendemos:

        - A rede ingress é a padrão criada junto com nosso swarm
        - O driver overlay é utilizado para a comunicação entre nós em diferentes hosts
        - Como criar nossas próprias redes com o driver overlay utilizando o comando docker network create -d overlay
        - Redes overlays criadas manualmente (User-Defined Overlay) permitem a comunicação entre serviços por seus nomes (Service Discovery)
        - As redes criadas com o driver overlay são listadas de maneira lazy para workers

    Questões Aula 06:
            
        1 - Conhecemos agora a rede ingress e vimos que, além de ser a rede padrão criada pelo Docker Swarm para os nós que fazem parte cluster, ela utiliza o driver overlay. 
            Qual das alternativas abaixo contém uma finalidade desse driver?

            Selecione 2 alternativas

            R1: O driver overlay comunica seus dados de maneira criptografada para garantir nossa segurança.

                Alternativa correta! Executando o comando docker node inspect podemos ver os certificados de segurança.

            R2: O driver overlay permite a comunicação entre diferentes hosts de um mesmo cluster.

                Alternativa correta! Além de garantir a segurança, o driver overlay também permite a comunicação entre diferentes hosts rodando Docker.

        
        2 - A rede ingress, utilizando o driver overlay, consegue fazer diversas coisas por nós, como garantir por exemplo, a comunicação entre diferentes nós de maneira altamente segura. 
            Porém, ela também possui certas limitações. Qual das alternativas abaixo contém uma dessas limitações?

            Selecione uma alternativa

            R: Serviços conseguem se comunicar apenas via endereço IP.

            Alternativa correta! Temos que saber o endereço IP do serviço que queremos nos comunicar utilizando a rede ingress.

        
        3 - Na última aula, aprendemos alguns fatos novos sobre User-Defined Overlay networks. Quais das alternativas abaixo contém informações verdadeiras sobre redes criadas manualmente?

            Selecione 2 alternativas

            R1: User-Defined Overlay são criadas de maneira lazy para workers.

            Alternativa correta! Essas redes só serão reconhecidas por workers que rodarem tarefas que utilizem a rede.

            R2: Serviços que utilizam redes customizadas conseguem descobrir outros serviços diretamente por nome.

            Alternativa correta! Podemos utilizar o conceito de Service Discovery com User-Defined Overlay.


Aula 07: Deploy com Docker Stack -------

    Lembrando do Docker Compose:

        Agora estamos nos aproximando de um momento muito importante, para fazer o que? Para pararmos para pensar, como é que é feito o deploy de uma aplicação utilizando o Swarm?

        Como é que subimos esses serviços se intercomunicando, tendo essa comunicação ali coletiva entre diferentes hosts para que consigamos ter uma grande aplicação composta por vários
        desses serviços independentemente? É meio que aquela ideia de micro serviços, vamos ter vários pontinhos isolados, vários serviços independentes, formando lá uma grande aplicação com
        eles já tendo toda essa escalabilidade para eles conseguirem se resolver.

        O que conseguíamos fazer lá com o Docker Stand Alone, trabalhando em um host só? Podíamos utilizar aquele tal de arquivo chmado docker-compose, utilizando arquivo yml podíamos definir
        além da versão, definíamos, repara a palavra-chave, 'serviços', nós já víamos essa palavra desde esse momento e não associavamos, podemos associar e criar diversos serviços definindo
        uma imagem...Obs: Arquivo do Docker compose na pasta do curso.

        Então vamos ter um serviço chamado redis utilizando a imagem aqui do 'redis:alpine', determinada a rede, rede network front-end, que foi declarada onde? Lá no final do nosso arquivo
        assim como a rede backend e podemos utilizar também o volume 'db-data', vamos fazer isso em alguns momentos.

        Então esse aqui é um docker compose de exemplo que estamos visualizando e que criamos diversos serviços, é claro, revisando aqui, definindo uma imagem, definindo um volume para esse
        serviço fazendo o binding, definindo uma rede, fazendo o binding de portas também, na porta 80 com a porta 5000, informando dependências que esse serviço depende.

        Diversas coisas que conseguíamos fazer já com esse tal de docker-compose, mas num contexto local, sem envolver diversos hosts, poderíamos utilizar o comando clássico, não precisa
        executar, mas só para lembrarmos, "docker-compose up ...". Conseguiríamos contruir a partir desse arquivo, docker-compose, diversos containers de uma única vez, subindo todos
        simultaneamente para fazer uma aplicação local.

        A ideia seguindo esse conceito de micro serviços, é que vamos fazer essa mesma coisa, só que em vez de entregarmos simples containers, vamos entregar diversos serviços dispostos,
        espalhados pelo nosso cluster em diversos nós, e eles vão se comunicar também a fim de construir uma grande aplicação.

        Mas cada um desses serviços que vamos ter a partir daqui, vão ser isolados, então vamos ver como é que podemos utilizar esse tal de docker-compose, colocando algumas novas instruções
        que vieram a partir da versão três que poderemos utilizar.

        Como é que podemos integrar esse docker compose, o arquivo com o Swarm, será que ele provê alguma maneira para podermos utilizar um arquivo yaml, dispor diversos serviços de uma vez?
        Não precisar ficar subindo um serviço por vez.

    
    Definindo o arquivo de composição:

        O arquivo com as modificações feitas nessa aula pode ser obtido aqui: https://caelum-online-public.s3.amazonaws.com/1486-docker-swarm-cluster-container/07/docker-compose.yml

        Atenção! Houve uma mudança em todo o repositório do Postgres no Docker Hub. Para o devido funcionamento, devemos utilizar uma variável de ambiente para ignorar o uso de senhas. A
        alteração já foi feita no arquivo para download:

            db:
                image: postgres:9.4
                volumes:
                - db-data:/var/lib/postgresql/data
                networks:
                - backend
                deploy:
                placement:
                    constraints: [node.role == manager]
                environment:
                    POSTGRES_HOST_AUTH_METHOD: trust

        Pessoal o que almejamos aqui? Queremos fazer o seguinte, agora utilizando esse docker-compose que já vimos tanto lá do curso anterior, vimos as instruções e a capacidade dele em nos
        ajudar, vamos utilizar as novas instruções que vieram na versão três para lançar não agora um simples container, mas serviços, construções específicas do Swarm que vão nos ajudar, seja
        com política de restart, seja com limitações dos nossos nós ou dos nossos serviços e assim por diante.

        E utilizando essas instruções, vamos ver que conseguimos, subindo esses múltiplos serviços, alcançar resultados bem legais como por exemplo, subir um serviço que vai ser responsável por
        ser o visualizador de como está rodando cada container, cada serviço, cada tarefa dentro do nosso Swarm.

        Então olha que legal, esse cara aqui que é o visualizer, ele mostra para nós que na vm1 está rodando alguns serviços, na vm2 está rodando uma única tarefa, na vm3 duas tarefas, e quais 
        são, e assim por diante.

        Vamos ter um core, faremos o seguinte, na parte da aplicação em si faremos uma votação entre gatos e cachorros e podemos votar entre estas duas espécies, teremos uma tela para mostrar
        o resultado das votações, poderemos alterar o nosso voto também.

        E essa aplicação não será tão trivial assim, ela terá multiplos serviços, local do repositório da aplicação: 
            'https://github.com/dockersamples/example-voting-app' 
        
        ela tem código aberto para qualquer um que quiser visualizar e utilizar, é uma aplicação de votação feita em 'python', que se comunica com o 'redis' que conta os votos e envia para o 
        worker que está rodando no ambiente '.Net', e ele faz esse envio do voto que foi feito para um banco de dados.

        Esse banco de dados exibe o resultado em uma aplicação 'node', ou seja, conseguimos ter um resultado bem legal combinando esses múltiplos serviços em diferentes plataformas, e o Docker, o
        Swarm em conjunto com o compose vai nos ajudar nisso.

        E como podemos alcançar esse resultado? Vamos fazer o seguinte, vamos no arquivo 'docker-compose.yml', no nosso 'Visual Studio Code', o que faremos? Precisamos tornar esse cara mais 
        próximo do Swarm, com a versão três que veio, além dessas instruções clássicas que conhecemos, vem um cara novo chamado 'deploy', e dentro dele podemos definir diversas coisas, como por
        exemplo, eu posso definir um número de réplicas que eu quero para as tarefas que vão ser geradas por esse serviço.

        Então por exemplo, eu quero que tenha uma réplica do registro funcionando, e posso definir também a política de restart, como por exemplo, qual é a condição para que esse cara reinicie?
        A condição para ele reiniciar é caso ele tenha falahado em algum momento, assim por diante, eu posso definindo cada um dos meus serviços, além de toda a imagem e volume que eu definir
        aqui.

            redis:
              image: redis:alpine
              networks:
                - frontend
              deploy:
                replicas: 1
                restart_policy:
                  condition: on-failure

        O que mais podemos fazer? No nosso serviço de banco de dados por exemplo, o db, podemos definir onde vamos colocar o serviço, qual é a minha restrição para esse serviço? Quero vir aqui
        e definir que ele só vai poder rodar em nós que sejam manager, que tenha esse papel dentro do nosso Swarm:

            db:
              image: postgres:9.4
              volumes:
                - db-data:/var/lib/postgresql/data
              networks:
                - backend
              deploy:
                placement:
                  constraints: [node.role == manager]
              environment:
                POSTGRES_HOST_AUTH_METHOD: trust

        No nosso serviço de votação, ele está rodando em Python, eu quero que ele rode em um nó qualquer, não preciso explicitar uma restrição para ele, então ele pode rodar tanto em um
        manager, quanto em um worker por exemplo, mas também podemos colocar no deploy qual vai ser o número de réplicas dele.

        Podemor definir como ele vai estar lidando com toda a votação, podem ter várias pessoas votando ao mesmo tempo, eu quero definir que o número de réplicas dele vão ser duas, para que
        o Docker, o Swarm através do algoritmo 'Round Robin', veja que está recebendo alguma informação e defina qual é o melhor nó para receber essa tarefa, então ele vai definir qual das 
        réplicas vai ser.

            vote:
              image: dockersamples/examplevotingapp_vote:before
              ports:
                - 5000:80
              networks:
                - frontend
              depends_on:
                - redis
              deploy:
                replicas: 2
                restart_policy:
                  condition: on-failure

            Então eu quero que tenha duas réplicas para caso tenha múltiplas votações ao mesmo tempo, consigamos lidar com isso.
            Também definimos a nossa política de restart, que vai ser em qual condição? Na condição de falha.

        E no nosso 'result'? Que é uma aplicação que está rodando em 'node', podemos também definir configurações de deploy, não é verdade? Quais outras coisas que podemos fazer, além do
        número de réplicas, que podemos colocar 1? Podemos definir mais uma vez a nossa política de restart, então eu quero que esse cara na condição de falha reinicie.

            result:
              image: dockersamples/examplevotingapp_result:before
              ports:
                - 5001:80
              networks:
                - backend
              depends_on:
                - db
              deploy:
                replicas: 1
                restart_policy:
                  condition: on-failure

        Agora vamos ver um diferente, vamos no nosso worker, o cara .Net, o que queremos que ele faça? Nós já sabemos que o papel dele é fazer a comunicação entre o voto que foi recebido e o
        envio para o banco de dados, então queremos que esse cara faça o seguinte: Podemos dizer que esse cara, por padrão já é replicado, vimos isso antes quando criamos um serviço na outra
        aula.

        Mas podemos explicitar também, podemos colocar só para mostrar as possibilidades, colocar ele como 'replicated' ou como 'global', vamos deixar ele replicado, mas, como já dito, explicito.

        Queremos que rode com apenas uma réplica, e podemos fazer o que nele? Podemos definir uma label, podemos definir uma etiqueta para ele, podemos fazer isso como? Colocamos entre 
        colchetes, chamamos esse cara de "VOTING", "[APP=VOTING]" no caso vai ser a etiqueta desse nosso serviço.

        E podemos mais uma vez definir a política de reinício e a política mais uma vez vai ser em caso de falha, mais do que isso, ainda por cima, eu quero colocar que este serviço, como ele
        faz o papel de "trabalho duro" e ele já está meio que explicitando para nós o nome do serviço que é worker, eu quero que esse cara só rode em nós que tenham o papel de "worker's", 
        [node.role == worker].

            worker:
              image: dockersamples/examplevotingapp_worker
              networks:
                - frontend
                - backend
              depends_on:
                - db
                - redis
              deploy:
                mode: replicated
                replicas: 1
                labels: [APP=VOTING]
                restart_policy:
                  condition: on-failure
                placement:
                  constraints: [node.role == worker]

        E para finalizarmos, o nosso último serviço, que é o 'visualizer', que é aquele que mostra o status todo do que está acontecendo para nós, como esse cara tem um papel mais administrativo, 
        no fim das contas, eu quero que esse cara aqui só rode no meu manager, e isso aqui não é nada de novo, vamos colocar "deploy:", "placement:", e no fim das contas, colocamos o nosso       
        "constraints" e o meu constraint vai ser o quê? "[node.role==manager]".

            visualizer:
              image: dockersamples/visualizer:stable
              ports:
                - 8080:8080
              stop_grace_period: 1m30s
              volumes:
                - "/var/run/docker.sock:/var/run/docker.sock"
              deploy:
                placement:
                  constraints: [node.role == manager]

        Então agora que definimos o nosso 'docker-compose', vamos ver como utilizamos o Docker Swarm, vamos fazer o deploy desses múltiplos serviços.

    
    Subindo a stack:
        
        Atenção! Houve uma mudança em todo o repositório do Postgres no Docker Hub. Para o devido funcionamento, devemos utilizar uma variável de ambiente para ignorar o uso de senhas.
        Ficando no seguinte formato:

            Obs: Se a alteração já foi feita na outra aula, não é mais necessário realiza-la.

        Definimos o nosso arquivo, agora precisamos copiar esse conteúdo aqui de algum modo para dentro de algum nó manager, porque isso aqui está na nossa máquina física, como é que eu posso
        copiar o conteúdo desse arquivo aqui e mover ele para dentro de alguma máquina virtual? Nesse caso, vamos aprender um pequeno truque de Linux para que nos ajude bastante.

        Off Topic -------

            Como é que dentro dessa máquina virtual Linux podemos criar um arquivo utilizando o menor ferramental possível, sem abrir nenhum tipo de editor de texto de terminal? Podemos fazer
            o seguinte, tem um cara bem famoso que se chama "cat", ele permite que leiamos e escrevamos arquivos...

            E para criarmo um arquivo, vamos aprender o truque, que basta colocar "cat >" e o nome do arquivo que queremos criar, que no caso é "docker-compose.yml", se dermos um 'Enter' repare,
            que ele faz uma quebra de linha e assim, basta colocar o conteúdo dentro deste arquivo clicando com o botão direito após ter copiado...

                $ cat > docker-compose.yml

                    version: "3"
                    services:
                    
                    redis:
                      image: redis:alpine
                      networks:
                        - frontend
                      deploy:
                        replicas: 1
                        restart_policy:
                          condition: on-failure
                            
                    db:
                      image: postgres:9.4
                      volumes:
                        - db-data:/var/lib/postgresql/data
                      networks:
                        - backend
                      deploy:
                        placement:
                          constraints: [node.role == manager]
                      environment:
                        POSTGRES_HOST_AUTH_METHOD: trust
                    
                    vote:
                      image: dockersamples/examplevotingapp_vote:before
                      ports:
                        - 5000:80
                      networks:
                        - frontend
                      depends_on:
                        - redis
                      deploy:
                        replicas: 2
                        restart_policy:
                          condition: on-failure
                    
                    result:
                      image: dockersamples/examplevotingapp_result:before
                      ports:
                        - 5001:80
                      networks:
                        - backend
                      depends_on:
                        - db
                      deploy:
                        replicas: 1
                        restart_policy:
                          condition: on-failure
                    
                    worker:
                        image: dockersamples/examplevotingapp_worker
                        networks:
                        - frontend
                        - backend
                        depends_on:
                        - db
                        - redis
                        deploy:
                        mode: replicated
                        replicas: 1
                        labels: [APP=VOTING]
                        restart_policy:
                            condition: on-failure
                        placement:
                            constraints: [node.role == worker]
                    
                    visualizer:
                      image: dockersamples/visualizer:stable
                      ports:
                        - 8080:8080
                      stop_grace_period: 1m30s
                      volumes:
                        - "/var/run/docker.sock:/var/run/docker.sock"
                      deploy:
                        placement:
                          constraints: [node.role == manager]
                    
                    networks:
                    frontend:
                    backend:
                    
                    volumes:
                    db-data:

            Obs: Caso esteja utilizando o prompt de comando do Windows e não esteja conseguindo colar o conteúdo, vá até propriedades, opções, e é só marcar a opção "Edição rápida", todas essas
            opções do prompt vão permitir que consigamos copiar e colar dentro do prompt de comando.

            Por fim, para sair do arquivo basta utilizar o "Ctrl + D"
        
        -------

        Agora se utilizarmos o comando "cat docker-compose.yml" ele vai listar o nosso arquivo, então o arquivo realmente foi criado, para fazer a contra prova, basta utilizar o "ls", e veremos
        o nosso arquivo.

        Mas o que importa é que agora vamos precisar fazer o deploy desse arquivo e gerar os nossos serviços, como vamos fazer isso? Temos o comando do Docker e faremos o deploy de uma pilha de
        arquivos.

        O nosso comando será o "docker stack deploy", ele faz o deploy de uma pilha de arquivos, e para esse cara podemos informar o seguinte: 
        "Faça o deploy dessa pilha utilizando um '--compose-file'", um arquivo de composição, e qual o nome do nosso arquivo? É "docker-compose.yml".

        Se dermos um 'Enter' agora, ele informará que está faltando um parâmetro, qual é esse último parâmetro? É o nome da stack que queremos criar, para isso vamos colocar o nome "vote", no
        momento que dermos um 'Enter', reparem ele vai começar a criar cada um daqueles atributos que estão declarados dentro do nosso arquivo, as nossas redes, a front-end, a backend, a rede
        padrão, a dbdata que é o nosso banco de dados, o nosso volume, os nossos serviços, cada um deles...

            $ docker stack deploy --compose-file docker-compose.yml "Nome da Stack"

            Saída:

                Creating network vote_backend
                Creating network vote_frontend
                Creating network vote_default
                Creating service vote_visualizer
                Creating service vote_redis
                Creating service vote_db
                Creating service vote_vote
                Creating service vote_result
                Creating service vote_worker

        Ele vai começar a criar cada uma separadamente e logo depois quando dermos um "docker service ls", ele vai estar listando tudo perfeitamente...

            ID                  NAME                MODE                REPLICAS            IMAGE                                          PORTS
            dq7po174szg9        vote_db             replicated          0/1                 postgres:9.4
            mtn063vnituq        vote_redis          replicated          1/1                 redis:alpine
            oz8zlqih5uqs        vote_result         replicated          0/1                 dockersamples/examplevotingapp_result:before   *:5001->80/tcp
            7xhf1gfq3k4c        vote_visualizer     replicated          0/1                 dockersamples/visualizer:stable                *:8080->8080/tcp
            4rlp5akzxbaf        vote_vote           replicated          0/2                 dockersamples/examplevotingapp_vote:before     *:5000->80/tcp
            hai71l8rwkfn        vote_worker         replicated          0/1                 dockersamples/examplevotingapp_worker:latest

        Cada um dos serviços criados para nós, e nesse meio tempo, se voltarmos no navegador, repare que nosso visualizer não está mostrando mais nenhum container, porque removemos antes de 
        fazer esta aula..."192.168.99.XXX:8080"...Temos que esperar um tempo...

        E agora quando ele termina, só falta mais um, se dermos um "docker stack ls", poderemos visualizar as nossas stacks, tem uma stack que se chama "vote" com 6 seviços e com o 
        orquestrador, do Swarm.

            NAME                SERVICES            ORCHESTRATOR
            vote                6                   Swarm
        
        Se dermos um "docker service ls" o que acontece? Vamos ver que tem todos esses serviços...

            ID                  NAME                MODE                REPLICAS            IMAGE                                          PORTS
            dq7po174szg9        vote_db             replicated          1/1                 postgres:9.4
            mtn063vnituq        vote_redis          replicated          1/1                 redis:alpine
            oz8zlqih5uqs        vote_result         replicated          1/1                 dockersamples/examplevotingapp_result:before   *:5001->80/tcp
            7xhf1gfq3k4c        vote_visualizer     replicated          1/1                 dockersamples/visualizer:stable                *:8080->8080/tcp
            4rlp5akzxbaf        vote_vote           replicated          2/2                 dockersamples/examplevotingapp_vote:before     *:5000->80/tcp
            hai71l8rwkfn        vote_worker         replicated          1/1                 dockersamples/examplevotingapp_worker:latest

        Agora vamos fazer o seguinte, vamos dar um "docker service ls --format" e vamos formatar ele utilizando lá uma técnica bem legal...Queremos forma ele como? Queremos visualizar quais
        campos? Queremos visualizar o "{{.Name}}" e queremos visualizar também o número de réplicas "{{.Replicas}}"

            $ docker service ls --format "{{.Name}} {{.Replicas}}"

                
                Saída:

                    vote_db 1/1
                    vote_redis 1/1
                    vote_result 1/1
                    vote_visualizer 1/1
                    vote_vote 2/2
                    vote_worker 1/1

        Se ele ainda não tiver todas as réplicas que solicitamos significa que ele ainda está baixando as nossas imagens, e ainda não subiu os serviços da forma que deveria, ou ele ainda está
        alocando todas as tarefas da maneira como deveriam, então se ficarmos dando aqui esse comando várias vezes, vamos ver que ele está aos poucos, subindo cada uma dessas tarefas.

        Mas quais que já subiram? Por exemplo o visualizer ainda não subiu, o result já subiu, então significa que se viermos aqui no 192.168.99.112 ou 107, 108 que são os IP's dos nossos nós
        que estão dentro desse Swarm, ou seja, conseguimos acessar com aquela mesma ideia do Routing Mesh qualquer um dos serviços.

        Temos que aguardar o Swarm subir os serviços aos poucos...

        Por fim, terminando de subir todos os serviços da Stack, se formos inspecionando um a um por exemplo, "docker service ps vote_db"...

            $ docker service ps vote_db

            ID                  NAME                IMAGE               NODE                        DESIRED STATE       CURRENT STATE             ERROR               PORTS
            vuljefjktm1m        vote_db.1           postgres:9.4        vm2                         Running             Running 55 minutes ago
            su2tnrmy2edj         \_ vote_db.1       postgres:9.4        q5duspz3ugglfw664ln3xr18n   Shutdown            Orphaned 24 minutes ago

            Ele está mostrando que na vm2 está rodando o nosso servilo com esse nome, essa tarefa e tudo perfeitamente, então ele vai ter respeitado todos as nossas restrições que impomos,
            todas as condições de restart.

        Se formos no nosso navegador, ele está exibindo tudo perfeitamente, se repararmos inclusive, que talvez, nenhuma tarefa, nenhum serviço foi alocado para determinado nó, então o Swarm
        definiu que para ele é melhor essa configuração atualmente, podemos passear agora pela aplicação na verdade.

        Podemos entrar na parte da votação que está na porta 5000, que se viermos aqui estamos vendo que o binding de portas está sendo feito na porta 5001 para de resultado, e na 5000 para
        a de votação, além disso 8080 foi reservada para a de visualizer.
        
        Então agora já temos a nossa aplicação feita, múltiplos serviços estão interagindo, e a parte mais legal, se formos na nossa vm1 e dermos um "$ docker service ps vote_vote" que é o
        nosso serviço de votação vamos ver que ele tentou subir a primeira vez, subiu as duas réplicas na verdade no fim das contas, uma na vm2 e uma na vm3.

        E o nosso serviço de resultado está rodando aqui na nossa vm2, então significa que se viermos na nossa vm2 o que acontece? 

            $ docker-machine ssh vm2

            $ docker container ls

            CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
            c3a47f8ef617        postgres:9.4        "docker-entrypoint.s…"   38 seconds ago      Up 31 seconds       5432/tcp            vote_db.1.yt26rn03k2mjums2em2kwekg5

            Está aqui o nosso container, e agora o momento mais belo desse curso, vamos dar um "docker container rm c3a47f8ef617" que é o ID, com "--force", vamos vir aqui, ver que esse é o
            nosso serviço de votação, esse aqui é o nosso serviço de resultado, e eu vou remover esse cara.

            Se dermos um F5 no monitoramento (http://192.168.99.XXX:8080/), poderemos ver que o container saiu do ar, mas todos os outros nossos serviços continuam funcionando normalmente,
            enquanto esse cara vai voltar a ser executado, se dermos um "docker service ps" de novo, o que acontece? Ele foi realocado novamente na vm2, mas nesse tempo aqui o serviço estava
            reiniciando, isso não comprometeu em nenhum momento o resto da nossa aplicação, porque os serviços são independentes.

            Vamos finalizar por aqui, vimos como podemos criar as nossas stack's agora, os nosso múltiplos serviços, só para finalizar aqui com chave de ouro, o último comando que vamos ver,
            caso quisessemos remover toda essa nossa stack de uma só vez, temos também o comando "docker stack rm" e com isso, podemos passar a stack que eu quero remover, de 'vote'...
            
                $ docker stack rm vote

                Removing service vote_db
                Removing service vote_redis
                Removing service vote_result
                Removing service vote_visualizer
                Removing service vote_vote
                Removing service vote_worker
                Removing network vote_default
                Removing network vote_frontend
                Removing network vote_backend

            se dermos um 'Enter', aqui, ele vai remover todos os serviços, e se dermos um "docker service ls" não tem mais nenhum serviço rodando...

                $ docker service ls
                
                ID                  NAME                MODE                REPLICAS            IMAGE               PORTS

            Com esse comando finalizamos...

        - Para saber mais...

            Por padrão, tanto o Docker no modo standalone quanto o Docker Swarm, partilham apenas de um driver local para uso de volumes. Isso quer dizer que o Docker Swarm não possui, até 
            então, solução nativa para distribuir volumes entre os nós.

            Então, no exemplo do vídeo anterior, ao definirmos o volume para cada serviço, criamos um volume local dentro de cada nó que for executar a tarefa. Logo, os volumes não são 
            compartilhados entre os diferentes nós do cluster.
            
            Existem soluções que não são nativas do Docker Swarm para utilizar volumes distribuídos entre nós, que podem ser consultadas na Docker Store 
            (https://hub.docker.com/search?category=volume&q=&type=plugin).

        
        - Nesta aula, aprendemos:

            - O docker-compose.yml também pode ser utilizado para serviços de um swarm
            - Novas instruções a partir da versão 3, como deploy e suas instruções internas
            - Uma stack é uma pilha de serviços trabalhando em conjunto
            - Como criar e remover nossa própria stack utilizando o comando docker stack


    Questões aula 07:

        1 - Você assumiu um projeto que usa Docker Swarm. No arquivo de configuração tem o seguinte trecho:

            deploy:
                mode: replicated
                replicas: 4
                restart_policy:
                    condition: on-failure
                placement:
                    constraints: [node.role == manager]

            O que podemos afirmar sobre o trecho acima?
        
            Selecione 2 alternativas

            R1: Garantirá que teremos exatamente 4 réplicas desse serviço.
                Alternativa correta! Isso será possível graças ao trecho replicas: 4.

            R2: Em caso de falhas o serviço será reiniciado.
                Alternativa correta! Isso foi definido no trecho de restart_policy.

        
        2 - Qual a finalidade do comando docker stack deploy?

            Selecione uma alternativa

            R: Utilizar um arquivo de composição e subir uma pilha de serviços simultaneamente.
               Alternativa correta! Podemos inclusive utilizar as instruções do docker-compose.

    
    Conclusão:

        Vimos qual é a real motivação do Swarm, porque nós utilizamos ele. Vimos que se nós fossemos subir diversos containers em uma única máquina, poderíamos ter diversos tipos de problemas.

        Como por exemplo, a máquina não ter recursos suficientes, a máquina recebia muitas requisições, e assim por diante, ou também o caso de um container falhar e nós não conseguirmos fazer
        isso, de fazer um container voltar a funcionar de maneira automática.

        E o Swarm provê todas as soluções para nós que fomos aplicando no decorrer do curso, vimos como inicializamos o nosso primeiro Swarm com o "docker swarm init", viu a boa prática de
        manter um IP estático para que consifamos sempre manter a comunicação ali do nosso manager, do nosso líder com os outros que vão entrar.

        E a partir daí vimos a diferença entre nós managers, que são responsáveis, majoritariamente pela orquestração e administração do Swarm e ao mesmo tempo eles são workers também, mas a 
        ideia é que restringimos esse serviço que queremos utilizar nesse tipo de nó. E temos os tipos workers que são os reais trabalhadores, que executam quaisquer serviços que vamos alocar.

        Logo depois disso, seguimos a nossa caminhada e vimos como poderíamos agora restringir serviços de serem executados em determinados tipos de nós, seja pelo papel dele, seja pelo 
        hostname, seja pelo IP que também podemos fazer.

        E depois, vimos como poderíamos restringir nossos nós também, quando queriamos que um nó não executasse determinado serviço, ou que ele não possa executar serviço nenhum, vimos todos 
        esses aspectos.

        E seguindo essa lógica, começamos a ver como é feita a comunicação ali por debaixo dos panos entre os diferentes nós de um Swarm e como é que essa rede é utilizada, que rede é essa, 
        e vimos que é um tal de ingress utilizando lá o driver overlay no escopo do Swarm.

        Como ele, podemos criar a nossa própria rede, e vimos que o benefício  disso é poder fazer a comunicação entre diferentes serviços,  emtre doferemtes containers gerados à partir de 
        apenas do hostname destes containers, à partir do nome dele, e não mais só do IP, conseguimos agora descobrir um serviço utilizando só um nome, e isso é bem interessante, porque está 
        utilizando o conceito de 'Service Discovery'.

        E depois avançando os nossos estudos, vimos também como poderíamos fazer o deploy de múltiplos serviços utilizando um "docker-compose" da mesma maneira que fazíamos lá com o
        "docker-compose" no Stand Alone, do Docker trabalhando em um único host, com diversos containers.

        Aplicamos a mesma ideia para diversos serviços agora, utilziando à partir da versão 3 do Docker-compose em que tínhamos diversas novas instruções para utilizar a mais próxima ao Swarm,
        ter políticas de restart, restrição de nós e assim por diante, e para fazer esse deploy, utilizamos o 'docker stack deploy' que é a meneira utilizada de fazermos o deploy, criar uma
        pilha de serviços.

        E chegando de fato ao final, antes da nossa despedida pra valer, é sempre bom lembrar que todo o conhecimento desse curso pode ser tirado à partir da documentação.

        E caso você ainda tenha mais alguma questão a ser solucionada, que você queira tirar alguma dúvida que você acha que não foi abordada nesse curso, ou que você pense:
        "Quero me aprofundar em determinado tópico...", ele com certeza vai ter a sua resposta, aqui a documentação ... docs.docker.com/swarm/, e podemos ver todos os aspectos que queiramos 
        consultar.