Aula 01: Conhecendo ReplicaSets e Deployments -------

    Conhecendo ReplicaSets:

        Já vimos que o Kubernetes consegue deinifir onde um Pod vai ser executado através do 'Scheduler'. Ele também cria um novo Pod do zero, caso o principal pare, um Pod
        iêntico, mas não o mesmo.

        Isso porque os Pods são efêmeros, então, eles devem estar prontos para serem substituídos por questão de falha ou de atualização, sem causar grande impacto na nossa 
        aplicação, como um todo.

        Mas, como o Kubernetes vai conseguir fazer isso? Para isso, temos recursos, como nós já vimos. E esses dois recursos vão nos ajudar a resolver este tipo de problema: 
        os 'ReplicaSets' e os 'Deployments'.

        Criamos um arquivo para de ReplicaSet para o portal-noticias...

            portal-noticias-replicaset.yaml

            apiVersion: apps/v1
            kind: ReplicaSet
            metadata:
            name: portal-noticias-replicaset
                spec:
                    template:
                        metadata:
                            name: portal-noticias
                            labels:
                                app: portal-noticias
                        spec:
                            containers:
                                - name: portal-noticias-container
                                image: aluracursos/portal-noticias:1
                                ports:
                                    - containerPort: 80
                                envFrom:
                                    - configMapRef:
                                        name: portal-configmap
                    replicas: 3
                    selector:
                        matchLabels:
                            app: portal-noticias

        
        Se deletarmos um Pod, por si só não subirá nenhum novo, mas agora como nosso portal-noticias-replicaset, teremos uma divergência quando um Pod for
        deletado, pois ali em réplicas estamos explicitando nosso número de réplicas do Pod desejadas, sendo assim o número de réplicas desejadas e ativas
        serão diferentes. Quando acontecer isso o recurso ReplicaSet irá igualar a quantidade de pods ativos com o que desejamos.

        Obs: Se não definirmos nenhum número ali, ele vai ser 1 por padrão.

        Mesmo definindo todas as características do nosso Pod ali, ainda temos que dizer para o 'Kubernetes' que template o 'ReplicaSet' deve gerenciar, para
        isso, utilizaremos um 'Selector' com uma 'tag' chamada 'matchLabels', quais recursos que tem esta 'Label'.

        Obs: Assim como já vimos antes ele vai verificar a label dos recursos, como nosso recurso 'portal-noticias' já tem a label:

            app: portal-noticias

            ...já está tudo ok.

        Se executarmos um 'kubectl get pods' veremos que agora temos 3 réplicas do recurso, mudando somente o identificador no final...

            NAME                               READY   STATUS    RESTARTS   AGE
            db-noticias                        1/1     Running   0          155m
            portal-noticias-replicaset-bnq64   1/1     Running   0          154m
            portal-noticias-replicaset-hpnlf   1/1     Running   0          154m
            portal-noticias-replicaset-pbqf7   1/1     Running   0          154m
            sistema-noticias                   1/1     Running   0          154m
        
        Todas as réplicas utilizaram as mesmas variáveis, são idênticas.

        Se quiser testar podemos abrir dois terminais, em um deles executamos o comando:

            $ kubectl get rs ou $ kubectl get replicasets

            Utilizando o flag --watch para que assistamos o que acontece...

        No outro terminal podemos deletar um dos pods com '$ kubectl delete pod portal-noticias-replicaset-bnq64'.
        Se olharmos os detalhes do que aconteceu...

            NAME                         DESIRED   CURRENT   READY   AGE
            portal-noticias-replicaset   3         3         3       160m
            portal-noticias-replicaset   3         2         2       160m
            portal-noticias-replicaset   3         3         2       160m
            portal-noticias-replicaset   3         3         3       160m
        
        Num primeiro momento estava tudo ok, entre réplicas desejadas, que existiam e não estavam rodando e, por fim, as prontas. Logo em seguida perdemos
        uma réplica, o que fez com que o saldo das que existiam e prontas decaísse em 1. Depois o recurso ReplicaSet criou um novo pod e o deixou pronto, em
        execução.

        Ao executarmos mais uma vez o 'kubectl get pods', veremos que foi criado um novo pod com o identificador diferente 'portal-noticias-replicaset-9mvkd'.

        Agora temos um load balancer praticamente funcionando da forma que tem que ser, replicas de um mesmo pod que são acessadas por uma mesma chave,
        'portal-noticias', e tendo seu trafego alternado pelo servidor web, neste caso, é claro, que é o 'nginx'.

    
    Conhecendo Deployments:

        Nós vimos que um ReplicaSet nada mais é do que esse conjunto de réplicas que permite a criação, de maneira automática, em caso de falhas de um Pod, 
        dentro de um cluster gerenciado por um ReplicaSet.

        Um Deployment nada mais é do que uma camada acima de um ReplicaSet. Então, quando nós definimos um Deployment, nós estamos, automaticamente, 
        definindo um ReplicaSet, sem nenhum mistério.

        Para demonstrar criamos um novo arquivo chamado 'nginx-deployment.yaml' com o seguinte conteúdo...

            apiVersion: apps/v1
            kind: Deployment
            metadata:
                name: nginx-deployment
            spec:
                replicas: 3
                template:
                    metadata:
                        name: nginx-pod
                        labels:
                            app: nginx-pod
                    spec:
                        containers:
                            - name: nginx-container
                              image: nginx:stable
                              ports:
                                - containerPort: 80
                selector:
                    matchLabels:
                        app: nginx-pod

            Ele é bem parecido com o ReplicaSet, com excessão do 'kind' que agora é 'deployment'. O 'apiVersion' também muda.

        Para aplicá-lo, basta realizar o mesmo comando que viemos utilizando:

            $ kubectl apply -f nginx-deployment.yaml

            Para verificar os novos pods criados:

                $ kubectl get pods

                NAME                                READY   STATUS    RESTARTS   AGE
                db-noticias                         1/1     Running   0          3h18m
                nginx-deployment-58d6df6b6b-lkkfm   1/1     Running   0          22m
                nginx-deployment-58d6df6b6b-t4wz8   1/1     Running   0          22m
                nginx-deployment-58d6df6b6b-v62zz   1/1     Running   0          22m
                portal-noticias-replicaset-9mvkd    1/1     Running   0          36m
                portal-noticias-replicaset-hpnlf    1/1     Running   0          3h17m
                portal-noticias-replicaset-pbqf7    1/1     Running   0          3h17m
                sistema-noticias                    1/1     Running   0          3h17m

            Para verificar que criamos mais um ReplicaSet com este nosso deployment...

                $ kubectl get rs

                NAME                          DESIRED   CURRENT   READY   AGE
                nginx-deployment-58d6df6b6b   3         3         3       23m
                portal-noticias-replicaset    3         3         3       3h18m

            Ou seja, nosso 'Deployment' nada mais é que mais uma camada acima do 'ReplicaSet'

            Para ver isso podemos realizar um '$ kubectl get deployments'

                NAME               READY   UP-TO-DATE   AVAILABLE   AGE
                nginx-deployment   3/3     3            3           25m

        
            O que vai mudar na prática agora? A grande vantagem do uso de Deployments é que, assim como temos um git, por exemplo, para o nosso controle 
            de versionamento de código, nós temos os Deployments em Kubernetes, que permitem o nosso controle de versionamento das nossas imagens e Pods, 
            olha que legal.

            Agora com o comando 'kubectl rollout history deployment nginx-deployment' vemos nossas revisões.

            Se mudarmos da versão 'stable' para 'latest' no código e executarmos o comando:

                $ kubectl apply -f nginx-deployment.yaml --record

        Se utilizarmos o comando:

            $ kubectl rollout history deployment nginx-deployment

            Veremos a label ou descrição do que ele executou...

                REVISION  CHANGE-CAUSE
                1         Versão stable dos nossos pods nginx
                2         kubectl apply --filename=nginx-deployment.yaml --record=true

            Mas queremos deixar a descrição da revisão 2 tão clara como está a 1.
            Então utilizamos o comando:

                $ kubectl annotate deployment nginx-deployment kubernetes.io/change-cause=""

                E colocamos entre "" qual descrição queremos para esta última revisão de 'deploy'...

            $ kubectl annotate deployment nginx-deployment kubernetes.io/change-cause="Versão latest dos nossos pods nginx"

            Utilizamos mais uma vez o comando:

                $ kubectl rollout history deployment nginx-deployment

                Saída:

                deployment.apps/nginx-deployment 
                REVISION  CHANGE-CAUSE
                1         Versão stable dos nossos pods nginx
                2         Versão latest dos nossos pods nginx

        Mais uma vez mudei no arquivo e voltei para a versão stable...

            $ kubectl rollout history deployment nginx-deployment

            deployment.apps/nginx-deployment 
            REVISION  CHANGE-CAUSE
            2         Versão latest dos nossos pods nginx
            3         Voltando para a versão stable

        Para verificar que realmente voltou para a versão stable podemos realizar um....

            $ kubectl describe pod nginx-deployment-58d6df6b6b-58dx4 
            
            Poderemos ver:

                Normal  Pulled     2m42s  kubelet            Container image "nginx:stable" already present on machine

        Podemos fazer roolback do nosso deployment, voltando para a versão latest com o comando...

            $ kubectl rollout undo deployment "nome do deployment" - que no nosso caso é 'nginx-deployment'
        
            deployment.apps/nginx-deployment 
            REVISION  CHANGE-CAUSE
            3         Voltando para a versão stable
            4         Versão latest dos nossos pods nginx

            Poderíamos ter específicado a revisão para a qual queriamos voltar com a flag --to-revision=2

            $ kubectl rollout undo deployment "nome do deployment" --to-revision=2

        Se utilizarmos os comandos 'kubectl get pods' e depois 'kubectl describe pod 'nome do pod'' poderemos ver que voltamos para a versão latest.
        
        Então, no fim das contas, a boa prática, o mais comum que vocês irão ver quando vocês forem criar Pods é criar eles através de Deployments, que eles 
        já vão permitir todo esse controle de versionamento e também os benefícios de um ReplicaSet.

        Nada impede de criarmos manualmente, como viemos fazendo com Pods e ReplicaSets, mas, o mais comum, a boa prática, é fazer a criação através de 
        Deployments, por conta desses benefícios de controle de versionamento e de estabilidade, disponibilidade da nossa aplicação.


    Aplicando Deployments ao projeto:

        Vantagens que podemos citar em criar deployments para nossos POD's, seriam, a questão do reinício automático em caso de falhas e também teremos
        a questão do controle de versionamento, para que tudo fique pradonizado sem nenhum problema, utilizando os melhores recursos possíveis para o nosso
        momento.

        Vamos aplicar ao projeto todo os deployments e deletar os deployments de teste que fizemos na última aula:

        Vamos ver os deployments que temos ativos no momento:

            $ kubectl get pods

        Para aplicar este conceito de deployment ao resto do projeto, primeiro deletamos os pods 'db-noticias' e 'sistema-noticias' com o comando...

            $ kubectl delete deployment nginx-deployment

            Obs: Podemos deletar utilizando o arquivo também...

            $ kubectl delete -f portal-noticias-replicaset.yaml

            Ctrl + L -> Para limpar o terminal...

        É muito simples mudar do ReplicaSet para o Deployment...A seguir temos o arquivo de ReplicaSet que criamos...


        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
            name: portal-noticias-replicaset
        spec:
            template:
                metadata:
                    name: portal-noticias
                    labels:
                        app: portal-noticias
                    spec:
                        containers:
                            - name: portal-noticias-container
                              image: aluracursos/portal-noticias:1
                              ports:
                                - containerPort: 80
                              envFrom:
                                configMapRef:
                                  name: portal-configmap
            replicas: 3
            selector:
                matchLabels:
                    app: portal-noticias


        Vamos pegar este conteúdo e colocar dentro de um novo arquivo chamado 'portal-noticias-deployment.yaml'...
        Primeiro trocamos a declaração do 'kind' para 'Deployment' ao invés de deixarmos 'ReplicaSet', assim como no name...

        apiVersion: apps/v1
        kind: Deployment
        metadata:
            name: portal-noticias-deployment

        Simples assim!

        Agora basta utilizarmos o comando:

            $ kubectl apply -f portal-noticias-deployment

            E pronto! Ele foi criado...

        Para verificarmos a nossa revisão veremos...

            deployment.apps/portal-noticias-deployment 
            REVISION  CHANGE-CAUSE
            1         <none>

        Para adicionarmos uma anotação à nossa revisão, realizamos o comando com a seguinte mensagem de exemplo...

            $ kubectl annotate deployment portal-noticias-deployment kubernetes.io/change-cause "Criando portal de notícias na versão 1 dia 09/07/2021"

        Se voltarmos o nosso navegador tudo vai estar funcionando corretamente...

            localhost:30000 o portal-noticias

        Vamos fazer isso com o sistema-noticias também e com o db-noticias...

        sistema-noticias...

        apiVersion: v1
        kind: Pod
        metadata:
          name: sistema-noticias
          labels:
            app: sistema-noticias
        spec:
          containers:
            - name: sistema-noticias-container
              image: aluracursos/sistema-noticias:1
              ports:
                - containerPort: 80
              envFrom:
                - configMapRef:
                    name: sistema-configmap

        Para transforma-lo em Deployment vamos fazer o caminho direto, sem antes criar o 'ReplicaSet', pois para o banco de dados e para o sistema noticias
        teremos apenas um POD.

        apiVersion: v1
        kind: Deployment # Trocamos o kind para Deployment
        metadata:
          name: sistema-noticias-deployment # o Metadata também acrescentamos o indicador que é um Deployment e logo abaixo tiramos o Label
        spec: # Primeiro as especificações do Deployment
            # replicas: Deixaremos vazio pois queremos apenas um pod do sistema-noticias, e para isso não precisamos explicitar
            template: # Template que utilizaremos no Deployment
                metadata:
                    name: sistema-noticias # O nome do nosso template
                    labels:
                        app: sistema-noticias # Chave valor de mapeamento que ele irá utilizar
                spec: # Agora as especificações do Template, que são basicamente as especificações do POD que este Deployment cuidará
                    containers:
                        - name: sistema-noticias-container
                            image: aluracursos/sistema-noticias:1
                            ports:
                                - containerPort: 80
                            envFrom:
                                - configMapRef:
                                    name: sistema-configmap
            selector: # Esta de fato é uma parte que adicionamos, com matchLabels e a nossa chave e valor
                matchLabels:
                    app: sistema-noticias


        db-noticias...

        apiVersion: v1
        kind: Pod
        metadata:
          name: db-noticias
          labels:
            app: db-noticias
        spec:
          containers:
            - name: db-noticias-container
              image: aluracursos/mysql-db:1
              ports:
                - containerPort: 3306
              envFrom:
                - configMapRef:
                    name: db-configmap
    
        Mesmo processo de antes para transforma-lo em Deployment...

        apiVersion: apps/v1
        kind: Deployment # Mudamos o kind
        metadata:
          name: db-noticias-deployment # Mudamos o nome no metadata indicando que agora é um Deployment
        spec:
          #replicas: 1 //Como exemplo, não definimos réplicas para fazer o teste se ele vai criar apenas um Pod mesmo
          template:
            metadata:
              name: db-noticias
              labels:
                app: db-noticias
            spec:
              containers:
                - name: db-noticias-container
                  image: aluracursos/mysql-db:1
                  ports:
                    - containerPort: 3306
                  envFrom:
                    - configMapRef:
                        name: db-configmap
          selector: # A última mudança é este selector, onde diremos para o Deployment que ele vai utilizar o db-noticias que já tinhamos criado
            matchLabels:
              app: db-noticias

        Deletamos o nossos PODs 'sistema-noticias' e 'db-noticias'

            $ kubectl delete pod db-noticias sistema-noticias

        Agora recriamos ele utilizando o 'Deployment'...

            $ kubectl apply -f sistema-noticias-deployment.yaml

            $ kubectl apply -f db-noticias-deployment.yaml

        Podemos verificar a revisão destes 'Deployments' utilizando novamente...

            $ kubectl rollout history deployment sistema-noticias-deployment

                deployment.apps/sistema-noticias-deployment 
                REVISION  CHANGE-CAUSE
                1         Subindo o sistema na versão 1

                Obs: Já tinha coloca o 'CHANGE-CAUSE' outro dia quando estava já fazendo estes exercícios

                Mas caso queira mudar o 'CHANGE-CAUSE' basta utilizar o comando...

                    $ kubectl annotate deployment sistema-noticias-deployments kubernetes.io/change-cause="Uma outra causa"

        E a mesma coisa podemos fazer com o db-noticias-deployment...

            deployment.apps/db-noticias-deployment 
            REVISION  CHANGE-CAUSE
            1         Subindo o banco na versão 1

                Obs: Mesmo caso do 'sistema-noticias-deployment' que já havia sido testado por mim anteriormente...

                Para mudar a causa...

                    $ kubectl annotate deployment db-noticias-deployment kubernetes.io/change-cause="Uma outra causa 2 - O retorno"

        E agora, temos todo esse controle, se caso alguma de nossas aplicações parem de funcionar, tudo vai voltar da maneira que estava. O deploymente se encarrega de subir
        outras réplicas caso o "número de réplicas desejadas" seja maior que o "número de réplicas prontas".

        Porém com todas estas mudanças, principalemente a do Banco de Dados, onde derrubamos seu POD para subi-lo novamente com o Deployment, perdemos toda a informação que 
        tinhamos salvo durante o curso, todas aquelas notícias que tinhamos postado junto com as imagens...

        Isto acontece porque os PODs são efêmeros eles não tem nenhum dado armazenado neles, porque eles estão prontos para serem armazenados, criados e destruídos.

        A pergunta que fica agora é: Como podemos persistir os dados em caso de falhas? Precisamos de alguma maneira, que, caso um container dentro de um POD reinicie, ou o POD
        todo reinicie, continuemos tendo acesso às informações que já estavam lá.

        E, para issom vamos começar a ver um conceito novo que vão ser os 'Volumes Persistentes', 'Storage Classes'. Um conteúdo bem vasto sobre toda esssa questão de
        armazenamento no Kubernetes.

    
    O que aprendemos nessa aula:

        - A manter pods em execução com ReplicaSets e Deployments através de arquivos declarativos
        - A fazer o controle de versionamento de Deployments com o kubectl
        - Como utilizar os comandos kubectl rollout para ver e alterar as versões de Deployments.
        - Que ReplicaSets são criados automaticamente dentro de um Deployment
        - Que Pods normalmente são criados através de Deployments, e não individualmente. 
            (Deployment é Padrão corporativo)
        

Questões aula 01:

    01 - Qual será o resultado produzido pelo arquivo declarativo abaixo?

        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
            name: nginx-replicaset
        spec:
            template:
                metadata:
                    name: nginx-app
                    labels:
                        app: nginx-app
                spec:
                    containers:
                        - name: nginx-container
                          image: nginx:latest
            replicas: 6
            selector:
                matchLabels:
                    app: nginx-app


        R: Ele criará um ReplicaSet que terá 6 réplicas do pod nginx-app.

    
    02 - Quais das alternativas abaixo contém diferenças entre ReplicaSets e Deployments?

        R: Quando criados, Deployments auxiliam com controle de versionamento e criam um ReplicaSet automaticamente.

    
    03 - Qual comando podemos utilizar para voltar um Deployment para uma revisão específica?

        R: kubectl rollout undo deployment <nome do deployment> --to-revision=<versão a ser retornada>


Aula 02: Persistindo dados com o Kubernetes -------

    Persistindo dados com volumes:

        Primeiro veremos a persistência de dados dentro de containers, depois veremos a persistência de dados dentro dos POD's.
        Já vimos que o Docker consegue fazer o compartilhamento de dados entre containers, agora veremos como o Kubernetes faz isso.

        Primeiro veremos compartilhamento de dados entre containers que estão dentro do mesmo POD, depois avançaremos.

        Temos 4 recursos do Kubernetes para tratarmos persistência de dados.

            - Volumes
            - Persistent Volumes
            - Persistent Volumes Claim
            - Storage Classes

        Começaremos por 'Volumes', utilizaremos ele para compartilhar dados e arquivos entre containers num mesmo POD.

        No Kubernetes temos a peculiaridade de que se os containers 'morrem' temos nosso volume salvo pelo POD.

        Ou seja, se dos containers que temos dentro de um POD sobrar ainda 1 'vivo', nosso volume e o POD permanecerão a 'salvo'.

        Mas, o que acontece com os nossos dados e arquivos?

        Podemos olhar a documentação do Kubernetes -> https://kubernetes.io/docs/concepts/storage/volumes/ podemos ver todos os detalhes sobre volumes, e já sabemos que 
        containers e PODs são efêmeros, porém se formos mais a fundo na documentação veremos que o Kubernetes suporta diversos tipos de volumes, por exemplo:
        'awsElasticBlockStore', 'azureDisk', 'azureFile' etc

        Lista dos tipos de volumes suportados pelo Kubernetes:

            - awsElasticBlockStore
            - azureDisk
            - azureFile
            - cephfs
            - cinder
            - configMap
            - downwardAPI
            - emptyDir
            - fc (fibre channel)
            - flocker (deprecated)
            - gcePersistentDisk
            - Regional persistent disks
            - gitRepo (deprecated)
            - glusterfs
            - hostPath
            - iscsi
            - local
            - nfs
            - persistentVolumeClaim
            - portworxVolume
            - projected
            - quobyte
            - rbd
            - scaleIO (deprecated)
            - secret
            - storageOS
            - vsphereVolume
            - csi
            - flexVolume
        
        Utilizaremos o 'hostPath' para exemplificar a nossa utilização de volumes, vai ser bem semelhante ao tipos de volume que utilizamos no curso de Docker.

        Nós precisamos fazer o bind de um diretório do nosso host para um diretório de dentro do nosso container do nosso Pod.

        Se olharmos na parte da documentação que explica o 'hostPath' (https://kubernetes.io/docs/concepts/storage/volumes/#hostpath), veremos que é exatamente da maneira
        que já vimos antes, o hostPath monta um caminho ou arquivo do nosso host e monta ele dentro dos containers do POD.

        E equanto o Pod ficar 'vivo', esse volume vai existir.

        Vamos exemplificar criando um arquivo yaml com o tipo Pod para facilitar a explicação, o nome do arquivo será pod-volume.yaml. Funcionaria também se fosse um
        'ReplicaSet' ou um 'Deployment'.

        Definimos o 'kind' que será 'Pod' e no 'metadata' definimos um 'name' que será 'pod-volume' mesmo.

            apiVersion: v1
            kind: Pod
            metadata: 
                name: pod-volume

        Nas especificações declararemos dois containers, para isso basta duplicar as tags a partir da tag '- name', e escolher seus atributos, como imagem, por exemplo...

            spec:
            containers:
                - name: nginx-container
                  image: nginx:latest

                - name: jenkins-container
                  image: jenkins:2.60.3-alpine

        Agora falta apenas definirmos qual será o volume que estes containers irão utilizar. Porém como já vimos antes o volume será do Pod na verdade, e não dos containers
        então temos que ajustar a declaração deste volume ao do POD.

        Criaremos o volume dando um nome pra ele, que será 'primeiro-volume' e definir qual é o tipo. Nós queremos um 'hostPath', sendo assim temos que definir um caminho
        na nossa máquina local para este volume, o local por mim escolhido foi 
            '/F/Projetos para Git e Github/Estudos/Alura/DevOps/11 - Kubernetes - Pods, Services e ConfigMaps/parte2/primeiro-volume'
        Depois basta declararmos que este tipo de volume é um diretório, 'Directory' na linguagem do Kubernetes.

            volumes:
                - name: primeiro-volume
                  hostPath: 
                    path: /F/Projetos para Git e Github/Estudos/Alura/DevOps/11 - Kubernetes - Pods, Services e ConfigMaps/parte2/primeiro-volume
                    type: Directory

        Definido para o POD qual será e onde ficará o volume, temos que fazer isso para os containers agora...Temos que dizer onde nos containers o POD vai montar o volume
        'primeiro-volume'.

        Então, dentro da definição de cada um desses containers, nós precisamos colocar a tag 'volmeMounts' e dentro dele basta definir qual é o caminho dentro do nosso
        container. Pode ser, por exemplo, 'volume-dentro-do-container', e qual é o volume que queremos montar? 'primeiro-volume', então o nome do volume será 'primeiro-volume'...

        - name: nginx-container
        image: nginx:latest
        volumeMounts:
          - mountPath: /volume-dentro-do-container
            name: primeiro-volume
      
      - name: jenkins-container
        image: jenkins:2.60.3-alpine
        volumeMounts:
          - mountPath: /volume-dentro-do-container
            name: primeiro-volume

        Por fim, se estivermos no Windows utilizando o Docker Desktop, nas configurações, desativar, caso esteja ativada, a utilização do WSL como base. 
        Depois em resources, basta irmos em 'File Sharing' e adicionar o diretório 'pai' de onde ficarão seus outros diretórios dos volumes, ou declarar o diretório do volume
        já de cara...No caso deixei um diretório acima para criar outros subdiretórios como volumes.

        Depois de termos feito tudo isso, basta irmos no terminal e digitar...

            $ kubectl apply -f pod-volume.yaml

        Para conferirmos...

            $ kubectl get pods

        Podemos reparar que foi criado um Pod com dois containers dentro.

        Entraremos no modo interativo para que possamos realizar uns testes...

            $ kubectl exec -it pod-volume --container nginx-container -- bash

        Se executarmos o comando 'ls' veremos que o nosso 'volume-dentro-do-container' está lá...

        Podemos criar um arquivo com 'touch arquivo.txt' e verificar na nossa máquina local...

        Entrar no outro container...

            $ kubectl exec -it pod-volume --container jenkins-container -- bash
        
        Executar novamente o comando 'ls', verificamos que o arquivo que foi criado no 'nginx-container' está ali também, pois eles compartilham o mesmo volume.

        Podemos criar outro arquivo neste nosso 'jenkins-container' com 'touch outro-arquivo.txt' se verificarmos na nossa máquina local e no 'nginx-container', veremos que
        temos mais um arquivo criado no mesmo diretório compartilhado.

        Este volume será deletado quando destruirmos o POD, mas os arquivos permanecerão na nossa pasta no computador físico.

        O que não continua é o Volume em si, se nós fizermos a mesma declaração e criarmos de novo o POD e o novo volume, vai estar tudo ali porque os arquivos já existem.


    Volumes no Linux:

        Por causa da máquina virtual que utilizamos para acessar o Minikube, não basta criarmos o diretório do 'hostPath' na nossa máquina local, na hora de subir o POD será
        apresentado um erro. Isso porque não máquina o diretório não existe, teríamos que acessar o Minikube e criar o diretório que declaramos no yaml.

        Mas isso é fácil de resolver, na declaração do tipo de volume dentro do yaml do POD, basta que coloquemos 'DirectoryOrCreate' porque assim se o diretório não existir
        ele será criado...

        volumes:
        - name: segundo-volume
          hostPath:
            path: /home/segundo-volume
                  type: DirectoryOrCreate


    Validando a definição de Volumes:

        Esta parte foi colocada em três YAMLs de exemplo na pasta do curso...

        - Para saber mais: Mais informações sobre PersistentVolumes

            Como dito, existem diversos tipos de plugins de volumes que podem ser utilizados pelos Cloud Providers, cada um com seu respectivo modo de acesso e nomes. 
            Por exemplo, o GCEPersistentDisk que usamos nos vídeos anteriores, permite apenas a criação de um PersistentVolume em modo de ReadWriteOnce ou ReadOnlyMany. 
            Diversas outras informações sobre plugins de volumes e modos de acesso podem ser conferidas nesse link 
            (https://kubernetes.io/docs/concepts/storage/persistent-volumes/) da documentação oficial.
        

    O que aprendemos nessa aula:

        - Como criar Volumes através de arquivos de definição
        - Volumes persistem dados de containers dentro de pods e permitem o compartilhamento de arquivo dentro dos pods
        - Que Volumes possuem ciclo de vida independente dos containers, porém, vinculados aos PODS
        - Como criar PersistentVolumes através de arquivos de definição
        - PersistentVolumes persistem dados de pods como um todo
        - PersistentVolumes possuem ciclo de vida independente de quaisquer outros recursos, inclusive pods
        - Como criar e para que servem os PersistentVolumeClaims
        - Que precisamos de um PersistentVolumeClaim para acessar um PersistentVolume


Questões aula 02:

    01 - Quais são as principais características de um Volume?

        R: Volumes possuem ciclo de vida dependentes de Pods e independentes de containers.

    
    02 - Qual será o resultado produzido pelo arquivo de definição abaixo?

        apiVersion: v1
        kind: Pod
        metadata:
            name: um-pod-qualquer
        spec:
            containers:
                - name: nginx-container
                  image: nginx:latest
                  volumeMounts:
                    - mountPath: /pasta-de-arquivos #Local no container onde será criado o diretório
                      name: volume-pod #Tag das especificações do volume que será criado no container
            volumes:
                - name: volume-pod #Especificações do volume que será criado nos containers que declararam esta tag
                  hostPath:
                    path: /C/Users/Daniel/Desktop/uma-pasta-no-host
                    type: Directory

        R: Caso a pasta /C/Users/Daniel/Desktop/uma-pasta-no-host exista no host, um volume chamado volume-pod será criado e montado na pasta pasta-de-arquivos dentro do 
            container do Pod.

    
    3 - Quais das alternativas abaixo são verdadeiras sobre PersistentVolumes?

        Selecione 2 alternativas

        R1: PersistentVolumes possuem ciclos de vida independentes de Pods.

        R2: É necessário um PersistentVolumeClaim para acessar um PersistentVolume.


Aula 03: Storage Classes e StatefulSets -------

    Utilizando Storage Classes:

        https://kubernetes.io/docs/concepts/storage/storage-classes/

        O que um StorageClass muda efetivamente? Ele muda no sentido de que agora nós vamos conseguir criar Volumes, PersistentVolumes, no caso, e discos dinamicamente.
        Ou seja, o Disco na Cloud e o recurso que abstraía a forma como os dados eram persistidos, serão criados e destruídos de forma dinâmica, depois que fizermos o bind
        entre o nosso POD e o PVC.

        Este foi o trecho de código que pegamos da documentação para construir nosso StorageClass
        Obs: Na documentação, pelo que pude notar, temos exemplos de praticamente todos os tipos de StorageClasses suportados pelo Kubernetes.

        Utilizamos no exemplo desta aula o GCEPersistentDisk que é o StorageClasss da Google Cloud Provider.

        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: slow
        provisioner: kubernetes.io/gce-pd
        parameters:
          type: pd-standard
          fstype: ext4
          replication-type: none
        
        Não foi mudado absolutamente nada! Tivemos apenas que construir o PersistentVolumeClaim e o YAML do POD para que pudessem ser utilizados na núvem.

        Depois de rodar os 3 arquivos (sc.yaml, pvc-sc.yaml e pod-sc.yaml), dentro da GCP com o velho comando 'kubectl apply -f "arquivo yaml"', se quiser podemos colocar a
        tag '--watch' para assistir a criação.

        Depois foi feito o mesmo procedimento para verificar o compartilhamento de arquivos...

            Comandos: 'kubectl exec -it "pod" -- bash', 'ls' etc

    
    Conhecendo StatefulSets:

        Nós tinhamos um problema inicial, nós tinhamos um Deployment para o nosso sistema notícias, mas quando o POD desse sistema falha, o Deployment, através do ReplicaSet,
        vai criar o POD novamente.

        Mas, os arquivos são perdidos, então, nós precisamos ter alguma maneira que, por mais que o POD seja reiniciado, o arquivo seja persistido.

        Agora tendo estudado um pouco StorageClasses, PersistentVolumes e PersistentVolumesClaim, temos uma ideia de como resolver isso.

        O Kubernetes possui um recurso chamado StatefulSet, que permite que mentenhamos o estado do POD como está, aplicando uma mesma idêntidade já existente a PODs que vão
        sendo criados, pois os POD's continuam efêmeros, a diferença é que com StatefulSet a identidade definida por este recurso a um POD, poderá continuar existindo em outro
        POD quando o primeiro for substituído.

        Para isso devemos definir dentro do StatefulSet um PersistentVolumeClaim. Isto acontecerá para cada um dos PODs dentro do StatefulSet.

        Cada POD dentro de um StatefulSet terá sua própria idêntidade.

        De certa forma, o StatefulSet trata o novo POD como se fosse o antigo. Eles terão acesso ao mesmo PersistentVolumeClaim.

        Criamos o nosso 'sistema-noticias-statefulset' baseado no nosso 'sistema-noticias-deployment'.

        Excluímos o Deployment antigo e criamos o StatefulSet...

            $ kubectl delete deployment sistema-noticias-deployment

            $ kubectl apply -f sistema-noticias-statefulset.yaml

            Obs: Como neste primeiro vídeo não definimos o PersistentVolumeClaim, conseguimos subir o StatefulSet porém os dados não foram persistidos.
                Porém, foi bom realizar este teste do StatefulSet para ver que o único POD que criamos veio com um ID na frente, e depois de deletarmos e recriarmos este POD,
                o ID permaneceu o mesmo.

    
    Utilizando um StatefulSet:

        Agora precisamos solucionar o problema, precisamos persistir as nossas imagens e a nossa sessão.

        Precisamos criar um PersistenVolumeClaim para um PersistentVolume, que vai armazenas as nossas imagens e um PersistentVolumeClaim que vai permitir o acesso com
        PersistentVolume que vai armazenar a nossa sessão.

        Os arquivos que foram necessários estão na pasta do curso, são eles: 'imagens-pvc.yaml', 'sessao-pvc.yaml' e o 'sistema-noticias-statefulset.yaml'

        Aplicamos os arquivos:

            $ kubectl apply -f imagens-pvc.yaml

            $ kubectl apply -f sessao-pvc.yaml

            $ kubectl apply -f sistema-noticias-statefulset.yaml

        Verificamos os pvc e o pv, este último foi criado dinamicamente pois temos nativo em nosso cluster um StorageClass assim como temos nos Clouds providers.

            $ kubectl get pvc

            $ kubectl get pv

        Se quiser assistir os recursos sendo criados:

            $ kubectl get pods --watch

        Utilizando o comando:

            $ kubectl describe pod sistema-noticias-statefulset-0

            Veremos que ele fez o mapeamento para o volume que foi definido em arquivo. Todo o bind foi feito de forma automática. Agora mesmo que o POD falhe, teremos os 
            dados persistido e quando o POD for substituído ainda teremos nossa sessão e nossas imagens.

    
    O que aprendemos nessa aula:

        - Como criar PersistentVolumes dinamicamente com StorageClasses
        - StorageClasses também são capazes de criar discos de armazenamento
        - O que é um StatefulSet
        - Como utilizar StatefulSets para garantir unicidade de Pods durante reinícios e atualizações
        - Clusters possuem StorageClasses "default" e podem ser usados automaticamente se não definirmos qual será utilizado


Questões aula 03:

    01 - Qual das afirmativas abaixo lista uma vantagem da utilização de Storage Classes?

        R: Storage Classes fornecem dinamismo para criação de PersistentVolumes conforme demanda.

    
    02 - Quais são as principais características de um StatefulSet?

        Selecione 2 alternativas

        R1: StatefulSets podem ser usados quando estados devem ser persistidos.

        R2: StatefulSets usam PersistentVolumes e PersistentVolumeClaims para persistência de dados.


    03 - O que podemos afirmar sobre o arquivo de definição abaixo?

    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
      name: um-statefulset-qualquer
    spec:
      #Restante omitido...
        spec:
          containers:
            - name: nginx-container
              image: nginx:latest
              ports:
                - containerPort: 80
              volumeMounts:
                - name: dados-persistidos
                  mountPath: /data-info
          volumes:
            - name: dados-persistidos
              persistentVolumeClaim:
                claimName: dados-pvc
      #Restante omitido....   

        R: Para que funcione da maneira esperada, devemos ter um PersistentVolumeClaim chamado dados-pvc criado.


Aula 04: Checando status com Probes -------

    Conhecendo Probes:

        Temos que ter ciência que o podemos a qualquer momento, ao tentar acessar uma aplicação dentro de um POD ou um balanceamento de carga, receber um código
        HTTP 500 erro interno do servidor ou algum erro 404, "NOT FOUND", e isso significa que a aplicação dentro deste Pod não está respondendo de maneira esperada.
        Ela não está funcionando de maneira que esperávamos.

        Ou seja, temos uma situação onde o POD está saudável mas a aplicação dentro dele não! O Kubernetes não sabe se pode ou não reiniciar este POD.

        Quando a aplicação ou o load balancer está funcionando perfeitamente recebemos o código 200, informando que está tudo ok. Mas e se não for o caso?

        Para resolver este tipo de problema, nós temos os 'Configure Liveness', 'Readiness e Startup Probes'.

            - Liveness Probes
                Nada mais é que uma prova de vida que a aplicação dentro de um container de um Pod está funcionando.

        O Kubelet vai usar o Liveness Probe como um critério para saber quando reiniciar o container de um Pod. Então, quando cair em algum deadlock ou a aplicação
        está rodando, mas, incapaz de manter progresso.

        Então o Kubelet é aquele componente dos nossos nodes que serão responsáveis, além do que já vimos, por, através do Liveness Probe, saber se a aplicação deve
        ser reiniciada ou não.

        A vantagem é que não precisamos declarar um arquivo só para o 'Liveness Probe', na própria declaração do nosso container podemos definir qual será a prova de
        vida, como garantimo que este container dentro do nosso Pod está vivo.

    
    Utilizando Liveness Probes:

        Para configurar o nosso LivenessProbe temos que colocar um trecho no padrão que temos abaixo.
        Obs: Um exemplo melhor de se visualizar está dentro dos arquivos 'portal-noticias-deployment.yaml' e 'sistema-noticias-statefulset.yaml'

            livenessProbe: # Declaração do bloco da "PROVA DE VIDA"
                httpGet: # Requisição utilizando o verbo Get
                    path: / # Caminho da aplicação, como estamos utilizando direto o localhost:30000, colocamos somente o '/'
                    port: 80 # Aqui deve ser definida em qual porta o container está escutando e não a porta de saída, de acesso do Load Balancer ou servidor web
                periodSeconds: 10 # De quanto em quanto tempo queremos fazer o teste da aplicação, a cada 10 segundos eu quero fazer a validação deste container
                failureThreshold: 3 # Número de falhas antes de realizar o reinicio do container
                initialDelaySeconds: 20 # Atraso inicial antes de começar a realizar os testes, pois o container sobe, mas nem sempre de imediato a aplicação está
                                        # pronta em execução

        Depois de adicionar este trecho de código nos arquivos citados, basta utilizar o comando

            $ kubectl apply -f "arquivo.yaml"

        Se depois disso quisermos averiguar com 'kubectl get pods' ou 'kubectl describe pod "POD"', ou até mesmo utilizar o debug do navegador com F12 (Google)
        para verificar que os PODS e containers estão todos ok retornando código 200.


    Utilizando Readiness Probes:

        Acima vimos como ter prova de que o container ainda está "Vivo", agora praticamente, após algum erro ou falha, precisamos da prova de que ele não está "morto".

        O POD está já está pronto, mas, o container que, também de certa forma já subiu mas ainda não terminou completamente, não está pronto para receber requisições. Ele ainda
        não terminou de executar os scripts que ele tem para inciiar ou qualquer outra coisa do tipo.

        Então, precisamos ter uma maneira de garantir quando o container deste Pod estará pronto para receber as requisições. Pois se temos 3 Pods e um deles para, ficaremos 
        com apenas dois recebendo as requisições do Load Balancer.

        Para isso basta definirmos o nosso 'Readiness Probe', que é um bloco de cógigo parecido com o 'Liveness Probe', o que muda são as consequências.

            Exemplo de bloco readinessProbe:

                readinessProbe: # Declaração do bloco da prova que ele está "PRONTO"
                httpGet: # Requisição utilizando o verbo Get
                    path: /inserir_noticias.php # Caminho da aplicação/Estará pronto caso consiga enviar requisições para esta página (Código 200 inclusive, 400 exclusive)
                    port: 80 # Porta do container
                periodSeconds: 10 # A cada 10 segundos ele vai mandar verificação
                failureThreshold: 5 # Se ele fizer o teste 5 vezes, na sexta vai enviar a requisição
                initialDelaySeconds: 3 # Depois que o POD e o container reiniciar, após 3 segundos, as requisições poderão voltar a ser enviadas

            Obs: Este bloco de código foi adicionado tanto ao arquivo 'protal-noticias-deployment.yaml' quando ao 'sistema-noticias-statefulset.yaml'.

        Se fizermos o 'kubectl apply -f "arquivo.yaml"' e depois o 'kubectl get pods --watch' veremos os POD's subirem novamente e que eles ficaram prontos após 3 segundos.

        Um extra...Podemos também fazer definirções através de TCP, então, podemos enviar através de um Socket com o TCP, por alguma porta, para validar se a aplicação está saudável
        ou não.

    
    Para saber mais: Startup Probes

        Há um terceiro tipo de probe voltado para aplicações legadas, o Startup Probe. Algumas aplicações legadas exigem tempo adicional para inicializar na primeira vez. 
        Nem sempre Liveness ou Readiness Probes vão conseguir resolver de maneira simples os problemas de inicialização de aplicações legadas. Mais informações sobre 
        Startup Probes podem ser adquiridas por aqui:

            https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes


    O que aprendemos?

        O que aprendemos nessa aula:

        - O Kubernetes nem sempre tem como saber se a aplicação está saudável
        - Podemos criar critérios para definir se a aplicação está saudável através de probes
        - Como criar LivenessProbes com o campo livenessProbe
        - LivenessProbes podem fazer a verificação em diferentes intervalos de tempo via HTTP
        - Como criar ReadinessProbes com o campo readinessProbe
        - ReadinessProbes podem fazer a verificação em diferentes intervalos de tempo via HTTP
        - LivenessProbes são para saber se a aplicação está saudável e/ou se deve ser reiniciada, enquanto ReadinessProbes são para saber se a aplicação já está pronta para receber requisições depois de iniciar
        - Além do HTTP, também podemos fazer verificações via TCP
    
Questões aula 04:

    01 - Qual é a principal utilização de probes?

        R: Tornar visível ao Kubernetes que uma aplicação não está se comportando da maneira esperada.

    
    02 - Utilizando o HTTP, como um Liveness Probe entende que a aplicação não está respondendo de maneira saudável?

        R: Ele indicará falha caso o código de retorno seja menor que 200 ou maior/igual a 400.

    
    03 - A quais conclusões podemos chegar analisando o trecho do arquivo de definição abaixo?

        readinessProbe:
        httpGet:
            path: /
            port: 80
        periodSeconds: 10
        initialDelaySeconds: 3

        Selecione duas alternativas

        R1: Ele começará a executar os testes apenas 3 segundos depois do container ser criado.

        R2: Ele fará os testes utilizando a porta 80 da aplicação.


Aula 05: Como escalar com o Horizontal Pod Autoscaler -------

    Escalando pods automaticamente:

        Sabendo que a quantidade de acesso no nosso site pode aumentar significativamente, podemos imaginar que os recursos computacionais do nosso POD pode ficar insuficientes
        para atender a tantas requisições, o que vai fazer com que o nosso POD pare de funcionar. Apesar de termos recursos que fazem ele subir novamente como o ReplicaSet,
        Deployment ou StatefulSet, ele vai continuar caindo por falta de recursos...

        Sendo assim temos um recurso que escala de forma automática a quantidade de recursos de um POD no Kubernetes, ou melhor dizendo, ele vai observar a quantidade de recursos
        e vai adicionar mais PODs conforme a necessidade, este recurso se chama HPA (Horizontal Pod Autoscaller).

        Ele também terá a capacidade de quando os acessos caírem diminuir o número de PODs.

        Se dermos uma lida na documentação do Kubernetes, ele resume para nós que o Horizontal Pod Autoescaler é um recurso capaz de, automaticamente, escalar o número de Pods em 
        um Deployment, um ReplicaSet ou em um StatefulSet, baseado na observação da CPU, então, nós temos um primeiro ponto que vamos precisar nos preocupar.

                https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

        Utilizamos o HPA no nosso 'portal-noticias-deployment', baseado no consumo de CPU dele, nós vamos aumentar ou diminuir, para suprir essa demanda, o número de Pods.

        Na documentação podemos verificar que o HPA faz todo este trabalho baseado em métricas, como por exemplo, utilização de CPU.

        Só que nós precisamos informar quanto este container dentro deste POD consome de CPU. Quanto ele pede de CPU para funcionar?

            Exemplo da definição deste bloco de código abaixo:

                resources: # Bloco do recurso exigido
                    requests: # Requisita o recurso
                        cpu: 10m # Este container "exige" 10 milicore de CPU para funcionar

            Este bloco está no nosso arquivo 'portal-noticias-deployment'...

        Agora, como os outros recursos, precisamos de um outro arquivo para criar o nosso Horizontal Pod Autoescaler, HPA também é um objeto da API.

        Criamos o arquivo de definição 'portal-noticias-hpa.yaml', as versões que temos para o HPA são, 'autoescaling/v1' e 'autoescaling/v2beta2', nós vamos utilizar a 
        'autoescaling/v2beta2', porque toda a documentação está começando a se basear mais nela. Está quase 100% estável já. (Na época deste curso)

        Como ficou o nosso arquivo 'portal-noticias-hpa.yaml' logo abaixo...

        apiVersion: autoscaling/v2beta2 # Versão do recurso, no caso HPA
        kind: HorizontalPodAutoscaler # Qual o tipo do recurso
        metadata:
          name: portal-noticias-hpa # Nome deste recurso que está sendo criado 
        spec:
          scaleTargetRef: # A que alvo queremos fazer referência
            apiVersion: apps/v1 # Qual a versão do alvo que eu quero escalar 
            kind: Deployment # Qual é o tipo do que eu quero escalar
            name: portal-noticias-deployment # Qual é o nome do alvo que eu quero escalar
          minReplicas: 1 # Qual é o número minímo permitido para o meu alvo
          maxReplicas: 10 # Qual é o número máximo permitido de réplicas para o meu alvo
          metrics: # Qual a métrica que vou definir para o meu HPA
            - type: Resource # Recursos do sistema, não do Kubernetes
              resource: # Qual recurso quero utilizar?
                name: cpu # Recurso CPU
                target: # Alvo da métrica
                  type: Utilization # Utilização do CPU citado acima 
                  averageUtilization: 50 # Consumo médio ficar acima de 50%, dos milicores que definimos no arquivo portal-noticias-deployment.yaml, que foi 10 milicores
                                          # Ou seja, se o consumo passar de 5 milicores do CPU vai ser criada uma réplica.

        Depois de aplicar estes arquivos podemos utilizar o 'kubectl apply -f "arquivo.yaml"' e depois 'kubectl get hpa'.

        NAME                  REFERENCE                               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE 
        portal-noticias-hpa   Deployment/portal-noticias-deployment   <unknown>/50%   1         10        3          169m

        Ele está mostrando que nós temos o HPA que faz referência ao nosso Deployment do portal-noticias.
        Ele identifica que temos 3 réplicas do portal-noticias porém não consegue identificar quanto dos 50% os PODs estão utilizando.
        Vamos olhar com um pouco mais a fundo o que está acontecendo 'kubectl describe hpa portal-noticias-hpa'

         # Warning  FailedGetResourceMetric  114s (x171 over 171m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to 
            # fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)'

        Ele foi incapaz de pegar as métricas para recurso de CPU. Ele não foi capaz de pegar esses dados da API de métricas. O servidor não pôde encontrar esse recurso pedido.
        E aqui ele fala que essa métrica foi inválida e nós precisamos definir o que é uma CPU. Vimos como declarar o HPA, agora veremos como faze-lo funcionar.

    
    Utilizando o HPA no Windows:

        Para fazer fazer o laboratório 'Utilizando o HPA no Windows' precisaremos baixar dois arquivos:

            Página do GitHub utilizada: https://github.com/kubernetes-sigs/metrics-server

            Script de stress: https://caelum-online-public.s3.amazonaws.com/1916-kubernetes/stress.zip
            Arquivo de definição do servidor de métricas: https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml

            Para fazer o download do GitBash: https://git-scm.com/download/win

        Na documentação do kubernetes-sigs/metrics-server, temos uma questão bem simples de caso de uso. Nós podemos usar um servidor de Métricas para definir, basear as nossas
        informações de consumo de CPU e memória para utilizar o Horizontal Pod Autoscaler e, também, fazer esse ajuste de maneira automática, conforme recursos necessitados pelos
        containers.

        apiVersion: v1
        kind: ServiceAccount
        metadata:
          labels:
            k8s-app: metrics-server
          name: metrics-server
          namespace: kube-system

        Nesta parte podemos ver que o 'components.yaml' nada mais é que um arquivo de definição...

        No decorrer deste arquivo veremos toda a definição do nosso Metric Server.

        Caso não tenhamos o objetvio de utiliza, a verificação de certificados, temos que desabilitar com a flag '- --kubelet-insecure-tls'.

        spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=443
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --kubelet-insecure-tls # Do note verify the CA of serving certificates presented by Kubelets. For testing purpose only.
          image: k8s.gcr.io/metrics-server/metrics-server:v0.5.0
          imagePullPolicy: IfNotPresent
          livenessProbe:

        Agora utilizamos o 'kubectl apply -f components.yaml'

        Damos um 'kubectl get hpa' e ele continua não identificando o consumo...

        Se utilizarmos o 'kubectl describe hpa' e passar o nosso 'portal-noticias-hpa'. Ele não vai mostrar métricas ainda pois tem que terminar de carregar todas as informações,
        para que ele consiga ler todos os detalhes de consumo de CPU.

        Isso é um "bug", que ele demora a fazer esse reconhecimento, então, assim que ele terminar, voltamos e seguimos com o nosso Horizontal Pod Autoscaler.

        Temos que fazer o teste com mais recursos sendo consumidos. Temos que enviar diversas requisições na porta 30000, o nosso portal.

        Para isso vamos utilizar o arquivo que baixamos, o 'stress.sh', e precisamos passar um valor pois ele precisa receber um parâmetro, o nosso intervalo, utilizamos 0.001
        segundos, e por fim, vamos colocar a saída dele em um arquivo.

            # sh stress.sh 0.001 > out.txt

            Obs: Para utilizar este comando precisamos do bash...

        Fiz o teste, conforme orientado, utilizando vários terminais do Bash, rodando várias vezes o script.

        \KUBERNETES\PARTE2\# kubectl get hpa --watch
        NAME                  REFERENCE                               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
        portal-noticias-hpa   Deployment/portal-noticias-deployment   70%/50%   1         10        1          26h
        portal-noticias-hpa   Deployment/portal-noticias-deployment   45%/50%   1         10        2          26h
        portal-noticias-hpa   Deployment/portal-noticias-deployment   40%/50%   1         10        2          26h
        portal-noticias-hpa   Deployment/portal-noticias-deployment   45%/50%   1         10        2          26h # Aqui eu fiz parar o ataque de requisições
        portal-noticias-hpa   Deployment/portal-noticias-deployment   10%/50%   1         10        2          26h
        portal-noticias-hpa   Deployment/portal-noticias-deployment   10%/50%   1         10        2          27h
        portal-noticias-hpa   Deployment/portal-noticias-deployment   10%/50%   1         10        1          27h


        Podemos ver que após começar a alta carga de requisições o consumo aumentou, consequentemente os PODs também foram scalados...

        Nesse caso, de 10 milicores, e sempre que ficarmos acima desse consumo, ele vai colocar mais Pods, tendo um limite de 10, como definimos, para que esse consumo seja 
        ajustado.

        Depois que os scripts pararam de executar, o consumo foi diminuindo e os POD's também...

        Então, o máximo que chegamos foi três, mas, ele agora vai regredir, aos poucos, como não está recebendo mais nada, ele vai ficar abaixo desses 50%, que é a nossa média, e 
        vai voltar para o estado inicial dele, que é de uma réplica, a aplicação está ociosa.


    Utilizando o HPA no Linux:

        No Linux utilizamos o servidor de métricas de forma diferente do Windows. No Windows, nós precisamos utilizar o arquivo de componentes, como já vimos, aquele que é descrito
        na documentação, que nós declaramos, definimos e aplicamos no nosso cluster. No caso do Linux, nós estamos utilizando o Minikube e ele, no Linux, tem diversas possibilidades
        e recursos para que utilizemos.

        Se executarmos um 'minikube addons list' ele mostra uma série de extensões que podemos utilizar e estão desabilitadas.

        Podemos ver nesta lista, entre os recursos, o servidor de métricas.

        $ minikube addons enable metrics-server

        Diferença no bash:

            #!/bin/bash
            for i in {1..10000}; do
                curl "IP DO NODE":30000
                sleep $1
            done

            Podemos criar este arquivo com o vim.

            O comando apra executá-lo é o mesmo: # sh stress.sh 0.001 > out.txt


    Para saber mais: VerticalPodAutoscaler:
    
        Além do HorizontalPodAutoscaler, o Kubernetes possui um recurso customizado chamado VerticalPodAutoscaler. O VerticalPodAutoscaler remove a necessidade de definir limites e 
        pedidos por recursos do sistema, como cpu e memória. Quando definido, ele define os consumos de maneira automática baseada na utilização em cada um dos nós, além disso, 
        quanto tem disponível ainda de recurso.

        Algumas configurações extras são necessárias para utilizar o VerticalPodAutoscaler. Mais informações podem ser obtidas nesse link:

            https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
        
        Também podemos ver mais informações específicas sobre diferentes Cloud Providers, como o GCP (https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler)
        e a AWS (https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html)


    O que aprendemos?
                
        O que aprendemos nessa aula:
        
        - Reiniciar a aplicação incessantemente através de ReplicaSets/Deployments nem sempre resolverá o problema
        - HorizontalPodAutoscalers são responsáveis por definir em quais circunstâncias escalaremos nossa aplicação automaticamente
        - Como definir o uso de recursos de cada container em um Pod
        - O que é, e como utilizar um servidor de métricas
        - Como utilizar um HorizontalPodAutoscaler através de arquivos de definição
        

Questões aula 05:

    01 - Por qual dos motivos abaixo o HorizontalPodAutoscaler pode não conseguir identificar o consumo de recursos dos Pods que está gerenciando?

        R: Não foi definido um servidor de métricas para que haja o funcionamento da maneira esperada.


    02 - Quais informações podemos extrair sobre o arquivo declarativo abaixo?

    apiVersion: autoscaling/v2beta2
    kind: HorizontalPodAutoscaler
    metadata:
      name: primeiro-hpa
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: primeiro-deployment
      minReplicas: 1
      maxReplicas: 5
      metrics:
        - type: Resource
          resource:
            name: cpu
            target:
              type: Utilization
              averageUtilization: 20

        R: O HPA visa manter o consumo médio de CPU o mais próximo de 20%.





        



